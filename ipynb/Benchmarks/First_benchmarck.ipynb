{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides the first naive benchmark for the geodesics in the latent space of the autoencoder.\n",
    "\n",
    "The benchmarks evaluates the effect of curvature regularization, measuring how far straight lines are from being the geodesics.\n",
    "\\medskip\n",
    "\n",
    "Let us consider straight line segments $\\alpha_{ij} \\subset M \\subset \\R^2$ connecting pairs of points $X_i$\n",
    "and $X_j$ given by:\n",
    "$$\n",
    "\\alpha_{ij} ( t ) = t \\Phi_{\\theta}(X_i) + (1-t) \\Phi_{\\theta}(X_j) \\ .\n",
    "$$\n",
    "Let us consider the images of $\\alpha_{ij}$ through the decoder $\\Psi$. One obtains curves in $\\R^D$ given by:\n",
    "$$\n",
    "\\gamma_{ij} ( t ) = \\Psi(t \\Phi_{\\theta}(X_i) + (1-t) \\Phi_{\\theta}(X_j)) \\ .\n",
    "$$\n",
    "\n",
    "Namely for a curve $\\gamma_{ij}(t)$ one can consider several functionals:\n",
    "The functional computing the second derivative of $\\gamma (t)$. If $t$ were a natural parameter, the functional below would compute the norm of the acceleration of the curve along the curve. However it does not, once the metric in $M$ is not Euclidean and thus $t$ is not the natural parameter.\n",
    "\\begin{equation}\n",
    "    \\text{1. }\\widetilde E_{ij} = \\int\\limits_0^1 \\|\\gamma_{ij}'' (t)\\|^2 dt \\ .\n",
    "\\end{equation}\n",
    "The energy functional of $\\alpha_{ij} ( t )$:\n",
    "\\begin{equation}\n",
    "    \\text{2. } E_{ij} = \\int\\limits_0^1 \\|\\alpha_{ij}' (t)\\|_g^2 dt = \\int\\limits_0^1 \\|\\gamma_{ij}' (t)\\|^2 dt \\ ,\n",
    "\\end{equation}\n",
    "recall $g = J_\\Psi^* J_\\Psi$ is the pull-back of the Euclidean metric by the decoder $\\Psi$.\n",
    "\n",
    "And finally, the acceleration functional:\n",
    "\\begin{equation}\n",
    "    \\text{3. } A_{ij} = \\int\\limits_0^1 \\| \\nabla_{\\alpha_{ij}' (t)} \\alpha_{ij}' (t) \\|^2 dt \\ ,\n",
    "\\end{equation}\n",
    "All the functionals are avereged:\n",
    "\\begin{equation}\n",
    "    \\mathcal{E} = \\frac{1}{\\binom{K}{2}} \\sum\\limits_{1 \\leq i < j \\leq K} E_{ij} \\ .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import yaml\n",
    "import ricci_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../experiments/MNIST_Setting_1_config.yaml', 'r') as yaml_file:\n",
    "#with open('../../experiments/MNIST01_exp7_config.yaml', 'r') as yaml_file:\n",
    "#with open('../../experiments/Swissroll_exp1_config.yaml', 'r') as yaml_file:\n",
    "    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "violent_saving = False # if False it will not save plots\n",
    "\n",
    "#number of points to be paiwise connected by straight lines\n",
    "#K = 100 # this can go up to 300 in practice\n",
    "K = 100\n",
    "d = 2\n",
    "print(\"number of points to be paiwise connected by straight lines:\", K)\n",
    "print(\"straight lines constructed\", int(K*(K-1)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and nn weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data loaders based on YAML configuration\n",
    "dict = ricci_regularization.DataLoaders.get_dataloaders(\n",
    "    dataset_config=yaml_config[\"dataset\"],\n",
    "    data_loader_config=yaml_config[\"data_loader_settings\"]\n",
    ")\n",
    "train_loader = dict[\"train_loader\"]\n",
    "test_loader = dict[\"test_loader\"]\n",
    "test_dataset = dict.get(\"test_dataset\")  # Assuming 'test_dataset' is a key returned by get_dataloaders\n",
    "\n",
    "print(\"Data loaders created successfully.\")\n",
    "additional_path=\"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = yaml_config[\"experiment\"][\"name\"]\n",
    "\n",
    "#Path_pictures = yaml_config[\"experiment\"][\"path\"]\n",
    "Path_pictures = additional_path + \"../experiments/\" + yaml_config[\"experiment\"][\"name\"]\n",
    "if violent_saving == True:\n",
    "    # Check and create directories based on configuration\n",
    "    if not os.path.exists(Path_pictures):  # Check if the picture path does not exist\n",
    "        os.mkdir(Path_pictures)  # Create the directory for plots if not yet created\n",
    "        print(f\"Created directory: {Path_pictures}\")  # Print directory creation feedback\n",
    "    else:\n",
    "        print(f\"Directiry already exists: {Path_pictures}\")\n",
    "\n",
    "curv_w = yaml_config[\"loss_settings\"][\"lambda_curv\"]\n",
    "\n",
    "dataset_name = yaml_config[\"dataset\"][\"name\"]\n",
    "D = yaml_config[\"architecture\"][\"input_dim\"]\n",
    "# D is the dimension of the dataset\n",
    "if dataset_name in [\"MNIST01\", \"Synthetic\"]:\n",
    "    # k from the JSON configuration file is the number of classes\n",
    "    #k = yaml_config[\"dataset\"][\"k\"]\n",
    "    k = len(yaml_config[\"dataset\"][\"selected_labels\"])\n",
    "    selected_labels = yaml_config[\"dataset\"][\"selected_labels\"]\n",
    "elif dataset_name == \"MNIST\":\n",
    "    k = 10\n",
    "print(\"Experiment name:\", experiment_name)\n",
    "print(\"Plots saved at:\", Path_pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(test_dataset)\n",
    "\n",
    "first_benchmark_data,rest = torch.utils.data.random_split(test_dataset, [K, l - K])\n",
    "\n",
    "#reformating\n",
    "first_benchmark_data = torch.stack([first_benchmark_data[i][0] for i in range(len(first_benchmark_data))])\n",
    "first_benchmark_data = first_benchmark_data.reshape(-1,D)\n",
    "\n",
    "d = yaml_config[\"architecture\"][\"latent_dim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../../experiments/Swissroll_exp0_config.yaml', 'r') as yaml_file:\n",
    "#    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "torus_ae, Path_ae_weights = ricci_regularization.DataLoaders.get_tuned_nn(config=yaml_config, additional_path = additional_path)\n",
    "\n",
    "torus_ae = torus_ae.to(\"cpu\")\n",
    "\n",
    "print(f\"AE weights loaded successfully from {Path_ae_weights}.\")\n",
    "\n",
    "encoder = torus_ae.encoder_torus\n",
    "decoder = torus_ae.decoder_torus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the curve $\\gamma (t)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluates gamma at t between x and y\n",
    "def gamma (t, x, y, decoder):\n",
    "    return decoder( y * t + x * ( 1 - t ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\gamma ' ( x ) \\approx \\frac{ \\gamma ( x + h ) - \\gamma ( x - h ) }{ 2 h }\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first derivative of gamma that connects x and y at t \n",
    "def gamma_prime(t, h,  x, y, decoder):\n",
    "    return (gamma(t+h, x, y,decoder) - gamma(t-h, x, y, decoder))/( 2 * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\gamma''(x) \\approx \\frac{\\gamma(x+h) - 2\\gamma(x) + \\gamma(x-h)}{h^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_second(t, h,  x, y, decoder ):\n",
    "    return (gamma(t+h, x, y, decoder = decoder) - 2*gamma(t, x, y, decoder = decoder) + gamma(t-h, x, y, decoder = decoder))/(h**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\n",
    " \\widetilde E_{ij} \\approx \\frac{1}{2 (n-1)} \\sum\\limits_{k=0}^{n-2} \\left( \\| \\gamma''_{ij}(t_k) \\|^2 + \\| \\gamma''_{ij}(t_{k+1}) \\|^2 \\right)\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_tilde(x_i,x_j,n_partition, decoder):\n",
    "    n = n_partition\n",
    "    segment_partition = ( 1 / (n-1) ) * torch.arange(n, dtype=torch.float32)\n",
    "    segment_partition_dim_d = segment_partition.repeat(d,1).T\n",
    "    gamma_second_array = gamma_second (segment_partition_dim_d,\n",
    "              h = 1/n, x = x_i, y = x_j,\n",
    "              decoder = decoder)\n",
    "    gamma_second_norm_array = gamma_second_array.norm(dim=1)\n",
    "    E_ij = ( 0.5 / ( n - 1 ) ) * torch.sum( (gamma_second_norm_array[:-1]**2 + gamma_second_norm_array[1:]**2) )\n",
    "    # return E_ij.item() doesnot work with vmap\n",
    "    return E_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    " E_{ij} \n",
    " &\\approx \\sum_{k=0}^{N} h \\left\\| \\frac{ \\gamma(kh+h) - \\gamma(kh-h)}{ 2 h } \\right\\|_2^2 \\\\\n",
    " &= \\frac{1}{4 h} \\sum_{k=0}^{N} \\left\\| \\gamma(kh+h) - \\gamma(kh-h) \\right\\|_2^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_gamma(t, h,  x, y, decoder):\n",
    "    return (gamma(t+h, x, y, decoder) - gamma(t-h, x, y, decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(x_i,x_j,n_partition, decoder):\n",
    "    # n_partition is number of points in partition for \n",
    "    # the integral approximation by its Riemann sum\n",
    "    n = n_partition\n",
    "    segment_partition = (1/(n-1))*torch.arange(n,dtype=torch.float32)\n",
    "    segment_partition_dim_d = segment_partition.repeat(d,1).T\n",
    "    delta_gamma_array = delta_gamma (segment_partition_dim_d,\n",
    "              h = 1/ (n + 1), x = x_i, y = x_j,\n",
    "              decoder = decoder)\n",
    "    delta_gamma_array_norm_array = delta_gamma_array.norm(dim=1)\n",
    "    # h = 1/ (N + 1)\n",
    "    E_ij = 0.25 * (n + 1) * torch.sum( delta_gamma_array_norm_array ** 2 )\n",
    "    # return E_ij.item() doesnot work with vmap\n",
    "    return E_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A_{ij} = \\int_0^1 \\| \\gamma''_T \\|^2 dt\n",
    "\\approx \\frac{1}{N - 1} \\sum_{k = 1}^N \\| Proj_{ \\langle v_1, v_2 \\rangle }( \\gamma(kh+h) - 2\\gamma(kh) + \\gamma(kh-h) ) \\|_2^2 \\ ,\n",
    "$$\n",
    "where $v_1$ and $v_2$ are orthogonormal basis of $\\langle d \\Psi e_1 , d \\Psi e_2 \\rangle$ at point $kh$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma(x+h) - 2\\gamma(x) + \\gamma(x-h)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_gamma_second(t, h,  x, y, decoder):\n",
    "    gamma_right = gamma(t+h, x, y, decoder)\n",
    "    gamma_left = gamma(t - h, x, y, decoder)\n",
    "    gamma_central = gamma(t, x, y, decoder)\n",
    "    return gamma_left + gamma_right - 2 * gamma_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite using delta_second of the gamma\n",
    "def A(x_i,x_j,n_partition, decoder):\n",
    "    # n_partition is number of points in partition for \n",
    "    # the integral approximation by its Riemann sum\n",
    "    n = n_partition\n",
    "    segment_partition = (1/(n-1))*torch.arange(n, dtype=torch.float32)\n",
    "    segment_partition_dim_d = segment_partition.repeat(d,1).T\n",
    "    alpha_array = x_j * segment_partition_dim_d + x_i * ( 1 - segment_partition_dim_d )\n",
    "\n",
    "    dPsi = torch.func.vmap( torch.func.jacfwd(decoder)) \n",
    "    Q,_ = dPsi(alpha_array).qr()\n",
    "    gamma_second_array = gamma_second (segment_partition_dim_d,\n",
    "                h = 1/(n - 1), x = x_i, y = x_j,\n",
    "                decoder = decoder)\n",
    "    # parallel multiplication of batches of matrices Q and vector gamma_second \n",
    "    # evaluated at intermediate points od segment partition\n",
    "    A_ij = (1 / ( n - 1 )) * ( torch.matmul(Q.transpose(-1,-2), gamma_second_array.unsqueeze(-1))**2  ).sum()\n",
    "    return A_ij.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def make_pairs(batch_of_points):\n",
    "    start_points_list = []\n",
    "    end_points_list = []\n",
    "    for i in range(K):\n",
    "        for j in range(i+1,K):\n",
    "            start_points_list.append(batch_of_points[i].unsqueeze(0))\n",
    "            end_points_list.append(batch_of_points[j].unsqueeze(0))\n",
    "    start_points = torch.cat(start_points_list, dim = 0)\n",
    "    end_points = torch.cat(end_points_list)\n",
    "    return start_points, end_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=1e-2\n",
    "(1/h**4)*delta_gamma_second(t=0,x=torch.tensor([[0., 0.]]), y =torch.tensor([[1., 1.]]), decoder=decoder, h=h).norm()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instable once h < 1.e-3\n",
    "gamma_second(t=0,x=torch.tensor([[0., 0.]]), y =torch.tensor([[1., 1.]]), decoder=decoder, h=h).norm()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : torch.sin(x)\n",
    "def ddf (x,h):\n",
    "    return f(x+h) - 2*f(x) +  f(x-h)\n",
    "def f_second (x,h):\n",
    "    return (f(x+h) - 2*f(x) +  f(x-h))/(h**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = torch.zeros(1) + 0.2\n",
    "print(f\"ddf({x_0.item()}) using h = 1.e-3\")\n",
    "print(ddf(x_0, h = 1.e-3).item())\n",
    "print(f\"ddf({x_0.item()}) using h = 1.e-5\")\n",
    "print(ddf(x_0, h = 1.e-5).item())\n",
    "\n",
    "print(f\"f''({x_0.item()}) using h = 1.e-3\")\n",
    "print(f_second(x_0, h = 1.e-3).item())\n",
    "print(f\"f''({x_0.item()}) uning h = 1.e-5\")\n",
    "print(f_second(x_0, h = 1.e-5).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the curves $\\alpha_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# define a square box where points are sampled\n",
    "square_side = 2*torch.pi\n",
    "center_of_square = torch.zeros(2)\n",
    "\n",
    "random_points_latent_space = square_side * ( torch.rand(K, 2) - 0.5 )  + center_of_square\n",
    "# Convert the tensor to a numpy array\n",
    "points_np = random_points_latent_space.numpy()\n",
    "\n",
    "# Plot the points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(points_np[:, 0], points_np[:, 1], c='blue', marker='o', edgecolor='k')\n",
    "plt.title('Random Points in Latent Space')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same Random points (uniformly distributed) not depending on the encoder:\n",
    "\n",
    "start_points_latent_space, end_points_latent_space = make_pairs(random_points_latent_space)\n",
    "# Random points depending on the encoder:\n",
    "#start_points, end_points = make_pairs(first_benchmark_data)\n",
    "#start_points_latent_space = encoder(start_points).detach()\n",
    "#end_points_latent_space = encoder(end_points).detach()\n",
    "\"\"\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(start_points_latent_space[:, 0], start_points_latent_space[:, 1], color='blue', label='Start Points')\n",
    "plt.scatter(end_points_latent_space[:, 0], end_points_latent_space[:, 1], color='red', label='End Points')\n",
    "for start, end in zip(start_points_latent_space, end_points_latent_space):\n",
    "    plt.plot([start[0], end[0]], [start[1], end[1]], 'k--', color='blue')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title(r'Lines $\\alpha_{ij}$ Connecting Points in the latent space')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vmap vectorization and computation of functionals $\\widetilde E$, $E$ and $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_tilde_vmap = torch.func.vmap(E_tilde)\n",
    "E_vmap = torch.func.vmap(E)\n",
    "A_vmap = torch.func.vmap(A)\n",
    "\n",
    "#computing Functiolnals \n",
    "n_partition = 100\n",
    "\n",
    "E_tilde_array = E_tilde_vmap (start_points_latent_space, end_points_latent_space, n_partition=n_partition, decoder = decoder)\n",
    "Energy_array = E_vmap (start_points_latent_space, end_points_latent_space, n_partition=n_partition, decoder = decoder)\n",
    "Acceleration_array = A_vmap (start_points_latent_space, end_points_latent_space, n_partition=n_partition, decoder = decoder)\n",
    "\n",
    "#E_tilde_array = E_tilde_vmap(encoder(start_points),encoder(end_points),n_partition=100,decoder = decoder)\n",
    "#Energy_array = E_vmap(encoder(start_points),encoder(end_points),n_partition=100,decoder = decoder)\n",
    "#Distance_ls_pairwize_array_no_curv_pen = (encoder(end_points) - encoder(start_points)).norm(dim=1)\n",
    "#Distance_RD_pairwize_array_no_curv_pen = (end_points - start_points).norm(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms and statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(samples, samples_name: str, n_bins = None, xlim = None, show_title_labels = False):\n",
    "    \n",
    "    samples = samples.detach()  # Detach from the computation graph\n",
    "    N = len(samples)  # Number of samples (straight lines)\n",
    "    if n_bins == None:\n",
    "        num_bins = 1 + math.ceil( math.log2(N) ) # Sturge's rule\n",
    "    else:\n",
    "        num_bins = n_bins\n",
    "    # num_bins = 30 was used before\n",
    "\n",
    "    # Create a histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(samples.detach(), bins=num_bins, edgecolor='black', alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    #plt.xlabel(f'${samples_name}_{{ij}}$ Values')\n",
    "    if show_title_labels:\n",
    "        plt.title(f'{samples_name} values')\n",
    "        plt.ylabel('Frequency')\n",
    "    if xlim != None:\n",
    "        plt.xlim(0, xlim)\n",
    "\n",
    "    # Show grid for better readability\n",
    "    plt.grid(True)\n",
    "    file_name = Path_pictures+f\"/Histogram_{samples_name}_ij_{experiment_name}.pdf\"\n",
    "    plt.savefig(file_name, bbox_inches='tight', format = \"pdf\")\n",
    "    print(\"Histogram saved at:\", file_name)\n",
    "    return plt\n",
    "\n",
    "def statistical_analysis(samples: torch.tensor, samples_name: str):\n",
    "    # Assuming the necessary imports and variable definitions are here\n",
    "    samples = samples.detach()  # Detach from the computation graph\n",
    "    N = len(samples)  # Number of samples (straight lines)\n",
    "    mean_value = samples.mean().item()  # Mean value of the energy functional\n",
    "    std_dev = torch.std(samples).item()  # Standard deviation\n",
    "    SE = std_dev / math.sqrt(N)  # Standard error (SE)\n",
    "    # printing here\n",
    "    print(\"Number of straight lines(samples):\", N)\n",
    "    print(f\"Mean value of {samples_name}: {mean_value:.3f}\")\n",
    "    print(f\"Std of {samples_name}: {std_dev:.3f}\")\n",
    "    print(f\"Standard error of mean (SE): {SE:.4f}\")\n",
    "    # Define the path to the output file\n",
    "    output_file_path = f\"{Path_pictures}/statistical_analysys_{samples_name}_{experiment_name}.txt\"\n",
    "\n",
    "    # Save the results to the text file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(f\"Number of straight lines (samples): {N}\\n\")\n",
    "        f.write(f\"Mean value of {samples_name}: {mean_value:.3f}\\n\")\n",
    "        f.write(f\"Standard deviation of {samples_name}: {std_dev:.3f}\\n\")\n",
    "        f.write(f\"Standard error of the mean (SE): {SE:.4f}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_file_path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionals $\\widetilde E_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_E_tilde = plot_histogram(samples=E_tilde_array, samples_name=\"E_tilde\",n_bins=30)\n",
    "p_E_tilde.show()\n",
    "statistical_analysis(E_tilde_array, \"E_tilde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionals $ E_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_E = plot_histogram(samples=Energy_array, samples_name=\"E\",n_bins=30)\n",
    "p_E.show()\n",
    "statistical_analysis(Energy_array, \"E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionals $ A_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_A = plot_histogram(samples=Acceleration_array, samples_name=\"A\", xlim=400e+3)\n",
    "p_A.show()\n",
    "statistical_analysis(Acceleration_array, \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy ratios: $ \\frac{\\int_0^1 \\| \\dot \\beta_{ij}(t)\\|_g^2 dt}{\\int_0^1 \\| \\dot \\gamma_{ij} (t)\\|_2^2 dt} \\ . $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A demo for two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stochman.manifold import EmbeddedManifold\n",
    "from stochman.curves import CubicSpline\n",
    "# geodesics are computed minimizing \"energy\" in the embedding of the manifold,\n",
    "# So no need to compute the Pullback metric. and thus the algorithm is fast\n",
    "# Define the embedding by the AE decoder\n",
    "class Autoencoder(EmbeddedManifold):\n",
    "    def embed(self, c, jacobian = False):\n",
    "        return torus_ae.decoder_torus(c)\n",
    "model = Autoencoder()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# for plotting\n",
    "t = torch.linspace(0.,1.,100)\n",
    "\n",
    "# p0 and p1 can be chosen anywhere on R^2 with 2\\pi periodic metric \n",
    "#p0 = torch.tensor([-2.,-2.]) #+11*torch.pi\n",
    "#p1 = torch.tensor([2.,2.]) #+ 11*torch.pi\n",
    "# take the first pair of points (of the pre-defined random points) \n",
    "p0 = start_points_latent_space[0]\n",
    "p1 = end_points_latent_space[0]\n",
    "print(f\"start point:{p0}, \\nend point: {p1}\")\n",
    "c, success = model.connecting_geodesic(p0, p1) # here the parameter t in c(t)should be a torch.tensor\n",
    "print(\"Success:\",success.item(),\"\\nGeodesic energy:\",model.curve_energy(c(t)).item())\n",
    "\n",
    "alpha = CubicSpline(p0, p1)\n",
    "\n",
    "print(\"Straight line energy:\",model.curve_energy(alpha(t)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the ratios on the same poins as for functionals $\\widetilde E$, $E$ and $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case it does not work try batches of smaller size num_of_points = 1000\n",
    "alpha_array = CubicSpline(start_points_latent_space, end_points_latent_space)\n",
    "alpha_energies_array = model.curve_energy(alpha_array(t),reduction=None) # straignt lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is costly\n",
    "beta_array,_ = model.connecting_geodesic(start_points_latent_space, end_points_latent_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_energies_array = model.curve_energy(beta_array(t),reduction=None) # geodesics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot_histogram(alpha_energies_array/beta_energies_array, samples_name=\"Naive_energy_ratio\")\n",
    "p.show()\n",
    "statistical_analysis(alpha_energies_array/beta_energies_array, samples_name=\"Naive_energy_ratio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here the mess starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with penalty on curvature\n",
    "load_weight_name = \"swissroll_curv_w=1_ls=R^2\"\n",
    "#load_weight_name = \"swissroll_curv_w=10_ls=R^2\"\n",
    "#load_weight_name = \"swissroll_curv_w=10_ls=R^2_20epochs_bs=32\"\n",
    "PATH_enc = f'../nn_weights/encoder_{load_weight_name}'\n",
    "encoder.load_state_dict(torch.load(PATH_enc))\n",
    "encoder.eval()\n",
    "PATH_dec = f'../nn_weights/decoder_{load_weight_name}'\n",
    "decoder.load_state_dict(torch.load(PATH_dec))\n",
    "decoder.eval()\n",
    "\n",
    "start_points, end_points = make_pairs()\n",
    "\n",
    "Energy_pairwise_array_with_curv_pen = E_vmap(encoder(start_points),encoder(end_points),n_partition=100)\n",
    "Distance_ls_pairwize_array_with_curv_pen = (encoder(end_points) - encoder(start_points)).norm(dim=1)\n",
    "Distance_RD_pairwize_array_with_curv_pen = (end_points - start_points).norm(dim=1)\n",
    "#Energy_pairwise_array_with_curv_pen = E_vmap(start_points,end_points,n_partition=100)\n",
    "#Distance_ls_pairwize_array_with_curv_pen = (end_points - start_points).norm(dim=1)\n",
    "#Distance_RD_pairwize_array_with_curv_pen = (decoder(end_points) - decoder(start_points)).norm(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with penalty on curvature\n",
    "\n",
    "load_weight_name = \"swissroll_curv_w=10_ls=R^2_20epochs_bs=32\"\n",
    "PATH_enc = f'../nn_weights/encoder_{load_weight_name}'\n",
    "encoder.load_state_dict(torch.load(PATH_enc))\n",
    "encoder.eval()\n",
    "PATH_dec = f'../nn_weights/decoder_{load_weight_name}'\n",
    "decoder.load_state_dict(torch.load(PATH_dec))\n",
    "decoder.eval()\n",
    "\n",
    "start_points, end_points = make_pairs()\n",
    "\n",
    "Energy_pairwise_array_curv_w10_pen = E_vmap(encoder(start_points),encoder(end_points),n_partition=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_RD_pairwize_array_with_curv_pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance_RD_pairwize_array_no_curv_pen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ground truth check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((Distance_RD_pairwize_array_no_curv_pen-Distance_RD_pairwize_array_with_curv_pen).detach(), bins = 50)\n",
    "plt.title(\"Distances in $\\mathbb{R}^D$ with and without curvature penalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((Distance_ls_pairwize_array_no_curv_pen-Distance_ls_pairwize_array_with_curv_pen).detach(), bins = 50)\n",
    "plt.title(\"Distances in latent space with and without curvature penalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 24})\n",
    "fig, ax = plt.subplots(figsize=(9,9),dpi = 300)\n",
    "#plt.title(f\"Swissroll: Energy of $C_{{{K}}}^2$ paths $E_{{ij}}$ vs distance in $\\mathbb{{R}}^3$\")\n",
    "ax.scatter(Distance_RD_pairwize_array_no_curv_pen.detach().numpy(),Energy_array.detach().numpy(),\n",
    "            color = \"red\", s = 10, label = \"$\\lambda_{curv} = 0$\")\n",
    "ax.scatter(Distance_RD_pairwize_array_with_curv_pen.detach().numpy(),Energy_pairwise_array_with_curv_pen.detach().numpy(),\n",
    "            color = \"blue\", s = 10, label = \"$\\lambda_{curv} = 1$\")\n",
    "ax.scatter(Distance_RD_pairwize_array_with_curv_pen.detach().numpy(),Energy_pairwise_array_curv_w10_pen.detach().numpy(),\n",
    "            color = \"green\", s = 10, label = \"$\\lambda_{curv} = 10$\")\n",
    "ax.legend(loc='upper left')\n",
    "#ax.set_xlabel('$\\|\\Psi \\circ \\Theta(X_i) - \\Psi \\circ \\Theta(X_j)\\|_2$, $\\Psi \\circ \\Theta(X_i)\\in \\mathbb{R}^3$')\n",
    "#ax.set_xlabel('$\\|X_i - X_j\\|_2$, $X_i \\in \\mathbb{R}^3$')\n",
    "ax.set_xlabel('$\\|X_i - X_j\\|_2$')\n",
    "ax.set_ylabel('$E_{ij}$')\n",
    "fig.savefig(\"Scatterplot_E_ij_dist_R3_swissroll.pdf\",bbox_inches='tight', format = \"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 24})\n",
    "fig, ax = plt.subplots(figsize=(9,9),dpi = 300)\n",
    "#plt.title(f\"Swissroll: enegry of paths change $E_{{ij}}^{{\\lambda_{{curv}} = 1}} - E_{{ij}}^{{\\lambda_{{curv}} = 0}}$ \\n vs distance in $\\mathbb{{R}}^3$ for $C_{{{K}}}^2$ paths\")\n",
    "ax.scatter(Distance_RD_pairwize_array_with_curv_pen,\n",
    "           (Energy_pairwise_array_with_curv_pen - Energy_array).detach().numpy(),\n",
    "            color = \"magenta\", s = 10)\n",
    "#ax.set_xlabel('$\\|\\Psi \\circ \\Theta(X_i) - \\Psi \\circ \\Theta(X_j)\\|_2$, $\\Psi \\circ \\Theta(X_i)\\in \\mathbb{R}^3$')\n",
    "#ax.set_xlabel('$\\|X_i - X_j\\|_2$, $X_i \\in \\mathbb{R}^3$')\n",
    "ax.set_xlabel('$\\|X_i - X_j\\|_2$')\n",
    "ax.set_ylabel('Change in $E_{ij}$')\n",
    "#ax.set_ylabel('$E_{ij}^{\\lambda_{curv} = 1} - E_{ij}^{\\lambda_{curv} = 0}$')\n",
    "fig.savefig(\"Scatterplot_change_E_ij_dist_R3_swissroll.pdf\",bbox_inches='tight', format = \"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "fig, ax = plt.subplots(figsize=(9,9),dpi = 300)\n",
    "plt.title(f\"Swissroll: Energy of $C_{{{K}}}^2$ paths $E_{{ij}}$ vs distance in $\\mathbb{{R}}^3$\")\n",
    "ax.scatter(Distance_RD_pairwize_array_no_curv_pen.detach().numpy(),Energy_array.detach().numpy(),\n",
    "            color = \"red\", s = 10, label = \"$\\lambda_{curv} = 0$\")\n",
    "ax.scatter(Distance_RD_pairwize_array_with_curv_pen.detach().numpy(),Energy_pairwise_array_with_curv_pen.detach().numpy(),\n",
    "            color = \"blue\", s = 10, label = \"$\\lambda_{curv} = 1$\")\n",
    "ax.legend(loc='upper left')\n",
    "#ax.set_xlabel('$\\|\\Psi \\circ \\Theta(X_i) - \\Psi \\circ \\Theta(X_j)\\|_2$, $\\Psi \\circ \\Theta(X_i)\\in \\mathbb{R}^3$')\n",
    "ax.set_xlabel('$\\|X_i - X_j\\|_2$, $X_i \\in \\mathbb{R}^3$')\n",
    "ax.set_ylabel('$E_{ij}$')\n",
    "#ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "fig.savefig(\"Scatterplot_E_ij_dist_R3_swissroll_ylogscale.pdf\", format = \"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "fig, ax = plt.subplots(figsize=(9,9),dpi = 300)\n",
    "plt.title(f\"Swissroll: Energy of $C_{{{K}}}^2$ paths $E_{{ij}}$ vs distance in latent space\")\n",
    "ax.scatter(Distance_ls_pairwize_array_no_curv_pen.detach().numpy(),Energy_array.detach().numpy(),\n",
    "            color = \"red\", s = 10, label = \"$\\lambda_{curv} = 0$\")\n",
    "ax.scatter(Distance_ls_pairwize_array_with_curv_pen.detach().numpy(),Energy_pairwise_array_with_curv_pen.detach().numpy(),\n",
    "            color = \"blue\", s = 10, label = \"$\\lambda_{curv} = 1$\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('$\\|\\Theta(X_i) - \\Theta(X_j)\\|_2$, $\\Theta(X_i) \\in \\mathbb{R}^2$')\n",
    "ax.set_ylabel('$E_{ij}$')\n",
    "fig.savefig(\"Scatterplot_E_ij_dist_ls_swissroll.pdf\", format = \"pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
