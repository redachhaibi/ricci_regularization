{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Hyperpameters: set and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "# adding path to the set generating package\n",
    "import sys\n",
    "sys.path.append('../') # have to go 1 level up\n",
    "import ricci_regularization as RR\n",
    "\n",
    "# Minimal imports\n",
    "from torch import nn\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOD_regime = True\n",
    "compute_curvature = True\n",
    "batch_size  = 64 # was 32 initially\n",
    "# curvature params\n",
    "mse_w = 1.0\n",
    "curv_w = 10.0 #weight on curvature\n",
    "start_curv = 0 # batch index to strart curvature computation from\n",
    "\n",
    "klw = 0 # AE mode off\n",
    "\n",
    "# Training and plotting params\n",
    "lr         = 4e-5 #initially 4e-5 for synthetic, 1e-4 for swissroll (or 2e-5 with curvature on)\n",
    "momentum   = 0.8 #initially 0.8\n",
    "batches_per_plot = 400 #initially 200 \n",
    "split_ratio = 0.2\n",
    "num_epochs = 200 \n",
    "\n",
    "random_state = 1\n",
    "seed_number = 0\n",
    "\n",
    "# plot saving params\n",
    "violent_saving = False # if False it will not save plots\n",
    "\n",
    "#experiment_name = \"swissroll_OOD_curv_w=10_sigma_ood=0.2_T_OOD=20_OOD_w=10_20epochs\"\n",
    "experiment_name = \"MNIST\"\n",
    "#experiment_name = \"synthetic_curv_w=1e+1_ls=R^2+OOD\"\n",
    "#experiment_name = f\"swissroll_rs={random_state}_curv_w=100_lr=4e-6\"\n",
    "#experiment_name = \"swissroll_curv_func_oscilations_curv_w=10_eps=0.01\"\n",
    "#experiment_name = \"current experiment\"\n",
    "\n",
    "\n",
    "weights_loaded = False\n",
    "model_weights_saved = True\n",
    "load_weight_name = \"synthetic_curv_w=1e+1_ls=R^2\"\n",
    "#load_weight_name = \"synthetic_curv_w=1e+1_ls=R^2\"\n",
    "save_weight_name = experiment_name\n",
    "#save_weight_name = \"swissroll_curv_w=1+OOD\"\n",
    "\n",
    "# here you can choose a path for saving the pictures\n",
    "if violent_saving == True: \n",
    "    Path_pictures = f\"/home/alazarev/CodeProjects/Experiments/{experiment_name}\"\n",
    "    if os.path.exists(Path_pictures) == False:\n",
    "        os.mkdir(Path_pictures) # needs to be commented once the folder for plots is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for dataset\n",
    "#set_name = \"Synthetic\"\n",
    "#where_to_compute_curv = \"random\"\n",
    "where_to_compute_curv = \"batch\"\n",
    "\n",
    "set_name = \"Swissroll\"\n",
    "sr_noise = 0.05\n",
    "sr_numpoints = 18000 #k*n\n",
    "\n",
    "#set_name = \"MNIST\"\n",
    "\n",
    "#D = 784       #dimension\n",
    "#D = 2\n",
    "k = 3         # num of 2d planes in dim D\n",
    "if set_name == \"Swissroll\":\n",
    "    D = 3 # for swissroll\n",
    "elif set_name == \"Synthetic\":\n",
    "    D = 784\n",
    "elif set_name == \"MNIST\":\n",
    "    D = 784\n",
    "    k = 10\n",
    "d = 2         # latent space dimension\n",
    "\n",
    "n = 6*(10**3) # num of points in each plane\n",
    "shift_class = 0\n",
    "var_class = 1 # variation of each Gaussian initially 0.1\n",
    "intercl_var = 0.1 # this creates a Gaussian, \n",
    "# i.e.random shift \n",
    "# proportional to the value of intercl_var\n",
    "# initially 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD sampling parameters\n",
    "T_ood = 20 # 100 # period of OOD penalization\n",
    "n_ood = 5 # number of OOD samples per point\n",
    "sigma_ood = 5e-1 # sigma of OOD Gaussian samples: 2e-1 swissroll\n",
    "N_extr = 16 # 32 batch size of extremal curvature points\n",
    "r_ood = 1e-3 # 1e-2 decay factor\n",
    "OOD_w = curv_w\n",
    "start_ood = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I*. Choosing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_name == \"Synthetic\":\n",
    "    # Generate dataset \n",
    "\n",
    "    # old style\n",
    "    # train_dataset = ricci_regularization.generate_dataset(D, k, n, shift_class=shift_class, intercl_var=intercl_var)\n",
    "\n",
    "    # via classes\n",
    "    #torch.manual_seed(0) # reproducibility\n",
    "    my_dataset = RR.SyntheticDataset(k=k,n=n,d=d,D=D,\n",
    "                                        shift_class=shift_class,\n",
    "                                        var_class = var_class, \n",
    "                                        intercl_var=intercl_var)\n",
    "\n",
    "    train_dataset = my_dataset.create\n",
    "elif set_name == \"Swissroll\":\n",
    "    train_dataset =  sklearn.datasets.make_swiss_roll(n_samples=sr_numpoints, noise=sr_noise, random_state = random_state) #random_state=1\n",
    "    sr_points = torch.from_numpy(train_dataset[0]).to(torch.float32)\n",
    "    #sr_points = torch.cat((sr_points,torch.zeros(sr_numpoints,D-3)),dim=1)\n",
    "    sr_colors = torch.from_numpy(train_dataset[1]).to(torch.float32)\n",
    "    from torch.utils.data import TensorDataset\n",
    "    train_dataset = TensorDataset(sr_points,sr_colors)\n",
    "elif set_name == \"MNIST\":\n",
    "    # MNIST Dataset\n",
    "    #train_dataset = datasets.MNIST(root='../datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    #test_dataset  = datasets.MNIST(root='../datasets/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    #train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    #test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    train_dataset = datasets.MNIST(root='../datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset  = torchvision.datasets.MNIST(root='../datasets/', train=False, download=True)\n",
    "\n",
    "m = len(train_dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(train_dataset, [int(m-m*split_ratio), int(m*split_ratio)])\n",
    "test_loader  = torch.utils.data.DataLoader(test_data , batch_size=batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# test_data[:][0] will give the vectors of data without labels from the test part of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. AE declatation and initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "cuda_on = torch.cuda.is_available()\n",
    "if cuda_on:\n",
    "    device  = torch.device(\"cuda\") \n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sine AE ls = R^2\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 128)\n",
    "        self.linear4 = nn.Linear(128, hidden_dim)\n",
    "        #self.activation = nn.ReLU()\n",
    "        self.activation = torch.sin\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear3(y)\n",
    "        y = self.activation(y)\n",
    "        out = self.linear4(y)\n",
    "        #out = self.activation(out)\n",
    "        return out\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 512)\n",
    "        self.linear4 = nn.Linear(512, output_dim)\n",
    "        self.activation = torch.sin\n",
    "        #self.activation = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear3(y)\n",
    "        y = self.activation(y)\n",
    "        out = self.linear4(y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, cuda=True):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.linear2 = nn.Linear(512, hidden_dim)\n",
    "        self.linear3 = nn.Linear(512, hidden_dim)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        if cuda:\n",
    "            self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "            self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = torch.sin(self.linear1(x)) \n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "current_time = time.monotonic_ns()\n",
    "torch.manual_seed(seed_number)\n",
    "#torch.manual_seed(current_time)\n",
    "#print(\"manual seed:\", current_time)\n",
    "# initially D=784, d=2\n",
    "# AE/VAE switch\n",
    "if klw > 0:\n",
    "    encoder = VariationalEncoder(input_dim=784, hidden_dim=d, cuda=cuda_on)\n",
    "else:\n",
    "    encoder = Encoder(input_dim=D, hidden_dim=d)\n",
    "decoder = Decoder(hidden_dim=d, output_dim=D)\n",
    "\n",
    "# ClassicalAE\n",
    "#encoder = Encoder(input_dim=D, hidden_dim=d)\n",
    "#decoder = Decoder(hidden_dim=d, output_dim=D)\n",
    "\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "#optimizer = torch.optim.Adam(params_to_optimize, lr=lr,weight_decay=0.0)\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.RMSprop(params_to_optimize, lr=lr, momentum=momentum, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weights_loaded==True:\n",
    "    PATH_enc = f'../nn_weights/encoder_{load_weight_name}'\n",
    "    encoder.load_state_dict(torch.load(PATH_enc))\n",
    "    encoder.eval()\n",
    "    PATH_dec = f'../nn_weights/decoder_{load_weight_name}'\n",
    "    decoder.load_state_dict(torch.load(PATH_dec))\n",
    "    decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choice of curvature functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Func(encoded_data):\n",
    "    metric_on_data = RR.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=decoder)\n",
    "    det_on_data = torch.det(metric_on_data)\n",
    "    Sc_on_data = RR.Sc_jacfwd_vmap(encoded_data,\n",
    "                                           function=decoder)\n",
    "    N = metric_on_data.shape[0]\n",
    "    Integral_of_Sc = (1/N)*(torch.sqrt(det_on_data)*torch.square(Sc_on_data)).sum()\n",
    "    return Integral_of_Sc\n",
    "\"\"\"\n",
    "# minimizing |g-I|_F\n",
    "def Func(encoded_data):\n",
    "    metric_on_data = RR.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=decoder)\n",
    "    N = metric_on_data.shape[0]\n",
    "    func = (1/N)*(metric_on_data-torch.eye(d)).norm(dim=(1,2)).sum()\n",
    "    return func\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Plotting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from https://gist.github.com/jakevdp/91077b0cae40f8f8244a\n",
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\"\"\"\n",
    "\n",
    "    # Note that if base_cmap is a string or None, you can simply do\n",
    "    return plt.cm.get_cmap(base_cmap, N)\n",
    "    # The following works for string, None, or a colormap instance:\n",
    "\"\"\"\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_plot(encoder, data, batch_idx, show_title = True, colormap = 'jet',s=40,draw_grid = True,figsize = (8, 6)):\n",
    "\n",
    "    if set_name == \"MNIST\":\n",
    "        data = test_data.dataset.test_data.to(torch.float32)\n",
    "        labels = test_data.dataset.targets\n",
    "    else:\n",
    "        labels = data[:][1]\n",
    "        data   = data[:][0]\n",
    "\n",
    "    # Encode\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.view(-1,D) # reshape the img\n",
    "        data = data.to(device)\n",
    "        encoded_data = encoder(data)\n",
    "\n",
    "    # Record codes\n",
    "    latent = encoded_data.cpu().numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if set_name == \"Swissroll\":\n",
    "        plt.scatter( latent[:,0], latent[:,1],s=s, c=labels, alpha=0.5, marker='o', edgecolor='none', cmap=colormap)\n",
    "    else:\n",
    "        plt.scatter( latent[:,0], latent[:,1],s=s, c=labels, alpha=0.5, marker='o', edgecolor='none', cmap=discrete_cmap(k, colormap))\n",
    "        #plt.scatter( latent[:,0], latent[:,1], c=labels, alpha=0.5, marker='o')\n",
    "        plt.colorbar(ticks=range(k),orientation='vertical',shrink = 0.7)\n",
    "    if show_title == True:\n",
    "        plt.title( f'''Latent space for test data in AE at batch {batch_idx}''')\n",
    "    axes = plt.gca()\n",
    "    plt.grid(draw_grid)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3losses(mse_train_list,curv_train_list,g_inv_train_list):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(6,18))\n",
    "    \n",
    "    axes[0].semilogy(mse_train_list, color = 'tab:red')\n",
    "    axes[0].set_ylabel('MSE')\n",
    "    \n",
    "    axes[1].semilogy(curv_train_list, color = 'tab:olive')\n",
    "    axes[1].set_ylabel('Curvature')\n",
    "    \n",
    "    axes[2].semilogy(g_inv_train_list, color = 'tab:blue')\n",
    "    axes[2].set_ylabel('$\\|G^{-1}\\|_F$')\n",
    "    for i in range(3):\n",
    "        axes[i].set_xlabel('Batches')\n",
    "    fig.show()\n",
    "    return fig,axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches per epoch\n",
    "print( \"Reality check of batch splitting: \")\n",
    "print( \"-- Batches per epoch\", len(train_loader) )\n",
    "print( \"batch size:\", batch_size )\n",
    "print( \"product: \", len(train_loader)*batch_size )\n",
    "print( \"-- To be compared to:\", (1.0-split_ratio)*n*k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches per epoch\n",
    "print( \"Reality check of batch splitting: \")\n",
    "print( \"-- Batches per epoch\", len(test_loader) )\n",
    "print( \"batch size:\", batch_size )\n",
    "print( \"product: \", len(test_loader)*batch_size )\n",
    "print( \"-- To be compared to:\", split_ratio*n*k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 0 # to enumerate the plots\n",
    "batch_idx = 0\n",
    "batches_per_epoch = len(train_loader)\n",
    "\n",
    "#extreme_curv_points_list = []\n",
    "\n",
    "mse_loss = []\n",
    "kl_loss = []\n",
    "curv_loss = []\n",
    "test_mse_loss_list = []\n",
    "test_curv_loss_list = []\n",
    "g_inv_norm_mean_train_list = []\n",
    "\n",
    "# initialize extreme curvature tensor\n",
    "with torch.no_grad():\n",
    "   extreme_curv_points_tensor = torch.rand(N_extr, d)\n",
    "   extreme_curv_value_tensor = torch.zeros(N_extr)\n",
    "         \n",
    "\n",
    "# to iterate though the batches of test data \n",
    "# simoultanuousely with train data\n",
    "iter_test_loader = iter(test_loader)\n",
    "      \n",
    "for epoch in range(num_epochs):\n",
    "   #if weights_loaded == True:\n",
    "   #   break\n",
    "   # Set train mode for both the encoder and the decoder\n",
    "   encoder.train()\n",
    "   decoder.train()\n",
    "   \n",
    "   \n",
    "   # Iterate the dataloader: no need  for the label\n",
    "   # values, this is unsupervised learning\n",
    "   for image_batch, _ in train_loader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "      #shaping the images properly\n",
    "      image_batch = image_batch.view(-1,D)\n",
    "      # Move tensor to the proper device\n",
    "      image_batch = image_batch.to(device)\n",
    "      # True batch size\n",
    "      true_batch_size = image_batch.shape[0]\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Front-propagation\n",
    "      # -- Encode data\n",
    "      encoded_data = encoder(image_batch)\n",
    "      # -- Decode data\n",
    "      decoded_data = decoder(encoded_data)\n",
    "      # --Evaluate loss\n",
    "      mse_loss_batch = torch.sum( (decoded_data-image_batch)**2 )/true_batch_size\n",
    "      \n",
    "      if OOD_regime == True:\n",
    "         # Exrteme curvature batch\n",
    "         new_curv_points_tensor = encoded_data\n",
    "         #print(new_curv_points_tensor.shape)\n",
    "         new_curv_value_tensor = RR.Sc_jacfwd_vmap(new_curv_points_tensor,\n",
    "                                                         function = decoder)\n",
    "      #print(new_curv_value_tensor.shape)\n",
    "\n",
    "      if compute_curvature == True:\n",
    "         if where_to_compute_curv == \"batch\":\n",
    "            curvature_train_batch = Func(encoded_data)\n",
    "            g_inv_train_batch = torch.linalg.inv(RR.metric_jacfwd_vmap(encoded_data,function=decoder))\n",
    "            g_inv_norm_train_batch = torch.linalg.matrix_norm(g_inv_train_batch)\n",
    "            g_inv_norm_mean_train_batch = torch.mean(g_inv_norm_train_batch)\n",
    "            #volume_form = torch.sqrt(torch.det(RR.metric_jacfwd_vmap(encoded_data, function=decoder)))\n",
    "            #curvature_train_batch = (1/true_batch_size)*(torch.square(new_curv_value_tensor)*volume_form).sum()\n",
    "         elif where_to_compute_curv == \"random\":\n",
    "            curvature_train_batch = Func(2*torch.rand(batch_size,2)-1)\n",
    "      else:\n",
    "         curvature_train_batch = 0.0\n",
    "      #if batch_idx < start_curv:\n",
    "      #   loss = mse_w*mse_loss_batch\n",
    "      #else:\n",
    "      loss = mse_w*mse_loss_batch + curv_w*curvature_train_batch\n",
    "      if OOD_regime == True:\n",
    "         with torch.no_grad():\n",
    "            # merge extreme points and new batch \n",
    "            extreme_curv_points_tensor = torch.cat((extreme_curv_points_tensor,\n",
    "                                                   new_curv_points_tensor),dim=0)\n",
    "            #print(extreme_curv_points_tensor.shape)\n",
    "            extreme_curv_value_tensor = torch.cat((extreme_curv_value_tensor,\n",
    "                                                   new_curv_value_tensor),dim=0)\n",
    "\n",
    "            #print(extreme_curv_value_tensor.shape)\n",
    "\n",
    "            # sort by curvature value points and curvature values. \n",
    "            indices = torch.argsort(extreme_curv_value_tensor)\n",
    "            #print(indices)\n",
    "            extreme_curv_points_tensor = torch.index_select(extreme_curv_points_tensor,dim = 0, index= indices)\n",
    "            extreme_curv_value_tensor = torch.index_select(extreme_curv_value_tensor,dim = 0, index= indices)\n",
    "            #print(extreme_curv_value_tensor)\n",
    "            # take most N_extr//2 negative and N_extr//2 most positive\n",
    "            extreme_curv_points_tensor = torch.cat((extreme_curv_points_tensor[:N_extr//2],extreme_curv_points_tensor[-N_extr//2:]),dim=0)\n",
    "            extreme_curv_value_tensor = torch.cat((extreme_curv_value_tensor[:N_extr//2],extreme_curv_value_tensor[-N_extr//2:]),dim=0)\n",
    "            #print(\"\\nwe keep\",extreme_curv_value_tensor.shape[0],\n",
    "            #      \"extreme curvature values:\\n\",extreme_curv_value_tensor,\n",
    "            #      \"\\nat points:\\n\", extreme_curv_points_tensor)\n",
    "            # multiply curv values by decay factor\n",
    "            extreme_curv_value_tensor = math.exp(-r_ood)*extreme_curv_value_tensor\n",
    "            # if not enough points, keep 16 of each (min and max) anyway      \n",
    "            # but when OOD sampling, sample around min negative\n",
    "            # and max positive. Print how many of each are used!\n",
    "         # end with\n",
    "         \n",
    "      # OOD sampling\n",
    "         if (batch_idx % T_ood == 0) & (batch_idx > start_ood):\n",
    "            #optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "               #centers = extreme_curv_points_tensor.repeat(n_ood,1)\n",
    "               centers = extreme_curv_points_tensor.repeat_interleave(n_ood,dim=0)\n",
    "               samples_centered_at_zero = (sigma_ood**2)*torch.randn(N_extr*n_ood, d)\n",
    "               OOD_batch = centers + samples_centered_at_zero\n",
    "            OOD_batch.requires_grad_()\n",
    "            #print(OOD_batch.shape)\n",
    "            #plt.scatter(OOD_batch.detach()[:,0], OOD_batch.detach()[:,1])\n",
    "            #plt.show()\n",
    "            # OOD change of loss function \n",
    "            Func_val = Func(OOD_batch)\n",
    "            #OOD_w = mse_loss_batch/Func_val\n",
    "            print(\"Func of OOD points\", Func_val)\n",
    "            #loss = OOD_w* Func(OOD_batch)\n",
    "            loss = curv_w * Func_val\n",
    "         # end if   \n",
    "      \n",
    "      # if VAE mode is on\n",
    "      if klw > 0:\n",
    "          kl_loss_batch = encoder.kl \n",
    "          loss += klw*kl_loss_batch\n",
    "          kl_loss.append(kl_loss_batch.data)\n",
    "      else:\n",
    "          kl_loss_batch = 0.0\n",
    "\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      # Print batch loss\n",
    "      #print('\\t MSE loss per batch (single batch): %f' % (mse_loss_batch.data))\n",
    "      #print('\\t Total loss per batch (single batch): %f' % (loss.data))\n",
    "\n",
    "      if batch_idx % len(test_loader) == 0:\n",
    "         iter_test_loader = iter(test_loader)\n",
    "      test_images = next(iter_test_loader)[0].view(-1,D).to(device)\n",
    "      encoded_test_data = encoder(test_images)\n",
    "      decoded_test_data = decoder(encoded_test_data)\n",
    "\n",
    "      # True test_batch size\n",
    "      true_test_batch_size = test_images.shape[0]\n",
    "      with torch.no_grad():\n",
    "         test_mse_loss = torch.sum( (decoded_test_data - test_images)**2 )/true_test_batch_size\n",
    "         test_mse_loss_list.append(test_mse_loss.detach().cpu().numpy())\n",
    "         if compute_curvature == True:\n",
    "            test_curv_loss = Func(encoded_test_data)\n",
    "            test_curv_loss_list.append(test_curv_loss.detach().cpu().numpy())\n",
    "         # end if\n",
    "      # end with\n",
    "      #print('\\t test MSE loss per batch (single batch): %f' % (test_mse_loss.data))\n",
    "      #print('\\t partial train loss (single batch): {:.6} \\t curv_loss {:.6} \\t mse {:.6}'.format(loss.data, new_loss, only_mse.data))\n",
    "      \n",
    "      mse_loss.append(float(mse_loss_batch.detach().cpu().numpy()))\n",
    "      if compute_curvature == True:\n",
    "         curv_loss.append(float(curvature_train_batch.detach().cpu().numpy()))\n",
    "         g_inv_norm_mean_train_list.append(g_inv_norm_mean_train_batch.item())\n",
    "\n",
    "      # Plot and compute test loss      \n",
    "\n",
    "      if (batch_idx % batches_per_plot == 0):\n",
    "         #test loss\n",
    "\n",
    "         #plotting\n",
    "         plot = point_plot(encoder, test_data, batch_idx)\n",
    "         if violent_saving == True:\n",
    "            plot.savefig('../plots/pointplots_in_training_testdata/pp{0}.eps'.format(num_plots),format='eps')\n",
    "         num_plots += 1\n",
    "         plot.show()\n",
    "\n",
    "         # plotting losses\n",
    "         if batch_idx>0:\n",
    "            if compute_curvature==True:\n",
    "               plot3losses(mse_loss,curv_loss,g_inv_norm_mean_train_list)\n",
    "            else:\n",
    "               fig, ax1 = plt.subplots()\n",
    "\n",
    "               ax1.set_xlabel('Batches')\n",
    "               ax1.set_ylabel('MSE')\n",
    "               ax1.semilogy(mse_loss, label='train_MSE_loss', color='tab:orange')\n",
    "               ax1.semilogy(test_mse_loss_list, label='test_MSE_loss', color='tab:red')\n",
    "               \n",
    "               ax1.tick_params(axis='y')\n",
    "               plt.legend(loc='lower left')\n",
    "               fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "               plt.show()\n",
    "         \"\"\"\n",
    "         if batch_idx>0:\n",
    "            fig, ax1 = plt.subplots()\n",
    "\n",
    "            ax1.set_xlabel('Batches')\n",
    "            ax1.set_ylabel('MSE')\n",
    "            ax1.semilogy(mse_loss, label='train_MSE_loss', color='tab:orange')\n",
    "            ax1.semilogy(test_mse_loss_list, label='test_MSE_loss', color='tab:red')\n",
    "            \n",
    "            ax1.tick_params(axis='y')\n",
    "            plt.legend(loc='lower left')\n",
    "\n",
    "            if compute_curvature == True:\n",
    "\n",
    "               ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "               ax2.set_ylabel('Curvature')  # we already handled the x-label with ax1\n",
    "               ax2.semilogy(curv_loss, label='train_Curv_loss',color='tab:olive')\n",
    "               ax2.semilogy(test_curv_loss_list, label='test_Curv_loss', color='tab:green')\n",
    "               \n",
    "               ax2.tick_params(axis='y')\n",
    "               plt.legend(loc='lower right')\n",
    "            # end if\n",
    "            fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "            plt.show()\n",
    "            # end if\n",
    "         # end if\n",
    "      # end if\n",
    "       \"\"\"\n",
    "      batch_idx += 1\n",
    "   # end for\n",
    "   #print('\\n EPOCH {}/{}. \\t Average values over epoch:\\n MSE loss: {}, Curvature loss: {}'.format(epoch + 1, num_epochs, np.mean(mse_loss[-batches_per_epoch:]),np.mean(curv_loss[-batches_per_epoch:])))\n",
    "   print(f'\\n EPOCH {epoch + 1}/{num_epochs}. \\t Average values over epoch:\\n MSE loss: {np.mean(mse_loss[-batches_per_epoch:])}, Curvature loss: {np.mean(curv_loss[-batches_per_epoch:])}, KL loss: {np.mean(kl_loss[-batches_per_epoch:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ae_outputs(encoder,decoder,n=10):\n",
    "    plt.figure(figsize=(16,4.5))\n",
    "    targets = test_dataset.targets.numpy()\n",
    "    t_idx = {i:np.where(targets==i)[0][0] for i in range(n)}\n",
    "    for i in range(n):\n",
    "      ax = plt.subplot(2,n,i+1)\n",
    "      img = test_dataset[t_idx[i]][0].unsqueeze(0).to(device)\n",
    "      encoder.eval()\n",
    "      decoder.eval()\n",
    "      with torch.no_grad():\n",
    "         rec_img  = decoder(encoder(img))\n",
    "      plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(2, n, i + 1 + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_saved = True\n",
    "\n",
    "if (model_weights_saved == True):\n",
    "    PATH_enc = f'../nn_weights/encoder_{save_weight_name}'\n",
    "    print( \"Saving encoder weights to\", PATH_enc)\n",
    "    torch.save(encoder.state_dict(), PATH_enc)\n",
    "    PATH_dec = f'../nn_weights/decoder_{save_weight_name}'\n",
    "    print( \"Saving decoder weights to\", PATH_dec)\n",
    "    torch.save(decoder.state_dict(), PATH_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### determination coefficient and training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_name == \"Swissroll\":\n",
    "    points_tensor = torch.tensor(sr_points)\n",
    "    cov_matrix = torch.cov(points_tensor.T)\n",
    "    print(\"Covariance matrix:\\n\", cov_matrix)\n",
    "    mean = points_tensor.mean(dim=0)\n",
    "    print(\"Mean vector:\", mean)\n",
    "    R_squared = 1 - ((decoder(encoder(sr_points)).data-sr_points).norm()**2)/((sr_points-mean).norm()**2)\n",
    "    print(\"Determination coef R^2:\", R_squared)\n",
    "    MSE = (1/sr_numpoints)*(decoder(encoder(sr_points)).data-sr_points).norm()**2\n",
    "    print(\"MSE:\", MSE)\n",
    "    print(\"tr(Q):\", cov_matrix.trace())\n",
    "    print(\"MSE/tr(Q):\", MSE/cov_matrix.trace())\n",
    "    print(\"R^2 = 1-MSE/tr(Q):\", 1-MSE/cov_matrix.trace())\n",
    "elif set_name ==\"Synthetic\":\n",
    "    points_tensor = train_dataset[:][0]\n",
    "    cov_matrix = torch.cov(points_tensor.T)\n",
    "    #print(\"Covariance matrix:\\n\", cov_matrix)\n",
    "    mean = points_tensor.mean(dim=0)\n",
    "    #print(\"Mean vector:\", mean)\n",
    "    R_squared = 1 - ((decoder(encoder(points_tensor)).data-train_dataset[:][0]).norm()**2)/((train_dataset[:][0]-mean).norm()**2)\n",
    "    print(\"Determination coef R^2:\", R_squared)\n",
    "elif set_name == \"MNIST\":\n",
    "    input = test_data.dataset.data.to(torch.float32).view(-1,D)\n",
    "    target = decoder(encoder(test_data.dataset.data.to(torch.float32).view(-1,D)))\n",
    "    #mean = input.mean(dim=0)\n",
    "    mse = F.mse_loss(input, target, reduction=\"mean\")\n",
    "    cov_matrix = torch.cov(input.T)\n",
    "    R_squared = 1 - mse/cov_matrix.trace()\n",
    "    print(\"Determination coef R^2:\", R_squared.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torcheval.metrics import R2Score\n",
    "#metric = R2Score()\n",
    "#input = torch.tensor([[0, 2], [1, 6]])\n",
    "#target = torch.tensor([[0, 1], [2, 5]])\n",
    "#metric.update(input[:2], target[:2])\n",
    "#metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curv_loss_on_train_data = Func(encoder(train_data[:][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 17}) # makes all fonts on the plot be 24\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Batches')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.semilogy(mse_loss, label='train_MSE_loss', color='tab:orange')\n",
    "ax1.semilogy(test_mse_loss_list, label='test_MSE_loss', color='tab:red')\n",
    "\n",
    "ax1.tick_params(axis='y')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "if compute_curvature == True:\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    #ax2.set_ylabel('G-I loss')\n",
    "    ax2.set_ylabel('Curvature')  # we already handled the x-label with ax1\n",
    "    ax2.semilogy(curv_loss, label='train_Curv_loss',color='tab:olive')\n",
    "    ax2.semilogy(test_curv_loss_list, label='test_Curv_loss', color='tab:green')\n",
    "    ax2.tick_params(axis='y')\n",
    "    plt.legend(loc='upper right')\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#fig.text(0.05,-0.25,f'The dataset has {k*n} points originated \\nby {k} Gaussian(s). \\nAverage losses over the last epoch: \\nMSE loss: {np.mean(mse_loss[-batches_per_epoch:]):.3f}, \\nCurvature loss: {np.mean(curv_loss[-batches_per_epoch:]):.3f}')\n",
    "# Average losses over the last epoch\n",
    "if set_name == \"Swissroll\":\n",
    "    fig.text(0.05,-0.25,f\"Set params: n={sr_numpoints}, noise={sr_noise}. \\nMSE loss: {np.mean(mse_loss[-batches_per_epoch:]):.3f}, \\nCurvature loss: {np.mean(curv_loss[-batches_per_epoch:]):.3f}, \\n$R^2=${R_squared:.4f}\")\n",
    "else:    \n",
    "    fig.text(0.05,-0.25,f\"Set params: n={n}, k={k}, d={d}, D={D}, $\\sigma$={var_class}, $\\sigma_{{I}}$={intercl_var}. \\nMSE loss: {np.mean(mse_loss[-batches_per_epoch:]):.3f}, \\nCurvature loss: {np.mean(curv_loss[-batches_per_epoch:]):.3f}, \\n $R^2=${R_squared:.4f}, \\n Curvature loss over train dataset: {curv_loss_on_train_data:.4f}\" )\n",
    "if OOD_regime == True:\n",
    "    fig.text(0.05, -0.35, f\"OOD params: T_ood = {T_ood}, n_ood = 5, OOD_w={OOD_w}, \\nsigma_ood = {sigma_ood}, N_extr = {N_extr}, r_ood = {r_ood}\")\n",
    "str_lambda_recon = \"$\\lambda_{recon}$\"\n",
    "str_lambda_curv = \"$\\lambda_{curv}$\"\n",
    "plt.title(f\"Params: lr={lr}, batch_size={batch_size},\\n {str_lambda_recon}={mse_w}, {str_lambda_curv}={curv_w}\")\n",
    "#plt.title(\"Params: lr={0}, batch_size={1},\\n $\\lambda_r$={2}, $\\lambda_c$={3},{4}\".format(lr,batch_size,mse_w,curv_w,str1))\n",
    "#violent_saving = True\n",
    "if (violent_saving == True): \n",
    "#& (weights_loaded == False):\n",
    "    plt.savefig(f'{Path_pictures}/losses.pdf',bbox_inches='tight',format='pdf')\n",
    "#plt.savefig(f'{Path_pictures}/losses.pdf',bbox_inches='tight',format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy\n",
    "curv_np = np.array(curv_loss)\n",
    "g_inv_np = np.array(g_inv_norm_mean_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_curvature == True:\n",
    "    fig, axes = plot3losses(mse_loss,curv_loss,g_inv_norm_mean_train_list)\n",
    "    if violent_saving==True:\n",
    "        plt.savefig(f'{Path_pictures}/3losses.pdf',bbox_inches='tight',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot9losses(mse_train_list,curv_train_list,g_inv_train_list):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18,18))\n",
    "    \n",
    "    win50 = signal.windows.hann(50)\n",
    "    win200 = signal.windows.hann(200)\n",
    "\n",
    "    axes[0,0].semilogy(mse_train_list, color = 'tab:red')\n",
    "    axes[0,0].set_ylabel('MSE')\n",
    "\n",
    "    axes[0,1].semilogy(signal.convolve(mse_train_list, win50, mode='same') / sum(win50), color = 'tab:red')\n",
    "    #axes[0,1].set_ylabel('MSE')\n",
    "\n",
    "    axes[0,2].semilogy(signal.convolve(mse_train_list, win200, mode='same') / sum(win200), color = 'tab:red')\n",
    "    #axes[0,2].set_ylabel('MSE')\n",
    "    \n",
    "    axes[1,0].semilogy(curv_train_list, color = 'tab:olive')\n",
    "    axes[1,0].set_ylabel('Curvature')\n",
    "\n",
    "    axes[1,1].semilogy(signal.convolve(curv_train_list, win50, mode='same') / sum(win50), color = 'tab:olive')\n",
    "    #axes[1,1].set_ylabel('Curvature')\n",
    "\n",
    "    axes[1,2].semilogy(signal.convolve(curv_train_list, win200, mode='same') / sum(win200), color = 'tab:olive')\n",
    "    #axes[1,2].set_ylabel('Curvature')\n",
    "    \n",
    "    axes[2,0].semilogy(g_inv_train_list, color = 'tab:blue')\n",
    "    axes[2,0].set_ylabel('$\\|G^{-1}\\|_F$')\n",
    "\n",
    "    axes[2,1].semilogy(signal.convolve(g_inv_train_list, win50, mode='same') / sum(win50), color = 'tab:blue')\n",
    "    #axes[2,1].set_ylabel('$\\|G^{-1}\\|_F$')\n",
    "\n",
    "    axes[2,2].semilogy(signal.convolve(g_inv_train_list, win200, mode='same') / sum(win200), color = 'tab:blue')\n",
    "    #axes[2,2].set_ylabel('$\\|G^{-1}\\|_F$')\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i==2:\n",
    "                axes[i,j].set_xlabel('Batches')\n",
    "    fig.show()\n",
    "    return fig,axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_curvature == True:\n",
    "    fig, axes = plot9losses(mse_loss,curv_loss,g_inv_norm_mean_train_list)\n",
    "    if violent_saving == True:\n",
    "        plt.savefig(f'{Path_pictures}/9losses.pdf',bbox_inches='tight',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if violent_saving == True:\n",
    "    with open(f\"{Path_pictures}/mse_loss\", \"w\") as mse_file:\n",
    "        json.dump(mse_loss, mse_file)\n",
    "    with open(f\"{Path_pictures}/curv_loss\", \"w\") as curv_file:\n",
    "        json.dump(curv_loss, curv_file)\n",
    "    with open(f\"{Path_pictures}/g_inv\", \"w\") as g_inv_file:\n",
    "        json.dump(g_inv_norm_mean_train_list, g_inv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
