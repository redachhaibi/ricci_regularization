{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.func import jacrev,jacfwd\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_name = \"Swissroll\"\n",
    "#set_name = \"Synthetic\"\n",
    "set_name = \"MNIST\"\n",
    "\n",
    "experiment_name = f\"{set_name}_torus_AE\"\n",
    "experiment_number = 0\n",
    "violent_saving = True # if False it will not save plots\n",
    "Path_experiments = \"../experiments/\"\n",
    "Path_pictures = f\"../experiments/{experiment_name}/experiment{experiment_number}\"\n",
    "if violent_saving == True:\n",
    "    if os.path.exists(Path_pictures) == False:\n",
    "        if os.path.exists(f\"../experiments/{experiment_name}/\") == False:\n",
    "            os.mkdir(f\"../experiments/{experiment_name}/\")\n",
    "        os.mkdir(Path_pictures) # needs to be commented once the folder for plots is created\n",
    "Path_weights = \"../nn_weights/\"\n",
    "\n",
    "d = 2         # latent space dimension\n",
    "weights_loaded = False\n",
    "weights_saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_w = 1e1 # 1e4\n",
    "unif_w = 0 # 4e1\n",
    "curv_w = 0 # if 0 curvature is not computed\n",
    "compute_curvature = False\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr         = 1e-3\n",
    "momentum   = 0.8\n",
    "num_epochs = 10\n",
    "\n",
    "# Hyperparameters for data loaders\n",
    "batch_size  = 64 # was 128 for MNIST\n",
    "split_ratio = 0.2\n",
    "weight_decay = 0.\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "# Set manual seed for reproducibility\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set uploading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('../') # have to go 1 level up\n",
    "import ricci_regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers in DataLoader\n",
    "num_workers = 10\n",
    "\n",
    "if set_name == \"MNIST\":\n",
    "    D = 784\n",
    "    k = 10 # number of classes\n",
    "    #MNIST_SIZE = 28\n",
    "    # MNIST Dataset\n",
    "    train_dataset = datasets.MNIST(root='../datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset  = datasets.MNIST(root='../datasets/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    #train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    #test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "elif set_name == \"Synthetic\":\n",
    "    D = 784       #dimension\n",
    "    k = 3         # num of 2d planes in dim D\n",
    "    n = 6*(10**3) # num of points in each plane\n",
    "    shift_class = 0.0\n",
    "    var_class = 1.0\n",
    "    intercl_var = 0.1 # this has to be greater than 0.04\n",
    "    # this creates a gaussian, \n",
    "    # i.e.random shift \n",
    "    # proportional to the value of intercl_var\n",
    "    # Generate dataset\n",
    "    # via classes\n",
    "    torch.manual_seed(0) # reproducibility\n",
    "    my_dataset = ricci_regularization.SyntheticDataset(k=k,n=n,d=d,D=D,\n",
    "                                        shift_class=shift_class, intercl_var=intercl_var, var_class=var_class)\n",
    "\n",
    "    train_dataset = my_dataset.create\n",
    "elif set_name == \"Swissroll\":\n",
    "    D = 3\n",
    "    sr_noise = 1e-6\n",
    "    sr_numpoints = 18000 #k*n\n",
    "    #sr_numpoints = 60000\n",
    "    train_dataset =  sklearn.datasets.make_swiss_roll(n_samples=sr_numpoints, noise=sr_noise)\n",
    "    sr_points = torch.from_numpy(train_dataset[0]).to(torch.float32)\n",
    "    #sr_points = torch.cat((sr_points,torch.zeros(sr_numpoints,D-3)),dim=1)\n",
    "    sr_colors = torch.from_numpy(train_dataset[1]).to(torch.float32)\n",
    "    from torch.utils.data import TensorDataset\n",
    "    train_dataset = TensorDataset(sr_points,sr_colors)\n",
    "\n",
    "m = len(train_dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(train_dataset, [int(m-m*split_ratio), int(m*split_ratio)])\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(test_data , batch_size=batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "torus_ae = ricci_regularization.Architectures.TorusAE(x_dim=D, h_dim1= 512, h_dim2=256, z_dim=d)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torus_ae.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if weights_loaded == True:\n",
    "    PATH_vae = f'../nn_weights/exp{experiment_number}.pt'\n",
    "    torus_ae.load_state_dict(torch.load(PATH_vae))\n",
    "    torus_ae.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(torus_ae.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_functionm_old(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='mean')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# MSE is computed with mean reduction in order for MSE and KLD to be of the same order\n",
    "# Inputs:\n",
    "#   recon_data: reconstructed data via decoder\n",
    "#   data: original data\n",
    "#   z: latent variable\n",
    "def loss_function_old2(recon_data, data, z, mu, Sigma):\n",
    "    MSE = F.mse_loss(recon_data, data.view(-1, D), reduction='mean')\n",
    "    KLD = 0.5 * ( torch.trace(Sigma) + mu.norm().pow(2) - d - Sigma.logdet() )\n",
    "    return (MSE + KLD)*1e4\n",
    "\n",
    "def curv_func(encoded_data, function):\n",
    "    metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=function)\n",
    "    det_on_data = torch.det(metric_on_data)\n",
    "    Sc_on_data = ricci_regularization.Sc_jacfwd_vmap(encoded_data,\n",
    "                                           function=function)\n",
    "    N = metric_on_data.shape[0]\n",
    "    Integral_of_Sc = (1/N)*(torch.sqrt(det_on_data)*torch.square(Sc_on_data)).sum()\n",
    "    return Integral_of_Sc\n",
    "    \n",
    "# Loss = MSE + Penalization + curv_loss\n",
    "#  where the penalization uses modulis of Fourier modes, of the empirical distribution.\n",
    "#  This requires batch size to be in the range of CLT.\n",
    "#\n",
    "# Inputs:\n",
    "#   recon_data: reconstructed data via decoder\n",
    "#   data: original data\n",
    "#   z: latent variable\n",
    "def loss_function(recon_data, data, z,compute_curvature = compute_curvature):\n",
    "    MSE = F.mse_loss(recon_data, data.view(-1, D), reduction='mean')\n",
    "    #\n",
    "    # Splits sines and cosines\n",
    "    z_sin = z[:, 0:d]\n",
    "    z_cos = z[:, d:2*d]\n",
    "    #\n",
    "    # Compute empirical first mode\n",
    "    mode1 = torch.mean( z, dim = 0)\n",
    "    mode1 = torch.sum( mode1*mode1 )\n",
    "    #\n",
    "    # Compute empirical second mode\n",
    "    mode2_1 = torch.mean( 2*z_cos*z_cos-1, dim = 0)\n",
    "    mode2_1 = torch.sum( mode2_1*mode2_1)\n",
    "    mode2_2 = torch.mean( 2*z_sin*z_cos, dim = 0)\n",
    "    mode2_2 = torch.sum( mode2_2*mode2_2 )\n",
    "    mode2 = mode2_1 + mode2_2\n",
    "    #\n",
    "    penalization = mode1 + mode2\n",
    "    #print(\"penalization: \", penalization)\n",
    "    if curv_w>0:\n",
    "        encoded_points = torus_ae.encoder2lifting(data.view(-1, D)).detach()\n",
    "        curv_loss = curv_func(encoded_points,function=torus_ae.decoder_torus)   \n",
    "    else:\n",
    "        if compute_curvature == True:\n",
    "            encoded_points = torus_ae.encoder2lifting(data.view(-1, D)).detach()\n",
    "            curv_loss = curv_func(encoded_points,function=torus_ae.decoder_torus)\n",
    "        else:\n",
    "            curv_loss = torch.zeros(1)\n",
    "    #print(\"curvature loss:\", curv_loss)\n",
    "    return MSE, penalization, curv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x = torch.rand(1,784)\n",
    "torus_ae = VAE(x_dim=D, h_dim1= 512, h_dim2=256, z_dim=d)\n",
    "optimizer = optim.Adam(torus_ae.parameters(),lr=lr)\n",
    "torus_ae.train()\n",
    "for (data,label) in tqdm(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    data = data.view(-1, D).cpu()\n",
    "    recon_data = torus_ae(data)[0]\n",
    "    #recon_data = torus_ae.decoder_torus(torus_ae.encoder2lifting(data))\n",
    "    mse_loss = F.mse_loss(recon_data, data, reduction='mean')\n",
    "    curv_loss = curv_func(torus_ae.encoder2lifting(data).detach(),function=torus_ae.decoder_torus) # use detach() to fix the points\n",
    "    \n",
    "    myloss = 1e4*mse_loss + 1e3*curv_loss\n",
    "    #myloss = 1e3*curv_func(torus_ae.encoder2lifting(data.view(-1,D)).detach(),function=torus_ae.decoder_torus)\n",
    "    #myloss = torus_ae.encoder2lifting(data.view(-1,D)).norm()\n",
    "    #myloss = torus_ae.encoder2lifting(x).norm()\n",
    "    #print(\"\\n 4d repr:\", torus_ae.encoder(x))\n",
    "    #myloss = 1e3*curv_func(torus_ae.encoder_torus(data.view(-1,D)),function=torus_ae.decoder_torus)\n",
    "    \n",
    "    myloss.backward()\n",
    "    optimizer.step()\n",
    "    print(myloss)\n",
    "    plot_ae_outputs(torus_ae.encoder2lifting,torus_ae.decoder_torus)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, mse_loss_array=[], uniform_loss_array=[], curvatue_loss_array = []):\n",
    "    batch_idx = 0\n",
    "    torus_ae.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch %d\"%epoch)\n",
    "    batches_per_epoch = len(train_loader)\n",
    "    t = tqdm( train_loader, position=0 )\n",
    "    for (data, labels) in t:\n",
    "        #data = data.cuda()\n",
    "        #print(data.shape)\n",
    "        data = data.cpu()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward\n",
    "        recon_batch, z = torus_ae(data)\n",
    "        mse_loss, uniform_loss, curvature_loss = loss_function(recon_batch, data, z)\n",
    "        loss = mse_w*mse_loss + unif_w*uniform_loss + curv_w*curvature_loss\n",
    "        #loss = mse_w*mse_loss + unif_w*uniform_loss \n",
    "        #loss = curv_w*curvature_loss\n",
    "        #print(f\"batch:{batch_idx}, MSE:{mse_loss}, Uniform:{uniform_loss}, Curvature:{curvature_loss}.\\n\")\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        mse_loss_array.append(mse_loss.item())\n",
    "        uniform_loss_array.append(uniform_loss.item())\n",
    "        curvatue_loss_array.append(curvature_loss.item())\n",
    "        #loss_array.append(loss.item())\n",
    "        # Progress bar\n",
    "        #t.set_description_str(desc=\"Average train loss: %.6f\"% (train_loss / len(train_loader.dataset)) )\n",
    "        batch_idx += 1\n",
    "        t.set_description_str(desc=f\"MSE:{np.array(mse_loss_array)[-batches_per_epoch:].mean()}, Uniform:{np.array(uniform_loss_array)[-batches_per_epoch:].mean()}, Curvature:{np.array(curvatue_loss_array)[-batches_per_epoch:].mean()}.\\n\")\n",
    "        \n",
    "        #if (batch_idx % 100 == 0):\n",
    "        #    plot3losses(mse_loss_array,uniform_loss_array,curvatue_loss_array)\n",
    "    # end for \n",
    "    \n",
    "    \n",
    "    return mse_loss_array, uniform_loss_array, curvatue_loss_array\n",
    "\n",
    "def test(epoch, mse_loss_array=[], uniform_loss_array=[], curvatue_loss_array = []):\n",
    "    torus_ae.eval()\n",
    "    with torch.no_grad():\n",
    "        t = tqdm( test_loader, desc=\"Test\", position=1 )\n",
    "        for data, _ in t:\n",
    "            data = data.cpu()\n",
    "            recon_batch, z = torus_ae(data)\n",
    "            mse_loss, uniform_loss, curvature_loss = loss_function(recon_batch, data, z)\n",
    "        \n",
    "            mse_loss_array.append(mse_loss.item())\n",
    "            uniform_loss_array.append(uniform_loss.item())\n",
    "            curvatue_loss_array.append(curvature_loss.item())\n",
    "    print(f\"Test losses. \\nMSE:{np.array(mse_loss_array).mean()}, Uniform:{np.array(uniform_loss_array).mean()}, Curvature:{np.array(curvatue_loss_array).mean()}.\\n\")\n",
    "    return mse_loss_array, uniform_loss_array, curvatue_loss_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_array=[]\n",
    "uniform_loss_array=[]\n",
    "curvatue_loss_array = []\n",
    "# Launch\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  mse_loss_array,uniform_loss_array,curvatue_loss_array = train(epoch, mse_loss_array, uniform_loss_array, curvatue_loss_array)\n",
    "  #plot3losses(mse_loss_array,uniform_loss_array,curvatue_loss_array)\n",
    "  if set_name == \"MNIST\":\n",
    "    ricci_regularization.PlottingTools.plot_ae_outputs(test_dataset=test_dataset,\n",
    "                                                       encoder=torus_ae.encoder2lifting,\n",
    "                                                       decoder=torus_ae.decoder_torus)\n",
    "  test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test losses and $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_R_squared_losses(data_loader=test_loader):\n",
    "    colorlist = []\n",
    "    z_list = []\n",
    "    enc_list = []\n",
    "    input_dataset_list = []\n",
    "    recon_dataset_list = []\n",
    "\n",
    "    for (data, labels) in data_loader:\n",
    "    #for (data, labels) in train_loader:\n",
    "        input_dataset_list.append(data)\n",
    "        recon_dataset_list.append(torus_ae(data)[0])\n",
    "        z_list.append(torus_ae(data)[1])\n",
    "        enc_list.append(torus_ae.encoder2lifting(data.view(-1,D)))\n",
    "        colorlist.append(labels)\n",
    "    input_dataset_tensor = torch.cat(input_dataset_list).view(-1,D)\n",
    "    recon_dataset_tensor = torch.cat(recon_dataset_list)\n",
    "    z_tensor = torch.cat(z_list)\n",
    "    mse_loss, unif_loss, curv_loss = loss_function(recon_dataset_tensor, \n",
    "                                                input_dataset_tensor, \n",
    "                                                z_tensor, \n",
    "                                                compute_curvature=True)\n",
    "    var = torch.var(input_dataset_tensor.flatten())\n",
    "    R_squared = 1 - mse_loss/var\n",
    "    return mse_loss, unif_loss, curv_loss, R_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_unif, test_curv, test_R_squared = compute_R_squared_losses(test_loader)\n",
    "train_mse, train_unif, train_curv, train_R_squared = compute_R_squared_losses(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train losses:\\nmse:{train_mse}, unif_loss:{train_unif}, curv_loss:{train_curv}\")\n",
    "print(f\"R_squared: {train_R_squared.item():.4f}\")\n",
    "print(f\"Test losses:\\nmse:{test_mse}, unif_loss:{test_unif}, curv_loss:{test_curv}\")\n",
    "print(f\"R_squared: {test_R_squared.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "from torcheval.metrics import R2Score\n",
    "R_squared = R2Score()#(multioutput=\"raw_values\")\n",
    "input = input_dataset_tensor.flatten()\n",
    "target = recon_dataset_tensor.flatten()\n",
    "R_squared.update(input, target)\n",
    "R_squared.compute()#.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weights_saved == True:\n",
    "    PATH_vae = f'../nn_weights/{set_name}_exp{experiment_number}.pt'\n",
    "    torch.save(torus_ae.state_dict(), PATH_vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss ploting\n",
    "fig,axes = ricci_regularization.PlottingTools.plot3losses(mse_loss_array,uniform_loss_array,curvatue_loss_array)\n",
    "if violent_saving == True:\n",
    "    fig.savefig(f\"{Path_pictures}/losses_exp{experiment_number}.pdf\",bbox_inches='tight',format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torus latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspiration for torus_ae.encoder2lifting\n",
    "\"\"\"\n",
    "def circle2anglevectorized(zLatentTensor,d = d):\n",
    "    cosphi = zLatentTensor[:, 0:d]\n",
    "    sinphi = zLatentTensor[:, d:2*d]\n",
    "    phi = torch.acos(cosphi)*torch.sgn(torch.asin(sinphi))\n",
    "    return phi\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zlist = []\n",
    "colorlist = []\n",
    "enc_list = []\n",
    "input_dataset_list = []\n",
    "recon_dataset_list = []\n",
    "for (data, labels) in tqdm( train_loader, position=0 ):\n",
    "#for (data, labels) in train_loader:\n",
    "    input_dataset_list.append(data)\n",
    "    recon_dataset_list.append(torus_ae(data)[0])\n",
    "    #zlist.append(torus_ae(data)[1])\n",
    "    enc_list.append(torus_ae.encoder2lifting(data.view(-1,D)))\n",
    "    colorlist.append(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = torch.cat(input_dataset_list)\n",
    "recon_dataset = torch.cat(recon_dataset_list)\n",
    "encoded_points = torch.cat(enc_list)\n",
    "encoded_points_no_grad = encoded_points.detach()/math.pi\n",
    "color_array = torch.cat(colorlist).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "if set_name == \"Swissroll\":\n",
    "    my_cmap = \"jet\"\n",
    "else:\n",
    "    my_cmap = ricci_regularization.PlottingTools.discrete_cmap(k, 'jet')\n",
    "plt.scatter(encoded_points_no_grad[:,0],encoded_points_no_grad[:,1], c=color_array, marker='o', edgecolor='none', cmap=my_cmap)\n",
    "\n",
    "if set_name in [\"Synthetic\",\"MNIST\"]:\n",
    "    plt.colorbar(ticks=range(k))\n",
    "plt.grid(True)\n",
    "if violent_saving == True:\n",
    "    plt.savefig(f\"{Path_pictures}/latent_space_exp{experiment_number}.pdf\",bbox_inches='tight',format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_config = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"experiment_number\": experiment_number,\n",
    "    \"dataset\":\n",
    "    {\n",
    "        \"name\": set_name,\n",
    "        \"D\"   : D,\n",
    "        \"split_ratio\": split_ratio\n",
    "    },\n",
    "    \"architecture\" :\n",
    "    {\n",
    "        \"latent_dim\": d\n",
    "    },\n",
    "    \"optimization_parameters\": \n",
    "    {\n",
    "\t    \"learning_rate\": lr,\n",
    "        \"momentum\":momentum,\n",
    "\t    \"batch_size\": batch_size,\n",
    "\t    \"num_epochs\": num_epochs,\n",
    "\t    \"mse_w\": mse_w,\n",
    "\t    \"unif_w\": unif_w,\n",
    "\t    \"curv_w\": curv_w,\n",
    "        \"compute_curvature\": compute_curvature,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"random_seed\": random_seed\n",
    "    },\n",
    "    \"training_results\":\n",
    "    {\n",
    "        \"R^2\": test_R_squared.item(),\n",
    "        \"mse_loss\": test_mse.item(),\n",
    "        \"unif_loss\": test_unif.item(),\n",
    "        \"curv_loss\": test_curv.item()\n",
    "    },\n",
    "    \"Path_pictures\": Path_pictures,\n",
    "    \"Path_weights\": Path_weights,\n",
    "    \"Path_experiments\": Path_experiments\n",
    "}\n",
    "\n",
    "# Save dictionary to JSON file\n",
    "with open(f'{Path_experiments}/{experiment_name}exp{experiment_number}.json', 'w') as json_file:\n",
    "    json.dump(json_config, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci2024",
   "language": "python",
   "name": ".venv_ricci2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
