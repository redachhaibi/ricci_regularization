{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torus AE training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the training of the autoencoder (AE). \n",
    "\n",
    "The AE consists of the encoder $\\Phi$ and the decoder $\\Psi$.\n",
    "The latent space of the AE is topologically a $d-$ dimensional torus $\\mathcal{T}^d$, i.e it can be considered as a periodic box $[-\\pi, \\pi]^d$. We define a Riemannian metric on the latent space  as the pull-back of the Euclidean metric in the output space $\\mathbb{R}^D$ by the decoder function $\\Psi$ of the AE:\n",
    "\\begin{equation}\n",
    "    g = \\nabla \\Psi ^* \\nabla \\Psi \\ .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "One can switch on/off following training modes: \n",
    "\n",
    "- \"compute_curvature\"\n",
    "- \"compute_contractive_loss\"\n",
    "- \"OOD_regime\"\n",
    "- \"diagnostic_mode\"\n",
    "\n",
    "If \"compute_curvature\"==True, curvature functional is computed and latent space is regularized. The curvature functional is given by:\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_\\mathrm{curv} := \\int_M R^2 \\mu \\ .\n",
    "\\end{equation*}\n",
    "\n",
    "If \"compute_contractive_loss\"==True, contractive loss that penalizes the Frobenius norm of outlyers of encoder's Jacobian is computed, i.e.:\n",
    "$$\n",
    " \\mathcal{L}_\\mathrm{contractive} = Relu\\left( \\|\\Phi\\|_F - \\delta_\\mathrm{encoder}\\right)\n",
    "$$\n",
    "the Frobenius norm of the encoder functional is computed and latent space is regularized. One might want to turn it off for faster training for initial tuning of the parameters.\n",
    "\n",
    "If \"OOD_regime\"==True, than OOD sampling is performed to refine the curvature regularization results.\n",
    "\n",
    "One might want to turn off any of the modes to speed up the training in order to tune faster the \"vanilla\" AE (without regularization) and obtain the optimal hyperparameters that are the initial guess to start from for training the AE with regularization.\n",
    "\n",
    "If \"diagnostic_mode\"==True, following losses are plotted: MSE, $\\mathcal{L}_\\mathrm{unif}$, $\\mathcal{L}_\\mathrm{curv}$, $\\det(g)$, $\\|g_{reg}^{-1}\\|_F$, $\\|\\nabla \\Psi \\|^2_F$, $\\|\\nabla \\Phi \\|^2_F$, where:\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_\\mathrm{curv} := \\int_M R^2 \\mu \\ ,\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_\\mathrm{unif} := \\sum\\limits_{k=1}^{m} |\\int_M z^k  \\mu_N (dz) |^2 \\ ,\n",
    "\\end{equation*}\n",
    "where $R$ states for scalar curvature (see https://en.wikipedia.org/wiki/Scalar_curvature), $\\mu_N = \\Phi\\# ( \\frac{1}{N}\\sum\\limits_{j=1}^{N} \\delta_{X_i} ) $ is the push-forward of the natural measure induced by the dataset by the encoder $\\Phi$, thus $\\mu_N$ is a measure on $\\mathcal{T}^d$,  $ \\alpha_k = \\frac{1}{N} \\sum_{j=1}^{N} z_j^k$ is the empirical estimator of the $k$ -th moment of the data distribution in the latent space.\n",
    "\n",
    "If $\\xi \\sim \\mathcal{U}[-\\pi, \\pi]$ and $z = e^{i \\xi}$ than all the moments of $z$ are zero, namely if $\\mathcal{L}_\\mathrm{unif} \\to 0$ as $m \\to \\infty$, one obtains weak convergence of the data distribution in the latent space to the uniform distribution.\n",
    "\n",
    "Also $g_{reg} = g + \\varepsilon \\cdot I$ is the regularized matrix of metric for stability of inverse matrix computation, $\\|\\|_F$ is the Frobenius norm of the matrix.\n",
    "\n",
    "\n",
    "The notebook consists of\n",
    "\n",
    "1) Imports. Choosing hyperparameters for dataset uploading, learning and plotting such as learning rate, batch size, weights of MSE loss, curvature loss, etc. Automatic loading of train and test dataloaders. Choice among data sets \"Synthetic\", \"Swissroll\", \"MNIST\", \"MNIST01\" (any selected labels from MNIST). \n",
    "2) Architecture and device. Architecture types: Fully connected (TorusAE), Convolutional (TorusConvAE). Device: cuda/cpu. \n",
    "3) Training.\n",
    "4) Report of training. Printing of graphs of losses, saves of a json file with training params.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import ricci_regularization\n",
    "import yaml\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters loading from YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose setting of the experiment\n",
    "AE_setting_name = 'MNIST_Setting_1_exp1'\n",
    "# Open and read the YAML configuration file\n",
    "with open(f'../experiments/{AE_setting_name}_config.yaml','r') as yaml_file:\n",
    "    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Print the loaded YAML configuration\n",
    "print(f\"YAML Configuration loaded successfully from \\n: {yaml_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the experiment_name\n",
    "print(\"Experiment Name:\", AE_setting_name)  # Print the constructed experiment name\n",
    "\n",
    "# Paths for saving  pictures\n",
    "Path_pictures = f\"../experiments/\" + AE_setting_name\n",
    "print(f\"Path for experiment results: {Path_pictures}\")  # Print the path for pictures\n",
    "\n",
    "# Check and create directories based on configuration\n",
    "if not os.path.exists(Path_pictures):  # Check if the picture path does not exist\n",
    "    os.mkdir(Path_pictures)  # Create the directory for plots if not yet created\n",
    "    print(f\"Created directory: {Path_pictures}\")  # Print directory creation feedback\n",
    "else:\n",
    "    print(f\"Directiry already exists: {Path_pictures}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for data loader reproducibility\n",
    "torch.manual_seed(yaml_config[\"data_loader_settings\"][\"random_seed\"])\n",
    "print(f\"Set random seed to: {yaml_config['data_loader_settings']['random_seed']}\")\n",
    "\n",
    "try:\n",
    "    dtype = getattr(torch, yaml_config[\"architecture\"][\"weights_dtype\"]) # Convert the string to actual torch dtype\n",
    "except KeyError:\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Load data loaders based on YAML configuration\n",
    "dict = ricci_regularization.DataLoaders.get_dataloaders(\n",
    "    dataset_config=yaml_config[\"dataset\"],\n",
    "    data_loader_config=yaml_config[\"data_loader_settings\"],\n",
    "    dtype = dtype )# Convert the string to actual torch dtype\n",
    "\n",
    "train_loader = dict[\"train_loader\"]\n",
    "test_loader = dict[\"test_loader\"]\n",
    "test_dataset = dict.get(\"test_dataset\")  # Assuming 'test_dataset' is a key returned by get_dataloaders\n",
    "test_dataset_labels = test_dataset.targets\n",
    "\n",
    "print(\"Data loaders created successfully.\")\n",
    "\n",
    "# Calculate number of batches per epoch\n",
    "batches_per_epoch = len(train_loader)\n",
    "print(f\"Number of batches per epoch: {batches_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Architecture and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(yaml_config[\"data_loader_settings\"][\"random_seed\"])\n",
    "print(f\"Set random seed to: {yaml_config['data_loader_settings']['random_seed']}\")\n",
    "\n",
    "# Selecting the architecture type based on YAML configuration\n",
    "if yaml_config[\"architecture\"][\"type\"] == \"TorusConvAE\":\n",
    "    torus_ae = ricci_regularization.Architectures.TorusConvAE(\n",
    "        x_dim=yaml_config[\"architecture\"][\"output_dim\"],\n",
    "        h_dim1=512,\n",
    "        h_dim2=256,\n",
    "        z_dim=yaml_config[\"architecture\"][\"latent_dim\"],\n",
    "        pixels=28\n",
    "    )\n",
    "    print(\"Selected architecture: TorusConvAE\")\n",
    "else:\n",
    "    torus_ae = ricci_regularization.Architectures.TorusAE(\n",
    "        x_dim=yaml_config[\"architecture\"][\"output_dim\"],\n",
    "        h_dim1=512,\n",
    "        h_dim2=256,\n",
    "        z_dim=yaml_config[\"architecture\"][\"latent_dim\"],\n",
    "        dtype = dtype\n",
    "    )\n",
    "    print(\"Selected architecture: TorusAE\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available! Training will use GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is NOT available! Using CPU.\")\n",
    "\n",
    "# Move the AE model to the selected device\n",
    "torus_ae.to(device)\n",
    "print(f\"Moved model to device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if yaml_config[\"experiment\"][\"weights_loaded_from\"] != False:\n",
    "    PATH_weights_loaded = yaml_config[\"experiment\"][\"weights_loaded_from\"]\n",
    "    torus_ae.load_state_dict(torch.load(PATH_weights_loaded))\n",
    "    torus_ae.eval()\n",
    "    print(f\"Weights loaded from {PATH_weights_loaded}\")\n",
    "else:\n",
    "    print(\"No pretrained weights loaded as per the config.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "batch_idx = 0\n",
    "test_batch_idx = 0\n",
    "dict_loss_arrays = {}\n",
    "test_dict_loss_arrays = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = yaml_config[\"optimizer_settings\"][\"num_epochs\"]\n",
    "lr = yaml_config[\"optimizer_settings\"][\"lr\"]\n",
    "#lr = 0.5e-3 # for manual change of the lr\n",
    "optimizer = torch.optim.Adam( torus_ae.parameters(),\n",
    "        lr = lr,\n",
    "        weight_decay = yaml_config[\"optimizer_settings\"][\"weight_decay\"] )\n",
    "print(f\"Optimizer configured with learning rate {lr} and weight decay {yaml_config['optimizer_settings']['weight_decay']}.\")\n",
    "print(\"Number of epochs:\", num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# timing\n",
    "start_time = time.time()\n",
    "# Launch\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  torus_ae.to(device)\n",
    "  #batch_idx, dict_loss_arrays = train(epoch=epoch,batch_idx=batch_idx,dict_loss_arrays=dict_loss_arrays)\n",
    "  batch_idx, dict_loss_arrays = ricci_regularization.train(torus_ae,training_config=yaml_config,\n",
    "      train_loader=train_loader,optimizer=optimizer, epoch=epoch,batch_idx=batch_idx,\n",
    "      dict_loss_arrays=dict_loss_arrays)\n",
    "  if yaml_config[\"training_mode\"][\"diagnostic_mode\"] == True :\n",
    "    dict_losses_to_plot = ricci_regularization.PlottingTools.translate_dict(dict_losses_to_plot=dict_loss_arrays, \n",
    "                include_curvature_plots=yaml_config[\"training_mode\"][\"compute_curvature\"],\n",
    "                eps=yaml_config[\"loss_settings\"][\"eps\"])\n",
    "    ricci_regularization.PlottingTools.plotsmart(dict_losses_to_plot)\n",
    "\n",
    "  #plotting\n",
    "  torus_ae.cpu()\n",
    "  with torch.no_grad():\n",
    "      encoded_points = torus_ae.encoder2lifting(test_dataset.data.view(-1,yaml_config[\"architecture\"][\"output_dim\"]).to(torch.float32)/255.)\n",
    "  ricci_regularization.point_plot_fast(encoded_points,test_dataset_labels,batch_idx,yaml_config)\n",
    "  torus_ae.cuda()\n",
    "  plt.show()\n",
    "  #fig.show()\n",
    "  #else:\n",
    "  #  ricci_regularization.PlottingTools.plotfromdict(dict_of_losses=dict_loss_arrays)  \n",
    "  if (yaml_config[\"dataset\"][\"name\"] in [\"MNIST01\",\"MNIST_subset\"]):\n",
    "    ricci_regularization.PlottingTools.plot_ae_outputs_selected(\n",
    "      test_dataset=test_dataset,\n",
    "      encoder=torus_ae.cpu().encoder2lifting,\n",
    "      decoder=torus_ae.cpu().decoder_torus,\n",
    "      selected_labels=yaml_config[\"dataset\"][\"selected_labels\"] )\n",
    "  elif (yaml_config[\"dataset\"][\"name\"] == \"MNIST\"):\n",
    "    ricci_regularization.PlottingTools.plot_ae_outputs(\n",
    "      test_dataset=test_dataset,\n",
    "      encoder=torus_ae.cpu().encoder2lifting,\n",
    "      decoder=torus_ae.cpu().decoder_torus )\n",
    "  # end if\n",
    "  # Test \n",
    "  torus_ae.to(device)\n",
    "  test_batch_idx,test_dict_loss_arrays = ricci_regularization.test(torus_ae, test_loader=test_loader,\n",
    "      training_config=yaml_config, batch_idx=test_batch_idx,dict_loss_arrays=test_dict_loss_arrays) \n",
    "  # end for\n",
    "\n",
    "#timing\n",
    "end_time = time.time()\n",
    "algorithm_execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Report of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torus_ae.state_dict(), f'{Path_pictures}/ae_weights.pt')\n",
    "print(\"AE weights saved at:\", Path_pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test losses, $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test losses\n",
    "_,dict_test_losses = ricci_regularization.test(torus_ae,train_loader,training_config=yaml_config)\n",
    "\n",
    "test_mse = np.array(dict_test_losses[\"MSE\"]).mean()\n",
    "# collect test batches in a list and then concatenate to get one tensor for test data\n",
    "\n",
    "list = []\n",
    "for data,_ in test_loader:\n",
    "    list.append(data.float())\n",
    "# compute variance\n",
    "var = torch.var(torch.cat(list).flatten())\n",
    "# compute R^2\n",
    "test_R_squared = 1 - test_mse/var\n",
    "#printing\n",
    "\n",
    "print(\"Test losses\")\n",
    "print(f\"MSE:, {test_mse.item():.4f}\")\n",
    "print(f\"R²: {test_R_squared.item():.4f}\")\n",
    "\n",
    "\n",
    "test_unif = np.array(dict_test_losses[\"Equidistribution\"]).mean()\n",
    "try:\n",
    "    test_curv = np.array(dict_test_losses[\"Curvature\"]).mean()\n",
    "    print(f\"Curvature: {test_curv.item():.4f}\")\n",
    "except KeyError:\n",
    "    test_curv = \"not_computed\"\n",
    "    print(\"Curvature:\", test_curv)\n",
    "print(f\"Equidistribution:, {test_unif.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting smooth:\n",
    "dict_losses_to_plot = ricci_regularization.PlottingTools.translate_dict(dict_losses_to_plot=dict_loss_arrays,\n",
    "                eps=yaml_config[\"loss_settings\"][\"eps\"])\n",
    "fig,axes = ricci_regularization.PlottingTools.PlotSmartConvolve(dict_losses_to_plot,numwindows1=10,numwindows2=50)\n",
    "# plot only non-smooth:\n",
    "#fig,axes = ricci_regularization.PlottingTools.plotfromdict(dict_loss_arrays)\n",
    "fig.savefig(f\"{Path_pictures}/losses.pdf\",bbox_inches='tight',format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it is optimal\n",
    "# saving loss history\n",
    "torch.save(dict_loss_arrays, f'{Path_pictures}/losses_history.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving test losses in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Define the file path to save results\n",
    "json_file_path = f\"{Path_pictures}/results.json\"\n",
    "# check if time was computed previousely\n",
    "try:\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        loaded_json_results = json.load(json_file)\n",
    "    training_time_accumulated_flag = True\n",
    "    total_training_time = loaded_json_results[-1][\"training_time\"]\n",
    "    total_epoch_count = loaded_json_results[-1][\"epoch_count\"]\n",
    "    total_training_time += algorithm_execution_time\n",
    "    total_epoch_count += num_epochs\n",
    "    print(f\"Training time is accumulated with previousely saved training time from{json_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    total_training_time = algorithm_execution_time\n",
    "    total_epoch_count = num_epochs\n",
    "    loaded_json_results = []\n",
    "    pass\n",
    "\n",
    "current_results = {\n",
    "        \"epoch_count\": total_epoch_count,\n",
    "        \"learning_rate\": lr,\n",
    "        \"R^2_test_data \": float(f\"{test_R_squared.item():.6f}\"),\n",
    "        \"MSE_loss_test_data\": float(f\"{test_mse.item():.6f}\"),  \n",
    "        \"Equidistribution_loss_test_data\": float(f\"{test_unif.item():.6f}\"),\n",
    "        \"training_time\": float(f\"{total_training_time:.3f}\")\n",
    "}\n",
    "if yaml_config[\"training_mode\"][\"compute_curvature\"] == True:\n",
    "    current_results[\"Curvature_loss_test_data\"] = test_curv.item()\n",
    "else:\n",
    "    current_results[\"Curvature_loss_test_data\"] = \"Not computed\"\n",
    "loaded_json_results.append(current_results)\n",
    "# save all the results\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(loaded_json_results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torus latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torus_ae.cpu() # switch device to cpu for plotting\n",
    "fig = ricci_regularization.point_plot(encoder=torus_ae.encoder2lifting, data_loader=test_loader,\n",
    "                                      batch_idx=batch_idx,config=yaml_config, \n",
    "                                      show_title=False, figsize=(9,9))\n",
    "fig.savefig( Path_pictures + \"/latent_space.pdf\", bbox_inches = 'tight', format = \"pdf\" )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
