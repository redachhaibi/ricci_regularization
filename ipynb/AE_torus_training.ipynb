{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torus AE training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the training of the autoencoder (AE). \n",
    "\n",
    "The AE consists of the encoder $\\Phi$ and the decoder $\\Psi$.\n",
    "The latent space of the AE is topologically a $d-$ dimensional torus $\\mathcal{T}^d$, i.e it can be considered as a periodic box $[-\\pi, \\pi]^d$. We define a Riemannian metric on the latent space  as the pull-back of the Euclidean metric in the output space $\\mathbb{R}^D$ by the decoder function $\\Psi$ of the AE:\n",
    "\\begin{equation}\n",
    "    g = \\nabla \\Psi ^* \\nabla \\Psi \\ .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "One can switch on/off following training modes: \n",
    "\n",
    "- \"compute_curvature\"\n",
    "- \"compute_contractive_loss\"\n",
    "- \"OOD_regime\"\n",
    "- \"diagnostic_mode\"\n",
    "\n",
    "If \"compute_curvature\"==True, curvature functional is computed and latent space is regularized. The curvature functional is given by:\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_\\mathrm{curv} := \\int_M R^2 \\mu \\ .\n",
    "\\end{equation*}\n",
    "\n",
    "If \"compute_contractive_loss\"==True, contractive loss that penalizes the Frobenius norm of outlyers of encoder's Jacobian is computed, i.e.:\n",
    "$$\n",
    " \\mathcal{L}_\\mathrm{contractive} = Relu\\left( \\|\\Phi\\|_F - \\delta_\\mathrm{encoder}\\right)\n",
    "$$\n",
    "the Frobenius norm of the encoder functional is computed and latent space is regularized. One might want to turn it off for faster training for initial tuning of the parameters.\n",
    "\n",
    "If \"OOD_regime\"==True, than OOD sampling is performed to refine the curvature regularization results.\n",
    "\n",
    "One might want to turn off any of the modes to speed up the training in order to tune faster the \"vanilla\" AE (without regularization) and obtain the optimal hyperparameters that are the initial guess to start from for training the AE with regularization.\n",
    "\n",
    "If \"diagnostic_mode\"==True, following losses are plotted: MSE, $\\mathcal{L}_\\mathrm{unif}$, $\\mathcal{L}_\\mathrm{curv}$, $\\det(g)$, $\\|g_{reg}^{-1}\\|_F$, $\\|\\nabla \\Psi \\|^2_F$, $\\|\\nabla \\Phi \\|^2_F$, where:\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_\\mathrm{curv} := \\int_M R^2 \\mu \\ ,\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_\\mathrm{unif} := \\sum\\limits_{k=1}^{m} |\\int_M z^k  \\mu_N (dz) |^2 \\ ,\n",
    "\\end{equation*}\n",
    "where $R$ states for scalar curvature (see https://en.wikipedia.org/wiki/Scalar_curvature), $\\mu_N = \\Phi\\# ( \\frac{1}{N}\\sum\\limits_{j=1}^{N} \\delta_{X_i} ) $ is the push-forward of the natural measure induced by the dataset by the encoder $\\Phi$, thus $\\mu_N$ is a measure on $\\mathcal{T}^d$,  $ \\alpha_k = \\frac{1}{N} \\sum_{j=1}^{N} z_j^k$ is the empirical estimator of the $k$ -th moment of the data distribution in the latent space.\n",
    "\n",
    "If $\\xi \\sim \\mathcal{U}[-\\pi, \\pi]$ and $z = e^{i \\xi}$ than all the moments of $z$ are zero, namely if $\\mathcal{L}_\\mathrm{unif} \\to 0$ as $m \\to \\infty$, one obtains weak convergence of the data distribution in the latent space to the uniform distribution.\n",
    "\n",
    "Also $g_{reg} = g + \\varepsilon \\cdot I$ is the regularized matrix of metric for stability of inverse matrix computation, $\\|\\|_F$ is the Frobenius norm of the matrix.\n",
    "\n",
    "\n",
    "The notebook consists of\n",
    "\n",
    "1) Imports. Choosing hyperparameters for dataset uploading, learning and plotting such as learning rate, batch size, weights of MSE loss, curvature loss, etc. Automatic loading of train and test dataloaders. Choice among data sets \"Synthetic\", \"Swissroll\", \"MNIST\", \"MNIST01\" (any selected labels from MNIST). \n",
    "2) Architecture and device. Architecture types: Fully connected (TorusAE), Convolutional (TorusConvAE). Device: cuda/cpu. \n",
    "3) Training.\n",
    "4) Report of training. Printing of graphs of losses, saves of a json file with training params.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import ricci_regularization\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters loading from YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the YAML configuration file\n",
    "#with open('../experiments/Swissroll_exp5_config.yaml', 'r') as yaml_file: \n",
    "\n",
    "# Some other experiments to try. Uncomment for trying and comment the previous line.\n",
    "\n",
    "#with open('../experiments/MNIST01_exp8_config.yaml', 'r') as yaml_file: # MNIST with labels 5,8 without curvature\n",
    "with open('../experiments/MNIST_Setting_1_config.yaml','r') as yaml_file:\n",
    "#with open('../experiments/Synthetic_uniform_config.yaml','r') as yaml_file:\n",
    "    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Print the loaded YAML configuration\n",
    "print(f\"YAML Configuration loaded successfully from \\n: {yaml_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the experiment_name\n",
    "print(\"Experiment Name:\", yaml_config[\"experiment\"][\"name\"])  # Print the constructed experiment name\n",
    "\n",
    "# Paths for saving  pictures\n",
    "Path_pictures = f\"../experiments/\" + yaml_config[\"experiment\"][\"name\"]\n",
    "print(f\"Path for experiment results: {Path_pictures}\")  # Print the path for pictures\n",
    "\n",
    "# Check and create directories based on configuration\n",
    "if not os.path.exists(Path_pictures):  # Check if the picture path does not exist\n",
    "    os.mkdir(Path_pictures)  # Create the directory for plots if not yet created\n",
    "    print(f\"Created directory: {Path_pictures}\")  # Print directory creation feedback\n",
    "else:\n",
    "    print(f\"Directiry already exists: {Path_pictures}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for data loader reproducibility\n",
    "torch.manual_seed(yaml_config[\"data_loader_settings\"][\"random_seed\"])\n",
    "print(f\"Set random seed to: {yaml_config['data_loader_settings']['random_seed']}\")\n",
    "\n",
    "try:\n",
    "    dtype = getattr(torch, yaml_config[\"architecture\"][\"weights_dtype\"]) # Convert the string to actual torch dtype\n",
    "except KeyError:\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Load data loaders based on YAML configuration\n",
    "dict = ricci_regularization.DataLoaders.get_dataloaders(\n",
    "    dataset_config=yaml_config[\"dataset\"],\n",
    "    data_loader_config=yaml_config[\"data_loader_settings\"],\n",
    "    dtype = dtype )# Convert the string to actual torch dtype\n",
    "\n",
    "train_loader = dict[\"train_loader\"]\n",
    "test_loader = dict[\"test_loader\"]\n",
    "test_dataset = dict.get(\"test_dataset\")  # Assuming 'test_dataset' is a key returned by get_dataloaders\n",
    "\n",
    "print(\"Data loaders created successfully.\")\n",
    "\n",
    "# Calculate number of batches per epoch\n",
    "batches_per_epoch = len(train_loader)\n",
    "print(f\"Number of batches per epoch: {batches_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Architecture and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(yaml_config[\"data_loader_settings\"][\"random_seed\"])\n",
    "print(f\"Set random seed to: {yaml_config['data_loader_settings']['random_seed']}\")\n",
    "\n",
    "# Selecting the architecture type based on YAML configuration\n",
    "if yaml_config[\"architecture\"][\"type\"] == \"TorusConvAE\":\n",
    "    torus_ae = ricci_regularization.Architectures.TorusConvAE(\n",
    "        x_dim=yaml_config[\"architecture\"][\"output_dim\"],\n",
    "        h_dim1=512,\n",
    "        h_dim2=256,\n",
    "        z_dim=yaml_config[\"architecture\"][\"latent_dim\"],\n",
    "        pixels=28\n",
    "    )\n",
    "    print(\"Selected architecture: TorusConvAE\")\n",
    "else:\n",
    "    torus_ae = ricci_regularization.Architectures.TorusAE(\n",
    "        x_dim=yaml_config[\"architecture\"][\"output_dim\"],\n",
    "        h_dim1=512,\n",
    "        h_dim2=256,\n",
    "        z_dim=yaml_config[\"architecture\"][\"latent_dim\"],\n",
    "        dtype = dtype\n",
    "    )\n",
    "    print(\"Selected architecture: TorusAE\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available! Training will use GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is NOT available! Using CPU.\")\n",
    "\n",
    "# Move the AE model to the selected device\n",
    "torus_ae.to(device)\n",
    "print(f\"Moved model to device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if yaml_config[\"experiment\"][\"weights_loaded_from\"] != False:\n",
    "    PATH_weights_loaded = yaml_config[\"experiment\"][\"weights_loaded_from\"]\n",
    "    torus_ae.load_state_dict(torch.load(PATH_weights_loaded))\n",
    "    torus_ae.eval()\n",
    "    print(f\"Weights loaded from {PATH_weights_loaded}\")\n",
    "else:\n",
    "    print(\"No pretrained weights loaded as per the config.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam( torus_ae.parameters(),\n",
    "        lr = yaml_config[\"optimizer_settings\"][\"lr\"],\n",
    "        weight_decay = yaml_config[\"optimizer_settings\"][\"weight_decay\"] )\n",
    "print(f\"Optimizer configured with learning rate {yaml_config['optimizer_settings']['lr']} and weight decay {yaml_config['optimizer_settings']['weight_decay']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss = MSE + uniform_loss + curv_loss + contractive_loss\n",
    "# add loss computation mode, that is yaml_config[\"training_mode\"]\n",
    "def loss_function(recon_data, data, z, torus_ae, config):\n",
    "    MSE = torch.nn.functional.mse_loss(recon_data, data, reduction='mean')\n",
    "    unif_loss = ricci_regularization.LossComputation.uniform_loss( z,\n",
    "        latent_dim = yaml_config[\"architecture\"][\"latent_dim\"],\n",
    "        num_moments = yaml_config[\"loss_settings\"][\"num_moments\"])\n",
    "    \n",
    "    dict_losses = {\n",
    "        \"MSE\": MSE,\n",
    "        \"Uniform\": unif_loss,\n",
    "    }\n",
    "    if config[\"training_mode\"][\"compute_contractive_loss\"] == True:\n",
    "        encoder_jac_norm = ricci_regularization.Jacobian_norm_jacrev_vmap( data, \n",
    "                                    function = torus_ae.encoder_torus,\n",
    "                                    input_dim = config[\"architecture\"][\"input_dim\"] )\n",
    "        outlyers_encoder_jac_norm = encoder_jac_norm - config[\"loss_settings\"][\"delta_encoder\"]\n",
    "        dict_losses[\"Contractive\"] = torch.nn.ReLU()( outlyers_encoder_jac_norm ).max()\n",
    "    if config[\"training_mode\"][\"compute_curvature\"] == True:\n",
    "        encoded_points_no_grad = torus_ae.encoder2lifting(data).detach()\n",
    "        if config[\"training_mode\"][\"curvature_computation_mode\"] == \"jacfwd\":        \n",
    "            #Sc_on_data, metric_on_data = ricci_regularization.Sc_g_jacfwd(encoded_points_no_grad,\n",
    "            #                                function=torus_ae.decoder_torus,eps=config[\"loss_settings\"][\"eps\"])\n",
    "            #det_on_data = torch.det(metric_on_data)\n",
    "            #dict_losses[\"Curvature\"] = (torch.sqrt(det_on_data)*torch.square(Sc_on_data)).mean() \n",
    "            \n",
    "            # avoiding recursive hell\n",
    "            dict_losses[\"Curvature\"] = ricci_regularization.curvature_loss_jacfwd(encoded_points_no_grad, function=torus_ae.decoder_torus,eps=config[\"loss_settings\"][\"eps\"])\n",
    "            if config[\"training_mode\"][\"diagnostic_mode\"] == True: # FIX this!\n",
    "                dict_losses[\"curv_squared_mean\"] = (torch.square(Sc_on_data)).mean()\n",
    "                dict_losses[\"curv_squared_max\"] = (torch.square(Sc_on_data)).max()\n",
    "        elif config[\"training_mode\"][\"curvature_computation_mode\"] == \"fd\":\n",
    "            dict_losses[\"Curvature\"] = ricci_regularization.curvature_loss(points=encoded_points_no_grad,\n",
    "                                                        function=torus_ae.decoder_torus, h = 0.01, eps=0.)\n",
    "    if config[\"training_mode\"][\"diagnostic_mode\"] == True:\n",
    "        if config[\"training_mode\"][\"compute_curvature\"] == False:\n",
    "            encoded_points_no_grad = torus_ae.encoder2lifting(data).detach()\n",
    "            metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_points_no_grad,\n",
    "                                                                     function = torus_ae.decoder_torus)\n",
    "            det_on_data = torch.det(metric_on_data)    \n",
    "        g_inv_train_batch = torch.linalg.inv(metric_on_data + config[\"loss_settings\"][\"eps\"]*torch.eye(config[\"architecture\"][\"latent_dim\"]).to(device))\n",
    "        g_inv_norm_train_batch = torch.linalg.matrix_norm(g_inv_train_batch)\n",
    "        dict_losses[\"g_inv_norm_mean\"] = torch.mean(g_inv_norm_train_batch)\n",
    "        dict_losses[\"g_inv_norm_max\"] = torch.max(g_inv_norm_train_batch)\n",
    "        dict_losses[\"g_det_mean\"] = det_on_data.mean()\n",
    "        dict_losses[\"g_det_max\"] = det_on_data.max()\n",
    "        dict_losses[\"g_det_min\"] = det_on_data.min()\n",
    "        decoder_jac_norm = torch.func.vmap(torch.trace)(metric_on_data)\n",
    "        dict_losses[\"decoder_jac_norm_mean\"] = decoder_jac_norm.mean()\n",
    "        dict_losses[\"decoder_jac_norm_max\"] = decoder_jac_norm.max()\n",
    "        dict_losses[\"decoder_contractive_loss\"] = (torch.nn.ReLU()(decoder_jac_norm)).max()\n",
    "        if config[\"training_mode\"][\"compute_contractive_loss\"] == False:\n",
    "            encoder_jac_norm = ricci_regularization.Jacobian_norm_jacrev_vmap( data, \n",
    "                                        function = torus_ae.encoder_torus,\n",
    "                                        input_dim = config[\"architecture\"][\"input_dim\"] )\n",
    "        encoder_jac_norm_mean = encoder_jac_norm.mean()\n",
    "        encoder_jac_norm_max = encoder_jac_norm.max()\n",
    "        dict_losses[\"encoder_jac_norm_mean\"] = encoder_jac_norm_mean\n",
    "        dict_losses[\"encoder_jac_norm_max\"] = encoder_jac_norm_max\n",
    "    return dict_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epoch=1, batch_idx = 0, dict_loss_arrays={}):\n",
    "    #  creating a dict for losses shown in progress bar\n",
    "    dict_loss2print = {}\n",
    "    if batch_idx == 0:\n",
    "        dict_loss_arrays = {}\n",
    "    torus_ae.train()\n",
    "    print(\"Epoch %d\"%epoch)\n",
    "    t = tqdm( train_loader, desc=\"Train\", position=0 )\n",
    "\n",
    "    # OOD points initialization\n",
    "    if yaml_config[\"training_mode\"][\"OOD_regime\"] == True:\n",
    "        first_batch,_ = next(iter(train_loader))\n",
    "        first_batch = first_batch.to(device)\n",
    "        extreme_curv_points_tensor = torus_ae.encoder2lifting(first_batch.view(-1,yaml_config[\"architecture\"][\"input_dim\"])[:yaml_config[\"OOD_settings\"][\"N_extr\"]]).detach()\n",
    "        extreme_curv_points_tensor.to(device)\n",
    "        extreme_curv_value_tensor = ricci_regularization.Sc_jacfwd_vmap(extreme_curv_points_tensor, \n",
    "                function=torus_ae.decoder_torus,eps=yaml_config[\"loss_settings\"][\"eps\"])\n",
    "    \n",
    "    for (data, labels) in t:\n",
    "        data = data.to(device)\n",
    "        data = data.reshape(-1, yaml_config[\"architecture\"][\"input_dim\"])\n",
    "        optimizer.zero_grad()\n",
    "        # Forward\n",
    "        recon_batch, z = torus_ae( data )\n",
    "        # Computing necessary losses on the current batch\n",
    "        dict_losses = loss_function( recon_batch, data, z,\n",
    "                                     torus_ae = torus_ae,\n",
    "                                     config = yaml_config )\n",
    "        # appending current losses to loss history\n",
    "        for key in dict_losses.keys():\n",
    "            if batch_idx == 0:\n",
    "                dict_loss_arrays[key] = []\n",
    "            # losses to keep in memory:\n",
    "            dict_loss_arrays[key].append(dict_losses[key].item())\n",
    "            # losses to show on progress bar: \n",
    "            dict_loss2print[key] = f\"{dict_losses[key].item():.4f}\"\n",
    "            # moving average (per epoch)\n",
    "            #dict_loss2print[key] = f\"{np.array(dict_loss_arrays[key])[-batches_per_epoch:].mean():.4f}\"\n",
    "        # end for \n",
    "        loss = yaml_config[\"loss_settings\"][\"lambda_recon\"] * dict_losses[\"MSE\"] \n",
    "        loss += yaml_config[\"loss_settings\"][\"lambda_unif\"] * dict_losses[\"Uniform\"] \n",
    "\n",
    "        if yaml_config[\"training_mode\"][\"compute_contractive_loss\"] == True:\n",
    "            # adding the contractive loss on the currenct batch to the loss function\n",
    "            loss += yaml_config[\"loss_settings\"][\"lambda_contractive_encoder\"] * dict_losses[\"Contractive\"]\n",
    "            \n",
    "        if (yaml_config[\"training_mode\"][\"compute_curvature\"] == True): \n",
    "            # adding the curvature loss on the currenct batch to the loss function\n",
    "            loss += yaml_config[\"loss_settings\"][\"lambda_curv\"] * dict_losses[\"Curvature\"]\n",
    "            \n",
    "        # OOD regime (optional)\n",
    "        if yaml_config[\"training_mode\"][\"OOD_regime\"] == True:\n",
    "            OOD_params_dict = yaml_config[\"OOD_settings\"]\n",
    "            extreme_curv_points_tensor, extreme_curv_value_tensor = ricci_regularization.find_extreme_curvature_points(\n",
    "                data_batch = data,\n",
    "                extreme_curv_points_tensor = extreme_curv_points_tensor,\n",
    "                extreme_curv_value_tensor = extreme_curv_value_tensor,\n",
    "                batch_idx = batch_idx,\n",
    "                encoder=torus_ae.encoder2lifting,decoder = torus_ae.decoder_torus,\n",
    "                OOD_params_dict = yaml_config[\"OOD_settings\"],\n",
    "                output_dim=yaml_config[\"architecture\"][\"input_dim\"])\n",
    "            \n",
    "            if (batch_idx % OOD_params_dict[\"T_ood\"] == 0) & (batch_idx >= yaml_config[\"OOD_settings\"][\"start_ood\"]):\n",
    "                OOD_curvature_loss = ricci_regularization.OODTools.curv_loss_on_OOD_samples(\n",
    "                    extreme_curv_points_tensor = extreme_curv_points_tensor,\n",
    "                    decoder = torus_ae.decoder_torus,\n",
    "                    OOD_params_dict = yaml_config[\"OOD_settings\"],\n",
    "                    latent_space_dim = yaml_config[\"architecture\"][\"latent_dim\"] )\n",
    "                if yaml_config[\"training_mode\"][\"diagnostic_mode\"] == True:\n",
    "                    print(\"Curvature functional at OOD points\", OOD_curvature_loss)\n",
    "                loss = yaml_config[\"OOD_settings\"][\"OOD_w\"] * OOD_curvature_loss\n",
    "            #end if\n",
    "        #end if\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Progress bar plotting\n",
    "        t.set_postfix(dict_loss2print)\n",
    "        # Switching batch index\n",
    "        batch_idx += 1\n",
    "    #end for\n",
    "    return batch_idx, dict_loss_arrays\n",
    "\n",
    "def test(dict_loss_arrays = {}, batch_idx = 0):\n",
    "    dict_loss2print = {}\n",
    "    torus_ae.to(device)\n",
    "    t = tqdm( test_loader, desc=\"Test\", position=1 )\n",
    "    for data, _ in t:\n",
    "        data = data.to(device)\n",
    "        data = data.reshape(-1, yaml_config[\"architecture\"][\"input_dim\"])\n",
    "        recon_batch, z = torus_ae(data)\n",
    "        dict_losses = loss_function(recon_batch, data, z,torus_ae = torus_ae, config=yaml_config)\n",
    "        # appending current losses to loss history    \n",
    "        for key in dict_losses.keys():\n",
    "            if batch_idx == 0:\n",
    "                dict_loss_arrays[key] = []\n",
    "            dict_loss_arrays[key].append(dict_losses[key].item())\n",
    "            # mean losses to print\n",
    "            dict_loss2print[key] = f\"{dict_losses[key]:.4f}\"\n",
    "#            dict_loss2print[key] = f\"{np.array(dict_loss_arrays[key]).mean():.4f}\"\n",
    "        t.set_postfix(dict_loss2print)\n",
    "        # switch batch index\n",
    "        batch_idx+=1\n",
    "    #end for\n",
    "    return batch_idx, dict_loss_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = 0\n",
    "test_batch_idx = 0\n",
    "dict_loss_arrays = {}\n",
    "test_dict_loss_arrays = {}\n",
    "\n",
    "# Launch\n",
    "for epoch in range(1, yaml_config[\"optimizer_settings\"][\"num_epochs\"] + 1):\n",
    "  torus_ae.to(device)\n",
    "  batch_idx, dict_loss_arrays = train(epoch=epoch,batch_idx=batch_idx,dict_loss_arrays=dict_loss_arrays)\n",
    "  if yaml_config[\"training_mode\"][\"diagnostic_mode\"] == True :\n",
    "    dict2print = ricci_regularization.PlottingTools.translate_dict(dict2print=dict_loss_arrays, \n",
    "                include_curvature_plots=yaml_config[\"training_mode\"][\"compute_curvature\"],\n",
    "                eps=yaml_config[\"loss_settings\"][\"eps\"])\n",
    "    ricci_regularization.PlottingTools.plotsmart(dict2print)\n",
    "    fig = ricci_regularization.point_plot(encoder=torus_ae.encoder2lifting, data=test_dataset,\n",
    "                                          dataset_name=yaml_config[\"dataset\"][\"name\"], \n",
    "                                          batch_idx=batch_idx,config=yaml_config, device = device)\n",
    "    #fig.show()\n",
    "  #else:\n",
    "  #  ricci_regularization.PlottingTools.plotfromdict(dict_of_losses=dict_loss_arrays)  \n",
    "  if (yaml_config[\"dataset\"][\"name\"] == \"MNIST01\"):\n",
    "    ricci_regularization.PlottingTools.plot_ae_outputs_selected(\n",
    "      test_dataset=test_dataset,\n",
    "      encoder=torus_ae.cpu().encoder2lifting,\n",
    "      decoder=torus_ae.cpu().decoder_torus,\n",
    "      selected_labels=yaml_config[\"dataset\"][\"selected_labels\"] )\n",
    "  elif (yaml_config[\"dataset\"][\"name\"] == \"MNIST\"):\n",
    "    ricci_regularization.PlottingTools.plot_ae_outputs(\n",
    "      test_dataset=test_dataset,\n",
    "      encoder=torus_ae.cpu().encoder2lifting,\n",
    "      decoder=torus_ae.cpu().decoder_torus )\n",
    "  # end if\n",
    "  # Test \n",
    "  torus_ae.to(device)\n",
    "  test_batch_idx,test_dict_loss_arrays = test(batch_idx=test_batch_idx,dict_loss_arrays=test_dict_loss_arrays) \n",
    "  # end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Report of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torus_ae.state_dict(), f'{Path_pictures}/ae_weights.pt')\n",
    "print(\"AE weights saved at:\", Path_pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test losses, $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test losses\n",
    "_,dict_test_losses = test()\n",
    "\n",
    "test_mse = np.array(dict_test_losses[\"MSE\"]).mean()\n",
    "# collect test batches in a list and then concatenate to get one tensor for test data\n",
    "\n",
    "list = []\n",
    "for data,_ in test_loader:\n",
    "    list.append(data.float())\n",
    "# compute variance\n",
    "var = torch.var(torch.cat(list).flatten())\n",
    "# compute R^2\n",
    "test_R_squared = 1 - test_mse/var\n",
    "#printing\n",
    "\n",
    "print(\"Test losses\")\n",
    "print(f\"MSE:, {test_mse.item():.4f}\")\n",
    "print(f\"R²: {test_R_squared.item():.4f}\")\n",
    "\n",
    "\n",
    "test_unif = np.array(dict_test_losses[\"Uniform\"]).mean()\n",
    "try:\n",
    "    test_curv = np.array(dict_test_losses[\"Curvature\"]).mean()\n",
    "    print(f\"Curvature: {test_curv.item():.4f}\")\n",
    "except KeyError:\n",
    "    test_curv = \"not_computed\"\n",
    "    print(test_curv)\n",
    "print(f\"Uniform:, {test_unif.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss ploting\n",
    "if yaml_config[\"training_mode\"][\"diagnostic_mode\"] == True:\n",
    "    #fig,axes = ricci_regularization.PlottingTools.plotsmart(dict2print)\n",
    "    fig,axes = ricci_regularization.PlottingTools.PlotSmartConvolve(dict2print)\n",
    "else:\n",
    "    fig,axes = ricci_regularization.PlottingTools.plotfromdict(dict_loss_arrays)\n",
    "fig.savefig(f\"{Path_pictures}/losses.pdf\",bbox_inches='tight',format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it is optimal\n",
    "# saving loss history\n",
    "torch.save(dict_loss_arrays, f'{Path_pictures}/losses_history.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torus latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torus_ae.cpu() # switch device to cpu for plotting\n",
    "fig = ricci_regularization.point_plot(encoder=torus_ae.encoder2lifting, data=test_dataset,\n",
    "                                      dataset_name = yaml_config[\"dataset\"][\"name\"], \n",
    "                                      batch_idx=batch_idx,config=yaml_config, \n",
    "                                      show_title=False, device = \"cpu\", figsize=(9,9))\n",
    "fig.savefig( Path_pictures + \"/latent_space.pdf\", bbox_inches = 'tight', format = \"pdf\" )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving test losses in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_results = {\n",
    "        \"R^2_test_data\": test_R_squared.item(),\n",
    "        \"mse_loss_test_data\": test_mse.item(),  \n",
    "        \"unif_loss_test_data\": test_unif.item()  \n",
    "}\n",
    "if yaml_config[\"training_mode\"][\"compute_curvature\"] == True:\n",
    "    json_results[\"curv_loss_test_data\"] = test_curv.item()\n",
    "else:\n",
    "    json_results[\"curv_loss_test_data\"] = \"Not computed\"\n",
    "\n",
    "with open(f\"{Path_pictures}/results.json\", 'w') as json_file:\n",
    "    json.dump(json_results, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
