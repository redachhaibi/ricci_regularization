{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.func import jacrev,jacfwd\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch.func as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_name = \"Swissroll\"\n",
    "#set_name = \"Synthetic\"\n",
    "set_name = \"MNIST\"\n",
    "\n",
    "experiment_name = f\"{set_name}_torus_AE\"\n",
    "experiment_number = 34\n",
    "violent_saving = True # if False it will not save plots\n",
    "Path_experiments = \"../experiments/\"\n",
    "Path_pictures = f\"../experiments/{experiment_name}/experiment{experiment_number}\"\n",
    "if violent_saving == True:\n",
    "    if os.path.exists(Path_pictures) == False:\n",
    "        if os.path.exists(f\"../experiments/{experiment_name}/\") == False:\n",
    "            os.mkdir(f\"../experiments/{experiment_name}/\")\n",
    "        os.mkdir(Path_pictures) # needs to be commented once the folder for plots is created\n",
    "Path_weights = \"../nn_weights/\"\n",
    "\n",
    "d = 2         # latent space dimension\n",
    "weights_loaded = False\n",
    "weights_saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_type = \"TorusAE\" # \n",
    "#architecture_type = \"TorusConvAE\" #\n",
    "\n",
    "mse_w = 1.0 # 1e1\n",
    "unif_w = 0.5e-3#1e-3 #0.5 #5e0 # 4e1 for mnist\n",
    "num_moments = 4\n",
    "lambda_contractive_decoder = 0.\n",
    "delta_decoder = 2 # \\delta\n",
    "#djnpm = \"mean\" #decoder_jac_norm_penalization_mode \n",
    "\n",
    "lambda_contractive_encoder = 0.#1e-4 #1e-5 #10.\n",
    "delta_encoder = 0. #1e+1\n",
    "#ejnpm = \"max\" #encoder_jac_norm_penalization_mode\n",
    "\n",
    "eps = 0.0\n",
    "\n",
    "diagnostic_mode = True\n",
    "compute_curvature = True\n",
    "curvature_penalization_mode = \"mean\" #\"mean\"\n",
    "\n",
    "if compute_curvature == True:\n",
    "    curv_w =  0.1 \n",
    "else:\n",
    "    curv_w = 0.\n",
    "\n",
    "delta_curv = 0.1\n",
    "    \n",
    "OOD_regime = False\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr         = 1e-3\n",
    "num_epochs = 40\n",
    "#num_batches = 15000\n",
    "\n",
    "#curv_w_increase_rate = 4/num_epochs\n",
    "\n",
    "# Hyperparameters for data loaders\n",
    "batch_size  = 128 # was 256 for MNIST\n",
    "split_ratio = 0.2\n",
    "weight_decay = 0.\n",
    "random_shuffling = False\n",
    "random_seed = 0\n",
    "Force_CPU = False\n",
    "\n",
    "# Set manual seed for reproducibility\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD sampling parameters\n",
    "T_ood = 20 # 100 # period of OOD penalization\n",
    "n_ood = 5 # number of OOD samples per point\n",
    "sigma_ood = 2e-1 # sigma of OOD Gaussian samples: 2e-1 swissroll\n",
    "N_extr = 16 # 32 batch size of extremal curvature points\n",
    "r_ood = 1e-3 # 1e-2 decay factor\n",
    "OOD_w = curv_w\n",
    "start_ood = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set uploading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('../') # have to go 1 level up\n",
    "import ricci_regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers in DataLoader\n",
    "num_workers = 11\n",
    "\n",
    "if set_name == \"MNIST\":\n",
    "    D = 784\n",
    "    k = 10 # number of classes\n",
    "    #MNIST_SIZE = 28\n",
    "    # MNIST Dataset\n",
    "    train_dataset = datasets.MNIST(root='../datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset  = datasets.MNIST(root='../datasets/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "    set_parameters = {\"k\" : k}\n",
    "elif set_name == \"Synthetic\":\n",
    "    D = 784       #dimension\n",
    "    k = 3         # num of 2d planes in dim D\n",
    "    n = 6*(10**3) # num of points in each plane\n",
    "    shift_class = 0.0\n",
    "    var_class = 1.0\n",
    "    intercl_var = 0.1 # this has to be greater than 0.04\n",
    "    # this creates a gaussian, \n",
    "    # i.e.random shift \n",
    "    # proportional to the value of intercl_var\n",
    "    # Generate dataset\n",
    "    # via classes\n",
    "    torch.manual_seed(0) # reproducibility\n",
    "    my_dataset = ricci_regularization.SyntheticDataset(k=k,n=n,d=d,D=D,\n",
    "                                        shift_class=shift_class, intercl_var=intercl_var, var_class=var_class)\n",
    "\n",
    "    train_dataset = my_dataset.create\n",
    "    set_parameters = {\n",
    "    \"k\" : k,\n",
    "    \"n\" : n,\n",
    "    \"shift_class\" : shift_class,\n",
    "    \"var_class\" : var_class,\n",
    "    \"intercl_var\" : intercl_var\n",
    "    }\n",
    "elif set_name == \"Swissroll\":\n",
    "    D = 3\n",
    "    sr_noise = 1e-6\n",
    "    sr_numpoints = 18000 #k*n\n",
    "    train_dataset =  sklearn.datasets.make_swiss_roll(n_samples=sr_numpoints, noise=sr_noise,random_state=random_seed)\n",
    "    sr_points = torch.from_numpy(train_dataset[0]).to(torch.float32)\n",
    "    #sr_points = torch.cat((sr_points,torch.zeros(sr_numpoints,D-3)),dim=1)\n",
    "    sr_colors = torch.from_numpy(train_dataset[1]).to(torch.float32)\n",
    "    from torch.utils.data import TensorDataset\n",
    "    train_dataset = TensorDataset(sr_points,sr_colors)\n",
    "    set_parameters = {\n",
    "    \"sr_noise\" : sr_noise,\n",
    "    \"sr_numpoints\" : sr_numpoints\n",
    "    }\n",
    "\n",
    "m = len(train_dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(train_dataset, [int(m-m*split_ratio), int(m*split_ratio)])\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(test_data , batch_size=batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=random_shuffling) # was true\n",
    "batches_per_epoch = len(train_loader)\n",
    "#start_ood = batches_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "if architecture_type == \"TorusConvAE\":\n",
    "    torus_ae = ricci_regularization.Architectures.TorusConvAE(x_dim=D, h_dim1= 512, h_dim2=256, z_dim=d,pixels=28)\n",
    "else:\n",
    "    torus_ae = ricci_regularization.Architectures.TorusAE(x_dim=D, h_dim1= 512, h_dim2=256, z_dim=d)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    torus_ae.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weights_loaded == True:\n",
    "    PATH_weights_loaded = f'../nn_weights/{set_name}_exp{experiment_number-1}.pt'\n",
    "    torus_ae.load_state_dict(torch.load(PATH_weights_loaded))\n",
    "    torus_ae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(torus_ae.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "#Force CPU\n",
    "if Force_CPU == True:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Move the AE to the selected device\n",
    "torus_ae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curv_func(encoded_data, function=torus_ae.decoder_torus):\n",
    "    metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=function)\n",
    "    det_on_data = torch.det(metric_on_data)\n",
    "    Sc_on_data = ricci_regularization.Sc_jacfwd_vmap(encoded_data,\n",
    "                                           function=function)\n",
    "    N = metric_on_data.shape[0]\n",
    "    Integral_of_Sc = (1/N)*(torch.sqrt(det_on_data)*torch.square(Sc_on_data)).sum()\n",
    "    #Integral_of_Sc = (1/N)*(torch.sqrt(det_on_data)*(Sc_on_data**4)).sum()\n",
    "    return Integral_of_Sc\n",
    "\"\"\"\n",
    "# minimizing |g-I|_F\n",
    "def curv_func(encoded_data, function=torus_ae.decoder_torus):\n",
    "    metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=function)\n",
    "    N = metric_on_data.shape[0]\n",
    "    func = (1/N)*(metric_on_data-torch.eye(d)).norm(dim=(1,2)).sum()\n",
    "    return func\n",
    "\"\"\"\n",
    "    \n",
    "# Loss = MSE + uniform_loss + curv_loss\n",
    "#  where the uniform_loss uses modulis of Fourier modes, of the empirical distribution.\n",
    "#  This requires batch size to be in the range of CLT.\n",
    "#\n",
    "# Inputs:\n",
    "#   recon_data: reconstructed data via decoder\n",
    "#   data: original data\n",
    "#   z: latent variable\n",
    "def loss_function(recon_data, data, z, decoder,device=device,compute_curvature = compute_curvature,diagnostic_mode = diagnostic_mode):\n",
    "    MSE = F.mse_loss(recon_data, data.view(-1, D), reduction='mean')\n",
    "    # Splits sines and cosines\n",
    "    z_sin = z[:, 0:d]\n",
    "    z_cos = z[:, d:2*d]\n",
    "    #\n",
    "    # Compute empirical first mode\n",
    "    mode1 = torch.mean( z, dim = 0)\n",
    "    mode1 = torch.sum( mode1*mode1 )\n",
    "    #\n",
    "    # Compute empirical second mode\n",
    "    mode2_1 = torch.mean( 2*z_cos*z_cos-1, dim = 0)\n",
    "    mode2_1 = torch.sum( mode2_1*mode2_1)\n",
    "    mode2_2 = torch.mean( 2*z_sin*z_cos, dim = 0)\n",
    "    mode2_2 = torch.sum( mode2_2*mode2_2 )\n",
    "    mode2 = mode2_1 + mode2_2\n",
    "    #\n",
    "    unif_loss = mode1 + mode2\n",
    "    #\n",
    "    if num_moments > 2:\n",
    "        mode3_1 = torch.mean( 4*z_cos**3-3*z_cos, dim = 0)\n",
    "        mode3_1 = torch.sum( mode3_1*mode3_1)\n",
    "        mode3_2 = torch.mean( z_sin*(8*z_cos**3-4*z_cos), dim = 0)\n",
    "        mode3_2 = torch.sum( mode3_2*mode3_2 )\n",
    "        mode3 = mode3_1 + mode3_2\n",
    "        unif_loss += mode3\n",
    "    #\n",
    "    if num_moments > 3:\n",
    "        mode4_1 = torch.mean( 8*z_cos**4-8*z_cos**2+1, dim = 0)\n",
    "        mode4_1 = torch.sum( mode4_1*mode4_1)\n",
    "        mode4_2 = torch.mean( z_sin*(16*z_cos**4-12*z_cos**2+1), dim = 0)\n",
    "        mode4_2 = torch.sum( mode4_2*mode4_2 )\n",
    "        mode4 = mode4_1 + mode4_2\n",
    "        unif_loss += mode4\n",
    "    dict_losses = {\n",
    "        \"MSE\": MSE,\n",
    "        \"uniform_loss\": unif_loss,\n",
    "    }\n",
    "    \n",
    "    if compute_curvature == True:\n",
    "        encoded_points_no_grad = torus_ae.encoder2lifting(data.view(-1, D)).detach()\n",
    "        #curv_loss = curv_func(encoded_points_no_grad,function=torus_ae.decoder_torus)\n",
    "\n",
    "        metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_points_no_grad,\n",
    "                                           function=decoder)\n",
    "        det_on_data = torch.det(metric_on_data)\n",
    "        Sc_on_data = ricci_regularization.Sc_jacfwd_vmap(encoded_points_no_grad,\n",
    "                                           function=decoder,device=device,eps=eps)\n",
    "        \n",
    "        if curvature_penalization_mode == \"mean\":\n",
    "            curv_loss = (torch.sqrt(det_on_data)*torch.square(Sc_on_data)).mean()\n",
    "        elif curvature_penalization_mode == \"max\":\n",
    "            curv_outlyers = torch.nn.ReLU()(torch.sqrt(det_on_data)*torch.square(Sc_on_data) - delta_curv)\n",
    "            curv_loss = (curv_outlyers).max()\n",
    "        \n",
    "        dict_losses[\"curvature_loss\"] = curv_loss\n",
    "        if diagnostic_mode == True:\n",
    "            curv_squared_mean = (torch.square(Sc_on_data)).mean()\n",
    "            curv_squared_max = (torch.square(Sc_on_data)).max()\n",
    "            dict_losses[\"curv_squared_mean\"] = curv_squared_mean\n",
    "            dict_losses[\"curv_squared_max\"] = curv_squared_max\n",
    "    if diagnostic_mode == True:\n",
    "        if compute_curvature == False:\n",
    "            encoded_points_no_grad = torus_ae.encoder2lifting(data.view(-1, D)).detach()\n",
    "            metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_points_no_grad,function=decoder)\n",
    "            det_on_data = torch.det(metric_on_data)    \n",
    "        #regularization term\n",
    "        #eps = 0.01\n",
    "        g_inv_train_batch = torch.linalg.inv(metric_on_data + eps*torch.eye(d).to(device))\n",
    "        g_inv_norm_train_batch = torch.linalg.matrix_norm(g_inv_train_batch)\n",
    "        g_inv_norm_mean = torch.mean(g_inv_norm_train_batch)\n",
    "        g_inv_norm_max = torch.max(g_inv_norm_train_batch)\n",
    "        g_det_mean = det_on_data.mean()\n",
    "        g_det_max = det_on_data.max()\n",
    "        g_det_min = det_on_data.min()\n",
    "\n",
    "        decoder_jac_norm = torch.func.vmap(torch.trace)(metric_on_data)\n",
    "        decoder_jac_norm_mean = decoder_jac_norm.mean()\n",
    "        decoder_jac_norm_max = decoder_jac_norm.max()\n",
    "        dict_losses[\"g_inv_norm_mean\"] = g_inv_norm_mean\n",
    "        dict_losses[\"g_inv_norm_max\"] = g_inv_norm_max\n",
    "        dict_losses[\"g_det_mean\"] = g_det_mean\n",
    "        dict_losses[\"g_det_max\"] = g_det_max\n",
    "        dict_losses[\"g_det_min\"] = g_det_min\n",
    "        \n",
    "        # decoder jac\n",
    "        dict_losses[\"decoder_jac_norm_mean\"] = decoder_jac_norm_mean\n",
    "        dict_losses[\"decoder_jac_norm_max\"] = decoder_jac_norm_max\n",
    "\n",
    "        outlyers_decoder_norm = torch.nn.ReLU()(decoder_jac_norm - delta_decoder)\n",
    "        dict_losses[\"decoder_contractive_loss\"] = (outlyers_decoder_norm).max()\n",
    "\n",
    "        # encoder jac. THIS is VERY suboptimal for large D!!! Redo with  \n",
    "        metric_array_encoder = ricci_regularization.metric_jacrev_vmap(data.view(-1, D),\n",
    "                                                                       function=torus_ae.encoder_torus,\n",
    "                                                                       latent_space_dim=D)\n",
    "        encoder_jac_norm = torch.func.vmap(torch.trace)(metric_array_encoder)\n",
    "        encoder_jac_norm_mean = encoder_jac_norm.mean()\n",
    "        encoder_jac_norm_max = encoder_jac_norm.max()\n",
    "\n",
    "        dict_losses[\"encoder_jac_norm_mean\"] = encoder_jac_norm_mean\n",
    "        dict_losses[\"encoder_jac_norm_max\"] = encoder_jac_norm_max\n",
    "        outlyers_encoder_norm = torch.nn.ReLU()(encoder_jac_norm - delta_encoder)\n",
    "        #num_outlyers = torch.nonzero(outlyers_encoder_norm).flatten().shape[0]\n",
    "        #dict_losses[\"encoder_contractive_loss\"] = (1/num_outlyers)*(outlyers_encoder_norm).sum()\n",
    "        dict_losses[\"encoder_contractive_loss\"] = (outlyers_encoder_norm).max()\n",
    "\n",
    "    return dict_losses\n",
    "\"\"\"\n",
    "dict_losses = {\n",
    "        \"MSE\": MSE,\n",
    "        \"uniform_loss\": unif_loss,\n",
    "        \"curvature_loss\":curv_loss,\n",
    "        \"curv_squared_mean\":curv_squared_mean,\n",
    "        \"curv_squared_max\":curv_squared_max,\n",
    "        \"g_inv_norm_mean\":g_inv_norm_mean,\n",
    "        \"g_inv_norm_max\":g_inv_norm_max,\n",
    "        \"g_det_mean\":g_det_mean,\n",
    "        \"g_det_max\":g_det_max,\n",
    "        \"decoder_jac_norm_mean\": decoder_jac_norm_mean,\n",
    "        \"decoder_jac_norm_max\": decoder_jac_norm_max\n",
    "    }\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1,batch_idx = 0,dict_loss_arrays={},diagnostic_mode = False):\n",
    "    if batch_idx == 0:\n",
    "        dict_loss_arrays = {}\n",
    "    torus_ae.train()\n",
    "    print(\"Epoch %d\"%epoch)\n",
    "    t = tqdm( train_loader, desc=\"Train\", position=0 )\n",
    "    \n",
    "    for (data, labels) in t:\n",
    "        #data = data.cuda()\n",
    "        #data = data.cpu()\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward\n",
    "        \"\"\"\n",
    "        if architecture_type == \"TorusConvAE\":\n",
    "            recon_batch, z = decoder(encoder(data)) , encoder(data)\n",
    "        else:\n",
    "            recon_batch, z = torus_ae(data)\n",
    "        \"\"\"\n",
    "        recon_batch, z = torus_ae(data)\n",
    "        #mse_loss, uniform_loss, curvature_loss, g_inv_loss, curvature_squared = loss_function(recon_batch, data, z,decoder=torus_ae.decoder_torus)\n",
    "        \n",
    "        dict_losses = loss_function(recon_batch, data, z,decoder=torus_ae.decoder_torus,diagnostic_mode=diagnostic_mode)\n",
    "        mse_loss = dict_losses[\"MSE\"]\n",
    "        uniform_loss = dict_losses[\"uniform_loss\"]\n",
    "        loss = mse_w*mse_loss + unif_w*uniform_loss \n",
    "\n",
    "        #relu_function = torch.nn.ReLU()\n",
    "        #decoder_penalty = relu_function( dict_losses[f\"decoder_jac_norm_{djnpm}\"]- delta_decoder)\n",
    "        #encoder_penalty = relu_function( dict_losses[f\"encoder_jac_norm_{ejnpm}\"]- delta_encoder)\n",
    "        if diagnostic_mode == True:\n",
    "            encoder_contractive_loss = dict_losses[\"encoder_contractive_loss\"]\n",
    "            decoder_contractive_loss = dict_losses[\"decoder_contractive_loss\"]\n",
    "            loss = loss + + lambda_contractive_decoder*decoder_contractive_loss + lambda_contractive_encoder*encoder_contractive_loss#lambda_contractive_encoder*encoder_penalty\n",
    "\n",
    "\n",
    "        \n",
    "        if (compute_curvature == True): \n",
    "            curvature_loss = dict_losses[\"curvature_loss\"]\n",
    "            loss = loss + curv_w*curvature_loss\n",
    "        else:\n",
    "            curvature_loss_mean_per_epoch = \"nan\"\n",
    "        \n",
    "        # OOD regime (optional)\n",
    "        if OOD_regime == True:\n",
    "            if batch_idx == 0:\n",
    "                extreme_curv_points_tensor = torus_ae.encoder2lifting(data.view(-1,D)[:N_extr]).detach()\n",
    "                extreme_curv_value_tensor = ricci_regularization.Sc_jacfwd_vmap(extreme_curv_points_tensor, function=torus_ae.decoder_torus,eps=eps)\n",
    "            extreme_curv_points_tensor, extreme_curv_value_tensor = ricci_regularization.find_extreme_curvature_points(data_batch=data,\n",
    "                                extreme_curv_points_tensor=extreme_curv_points_tensor,\n",
    "                                extreme_curv_value_tensor=extreme_curv_value_tensor,batch_idx=batch_idx,\n",
    "                                encoder=torus_ae.encoder2lifting,decoder=torus_ae.decoder_torus,r_ood=r_ood,N_extr=N_extr,output_dim=D)\n",
    "            \n",
    "            if (batch_idx % T_ood == 0) & (batch_idx > start_ood):\n",
    "                OOD_curvature_loss = ricci_regularization.OODTools.curv_loss_on_OOD_samples(extreme_curv_points_tensor=extreme_curv_points_tensor,\n",
    "                                                                                            decoder=torus_ae.decoder_torus,\n",
    "                                                                                            sigma_ood=sigma_ood,n_ood=n_ood,N_extr=N_extr,\n",
    "                                                                                            latent_space_dim=d)\n",
    "                if diagnostic_mode == True:\n",
    "                    print(\"Curvature functional at OOD points\", OOD_curvature_loss)\n",
    "                loss = OOD_w * OOD_curvature_loss\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #appending losses per batch to loss arrays\n",
    "        if batch_idx == 0:\n",
    "            #dict_loss_arrays[\"decoder_penalty\"] = []\n",
    "            #dict_loss_arrays[\"encoder_penalty\"] = []\n",
    "            for key in dict_losses.keys():\n",
    "                dict_loss_arrays[key] = []\n",
    "        #dict_loss_arrays[\"decoder_penalty\"].append(decoder_penalty.item())\n",
    "        #dict_loss_arrays[\"encoder_penalty\"].append(decoder_penalty.item())\n",
    "        for key in dict_losses.keys():\n",
    "            dict_loss_arrays[key].append(dict_losses[key].item())\n",
    "        \n",
    "        # Progress bar\n",
    "        batch_idx += 1\n",
    "        MSE_mean_per_epoch = np.array(dict_loss_arrays[\"MSE\"])[-batches_per_epoch:].mean()\n",
    "        uniform_loss_mean_per_epoch = np.array(dict_loss_arrays[\"uniform_loss\"])[-batches_per_epoch:].mean()    \n",
    "        if compute_curvature == True:\n",
    "            curvature_loss_mean_per_epoch = np.array(dict_loss_arrays[\"curvature_loss\"])[-batches_per_epoch:].mean()\n",
    "        else:\n",
    "            curvature_loss_mean_per_epoch = \"nan\"\n",
    "        #decoder_penalty_mean_per_epoch = np.array(dict_loss_arrays[\"decoder_contractive_loss\"])[-batches_per_epoch:].mean()\n",
    "        if diagnostic_mode == True:\n",
    "            encoder_contractive_loss_mean_per_epoch = np.array(dict_loss_arrays[\"encoder_contractive_loss\"])[-batches_per_epoch:].mean()\n",
    "            t.set_description_str(desc=f\"MSE:{MSE_mean_per_epoch}, Uniform:{uniform_loss_mean_per_epoch}, Encoder_penalty:{encoder_contractive_loss_mean_per_epoch}, Curvature:{curvature_loss_mean_per_epoch}.\\n\")\n",
    "        else:\n",
    "            t.set_description_str(desc=f\"MSE:{MSE_mean_per_epoch}, Uniform:{uniform_loss_mean_per_epoch}\")\n",
    "        #if batch_idx > num_batches:\n",
    "        #    break\n",
    "    #end for\n",
    "    return batch_idx, dict_loss_arrays\n",
    "\n",
    "def test(mse_loss_array=[], uniform_loss_array=[], curvature_loss_array = [],g_inv_loss_array=[],curvature_squared_array=[]):\n",
    "    torus_ae.eval()\n",
    "    with torch.no_grad():\n",
    "        t = tqdm( test_loader, desc=\"Test\", position=1 )\n",
    "        for data, _ in t:\n",
    "            data = data.cpu()\n",
    "            recon_batch, z = torus_ae(data)\n",
    "            dict_losses = loss_function(recon_batch, data, z,decoder=torus_ae.decoder_torus)\n",
    "            mse_loss = dict_losses[\"MSE\"]\n",
    "            uniform_loss = dict_losses[\"uniform_loss\"]\n",
    "            if compute_curvature == True:\n",
    "                curvature_loss = dict_losses[\"curvature_loss\"]\n",
    "            else:\n",
    "                curvature_loss = torch.zeros(1)\n",
    "        \n",
    "            mse_loss_array.append(mse_loss.item())\n",
    "            uniform_loss_array.append(uniform_loss.item())\n",
    "            curvature_loss_array.append(curvature_loss.item())\n",
    "    print(f\"Test losses. \\nMSE:{np.array(mse_loss_array).mean()}, Uniform:{np.array(uniform_loss_array).mean()}, Curvature:{np.array(curvature_loss_array).mean()}.\\n\")\n",
    "    return mse_loss_array, uniform_loss_array, curvature_loss_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx=0\n",
    "dict_loss_arrays = {}\n",
    "#diagnostic_mode = False\n",
    "\n",
    "# Launch\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  torus_ae.to(device)\n",
    "  batch_idx, dict_loss_arrays = train(epoch=epoch,batch_idx=batch_idx,dict_loss_arrays=dict_loss_arrays,\n",
    "                                                 diagnostic_mode=diagnostic_mode)\n",
    "  if diagnostic_mode == True :\n",
    "    dict2print = ricci_regularization.PlottingTools.translate_dict(dict2print=dict_loss_arrays, include_curvature_plots=compute_curvature,eps=eps)\n",
    "    ricci_regularization.PlottingTools.plotsmart(dict2print)\n",
    "  #else:\n",
    "  #  ricci_regularization.PlottingTools.plotfromdict(dict_of_losses=dict_loss_arrays)\n",
    "      \n",
    "  # update curvature weight\n",
    "  #curv_w = curv_w * 10**(curv_w_increase_rate)\n",
    "    \n",
    "  if (set_name == \"MNIST\"): #& (architecture_type == \"TorusAE\"):\n",
    "    ricci_regularization.PlottingTools.plot_ae_outputs(test_dataset=test_dataset,\n",
    "                                                       encoder=torus_ae.cpu().encoder2lifting,\n",
    "                                                       decoder=torus_ae.cpu().decoder_torus)\n",
    "  #test() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test losses and $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_R_squared_losses(data_loader=test_loader):\n",
    "    len_test_loader = len(test_loader)\n",
    "    mse_loss = 0\n",
    "    curv_loss = 0\n",
    "    unif_loss = 0\n",
    "    input_dataset_list = []\n",
    "    torus_ae.to(device)\n",
    "    for (data, labels) in test_loader:\n",
    "        data = data.to(device)\n",
    "        input_dataset_list.append(data.cpu())\n",
    "        input = data\n",
    "        recon = torus_ae(data)[0]\n",
    "        z = torus_ae(data)[1]\n",
    "        enc = torus_ae.encoder2lifting(data.view(-1,D))\n",
    "        dict_losses = loss_function(recon, input, z,\n",
    "                                    device=device,\n",
    "                                    decoder=torus_ae.decoder_torus, \n",
    "                                    compute_curvature=True,diagnostic_mode=False)\n",
    "        mse_loss += dict_losses[\"MSE\"].cpu().detach()/len_test_loader\n",
    "        unif_loss += dict_losses[\"uniform_loss\"].cpu().detach()/len_test_loader\n",
    "        curv_loss += dict_losses[\"curvature_loss\"].cpu().detach()/len_test_loader\n",
    "    input_dataset_tensor = torch.cat(input_dataset_list).view(-1,D)\n",
    "    var = torch.var(input_dataset_tensor.flatten())\n",
    "    R_squared = 1 - mse_loss/var\n",
    "    return mse_loss, unif_loss, curv_loss, R_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_unif, test_curv, test_R_squared = compute_R_squared_losses(test_loader)\n",
    "#train_mse, train_unif, train_curv, train_R_squared, train_decoder_penalty = compute_R_squared_losses(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Train losses:\\nmse:{train_mse}, unif_loss:{train_unif}, decoder_penalty:{train_decoder_penalty}, curv_loss:{train_curv}\")\n",
    "#print(f\"R_squared: {train_R_squared.item():.4f}\")\n",
    "#print(f\"Test losses:\\nmse:{test_mse}\")\n",
    "print(f\"Test losses:\\nmse:{test_mse}, unif_loss:{test_unif}, curv_loss:{test_curv}\")\n",
    "print(f\"R_squared: {test_R_squared.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "from torcheval.metrics import R2Score\n",
    "R_squared = R2Score()#(multioutput=\"raw_values\")\n",
    "input = input_dataset_tensor.flatten()\n",
    "target = recon_dataset_tensor.flatten()\n",
    "R_squared.update(input, target)\n",
    "R_squared.compute()#.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weights_saved == True:\n",
    "    PATH_vae = f'../nn_weights/{set_name}_exp{experiment_number}.pt'\n",
    "    torch.save(torus_ae.state_dict(), PATH_vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss ploting\n",
    "if diagnostic_mode == True:\n",
    "    #fig,axes = ricci_regularization.PlottingTools.plotsmart(dict2print)\n",
    "    fig,axes = ricci_regularization.PlottingTools.PlotSmartConvolve(dict2print)\n",
    "else:\n",
    "    fig,axes = ricci_regularization.PlottingTools.plotfromdict(dict_loss_arrays)\n",
    "if violent_saving == True:\n",
    "    fig.savefig(f\"{Path_pictures}/losses_exp{experiment_number}.pdf\",bbox_inches='tight',format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig,axes = ricci_regularization.PlottingTools.plot9losses(mse_loss_array,curvature_loss_array,g_inv_meanperbatch_array)\n",
    "if violent_saving == True:\n",
    "    fig.savefig(f\"{Path_pictures}/9losses_exp{experiment_number}.pdf\",bbox_inches='tight',format=\"pdf\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torus latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspiration for torus_ae.encoder2lifting\n",
    "\"\"\"\n",
    "def circle2anglevectorized(zLatentTensor,d = d):\n",
    "    cosphi = zLatentTensor[:, 0:d]\n",
    "    sinphi = zLatentTensor[:, d:2*d]\n",
    "    phi = torch.acos(cosphi)*torch.sgn(torch.asin(sinphi))\n",
    "    return phi\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zlist = []\n",
    "torus_ae.cpu()\n",
    "colorlist = []\n",
    "enc_list = []\n",
    "input_dataset_list = []\n",
    "recon_dataset_list = []\n",
    "for (data, labels) in tqdm( train_loader, position=0 ):\n",
    "#for (data, labels) in train_loader:\n",
    "    input_dataset_list.append(data)\n",
    "    recon_dataset_list.append(torus_ae(data)[0])\n",
    "    #zlist.append(torus_ae(data)[1])\n",
    "    enc_list.append(torus_ae.encoder2lifting(data.view(-1,D)))\n",
    "    colorlist.append(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = torch.cat(input_dataset_list)\n",
    "recon_dataset = torch.cat(recon_dataset_list)\n",
    "encoded_points = torch.cat(enc_list)\n",
    "encoded_points_no_grad = encoded_points.detach()/math.pi\n",
    "color_array = torch.cat(colorlist).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "if set_name == \"Swissroll\":\n",
    "    my_cmap = \"jet\"\n",
    "else:\n",
    "    my_cmap = ricci_regularization.PlottingTools.discrete_cmap(k, 'jet')\n",
    "plt.scatter(encoded_points_no_grad[:,0],encoded_points_no_grad[:,1], c=color_array, marker='o', edgecolor='none', cmap=my_cmap)\n",
    "\n",
    "if set_name in [\"Synthetic\",\"MNIST\"]:\n",
    "    plt.colorbar(ticks=range(k))\n",
    "plt.grid(True)\n",
    "if violent_saving == True:\n",
    "    plt.savefig(f\"{Path_pictures}/latent_space_exp{experiment_number}.pdf\",bbox_inches='tight',format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_config = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"experiment_number\": experiment_number,\n",
    "    \"dataset\":\n",
    "    {\n",
    "        \"name\": set_name,\n",
    "        \"parameters\": set_parameters,\n",
    "    },\n",
    "    \"architecture\" :\n",
    "    {\n",
    "        \"name\":architecture_type,\n",
    "        \"input_dim\": D,\n",
    "        \"latent_dim\": d\n",
    "    },\n",
    "    \"optimization_parameters\": \n",
    "    {\n",
    "\t    \"learning_rate\": lr,\n",
    "\t    \"batch_size\": batch_size,\n",
    "        \"split_ratio\": split_ratio,\n",
    "\t    \"num_epochs\": num_epochs,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"random_shuffling\":random_shuffling,\n",
    "        \"random_seed\": random_seed,\n",
    "        \"device\": device.type\n",
    "    },\n",
    "    \"losses\":\n",
    "    {\n",
    "\t    \"mse_w\": mse_w,\n",
    "\t    \"unif_w\": unif_w,\n",
    "        \"Number of moments used\": num_moments,\n",
    "\t    \"curv_w\": curv_w,\n",
    "        \"delta_curv\": delta_curv,\n",
    "        \"curvature_penalization_mode\": curvature_penalization_mode,\n",
    "        \"g_inv regularization eps\": eps,\n",
    "        \"lambda_contractive_encoder\": lambda_contractive_encoder,\n",
    "        \"delta_encoder\" : delta_encoder,\n",
    "        \"lambda_contractive_decoder\" : lambda_contractive_decoder,\n",
    "        \"delta_decoder\" : delta_decoder,\n",
    "#        \"decoder_jac_norm_penalization_mode \" : djnpm,\n",
    "#        \"encoder_jac_norm_penalization_mode \" : ejnpm,\n",
    "        \"diagnostic_mode\": diagnostic_mode,\n",
    "        \"compute_curvature\": compute_curvature\n",
    "    },\n",
    "    \"OOD_parameters\": \n",
    "    {\n",
    "        \"OOD_regime\": OOD_regime,\n",
    "        \"start_ood\":start_ood,\n",
    "        \"T_ood\":T_ood,\n",
    "        \"n_ood\":n_ood,\n",
    "        \"sigma_ood\":sigma_ood,\n",
    "        \"N_extr\":N_extr,\n",
    "        \"r_ood\": r_ood,\n",
    "        \"OOD_w\":OOD_w\n",
    "    },\n",
    "    \"training_results_on_test_data\":\n",
    "    {\n",
    "        \"R^2\": test_R_squared.item(),\n",
    "        \"mse_loss\": test_mse.item(),\n",
    "        \"unif_loss\": test_unif.item(),\n",
    "        \"curv_loss\": test_curv.item()\n",
    "    },\n",
    "    \"Path_pictures\": Path_pictures,\n",
    "    \"Path_weights\": Path_weights,\n",
    "    \"Path_experiments\": Path_experiments,\n",
    "    \"weights_saved_at\": PATH_vae\n",
    "}\n",
    "if weights_loaded == True:\n",
    "    json_config[\"weights_loaded_from\"] = PATH_weights_loaded\n",
    "# Save dictionary to JSON file\n",
    "with open(f'{Path_experiments}/{experiment_name}exp{experiment_number}.json', 'w') as json_file:\n",
    "    json.dump(json_config, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
