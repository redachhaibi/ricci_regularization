{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torus AE training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the training of the autoencoder (AE). \n",
    "\n",
    "The AE consists of the encoder $\\Phi$ and the decoder $\\Psi$.\n",
    "The latent space of the AE is topologically a $d-$ dimensional torus $\\mathcal{T}^d$, i.e it can be considered as a periodic box $[-\\pi, \\pi]^d$. We define a Riemannian metric on the latent space  as the pull-back of the Euclidean metric in the output space $\\mathbb{R}^D$ by the decoder function $\\Psi$ of the AE:\n",
    "\\begin{equation}\n",
    "    g = \\nabla \\Psi ^* \\nabla \\Psi \\ .\n",
    "\\end{equation}\n",
    "\n",
    "The notebook consists of\n",
    "1) Choosing hyperparameters for dataset uploading, learning and plotting such as learning rate, batch size, weights of MSE loss, curvature loss, etc.\n",
    "\n",
    "One can switch between regimes: \n",
    "\"diagnostic_mode\", \"compute_curvature_mode\", \"OOD_regime\".\n",
    "\n",
    "If \"diagnostic_mode\"==True, following losses are plotted: MSE, $\\mathcal{L}_\\mathrm{unif}$, $\\mathcal{L}_\\mathrm{curv}$, $\\det(g)$, $\\|g_{reg}^{-1}\\|_F$, $\\|\\nabla \\Psi \\|^2_F$, $\\|\\nabla \\Phi \\|^2_F$, where:\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_\\mathrm{curv} := \\int_M R^2 \\mu \\ ,\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}_\\mathrm{unif} := \\sum\\limits_{k=1}^{m} |\\int_M z^k  \\mu_N (dz) |^2 \\ ,\n",
    "\\end{equation*}\n",
    "where $R$ states for scalar curvature (see https://en.wikipedia.org/wiki/Scalar_curvature), $\\mu_N = \\Phi\\# ( \\frac{1}{N}\\sum\\limits_{j=1}^{N} \\delta_{X_i} ) $ is the push-forward of the natural measure induced by the dataset by the encoder $\\Phi$, thus $\\mu_N$ is a measure on $\\mathcal{T}^d$,  $ \\alpha_k = \\frac{1}{N} \\sum_{j=1}^{N} z_j^k$ is the empirical estimator of the $k$ -th moment of the data distribution in the latent space.\n",
    "\n",
    "If $\\xi \\sim \\mathcal{U}[-\\pi, \\pi]$ and $z = e^{i \\xi}$ than all the moments of $z$ are zero, namely if $\\mathcal{L}_\\mathrm{unif} \\to 0$ as $m \\to \\infty$, one obtains weak convergence of the data distribution in the latent space to the uniform distribution.\n",
    "\n",
    "Also $g_{reg} = g + \\varepsilon \\cdot I$ is the regularized matrix of metric for stability of inverse matrix computation, $\\|\\|_F$ is the Frobenius norm of the matrix.\n",
    "\n",
    "2) Imports: Automatic loading of train and test dataloaders. Choice among data sets \"Synthetic\", \"Swissroll\", \"Mnist\". \n",
    "3) Architecture and device. Architecture types: Fully connected (TorusAE), Convolutional (TorusConvAE). Device: cuda/cpu. \n",
    "4) Training: Contains neural net training.\n",
    "5) Building a report of the performed training. Printing of graphs of losses, saves of a json file with training params.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_saving = False  # If True a folder for experiment results is created and all plots will be saved there\n",
    "\n",
    "# choose dataset\n",
    "#set_name = \"Swissroll\"\n",
    "#set_name = \"Synthetic\"\n",
    "#set_name = \"MNIST\"\n",
    "set_name = \"MNIST01\"  # MNIST01 is a dataset with ANY two labels chosen from mnist\n",
    "\n",
    "selected_labels = [5, 8]  # (Only for MNIST) Labels selected for the experiment\n",
    "\n",
    "experiment_name = f\"{set_name}_torus_AE\"  # Name of the experiment\n",
    "experiment_number = 9  # Experiment number\n",
    "\n",
    "\n",
    "# Paths for saving experiments and pictures\n",
    "Path_experiments = \"../experiments/\"\n",
    "Path_pictures = f\"../experiments/{experiment_name}/experiment{experiment_number}\"\n",
    "if violent_saving == True:  # Check if violent saving is enabled\n",
    "    if os.path.exists(Path_pictures) == False:  # Check if the picture path does not exist\n",
    "        if os.path.exists(f\"../experiments/{experiment_name}/\") == False:  # Check if the experiment directory does not exist\n",
    "            os.mkdir(f\"../experiments/{experiment_name}/\")  # Create the experiment directory, if not yet created\n",
    "        os.mkdir(Path_pictures)  # Create the directory for plots, if not yet created\n",
    "\n",
    "Path_weights = \"../nn_weights/\"  # Path for saving neural network weights\n",
    "\n",
    "d = 2  # Latent space dimension\n",
    "weights_loaded = False  # Flag indicating whether weights are loaded\n",
    "weights_saved = False  # Flag indicating whether weights should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of arhitecture\n",
    "architecture_type = \"TorusAE\"  # Type of architecture being used\n",
    "#architecture_type = \"TorusConvAE\"  # Another type of architecture \n",
    "\n",
    "# setting training mode\n",
    "diagnostic_mode = False  # Diagnostic mode flag\n",
    "compute_curvature = True  # Flag to indicate whether to compute curvature\n",
    "OOD_regime = False  # Out-of-distribution regime flag\n",
    "\n",
    "mse_w = 1.0  # Weight for mean squared error in the loss function\n",
    "unif_w = 0.5e-3  # Weight for week convergence to uniform distribution in the loss function\n",
    "num_moments = 4  # Number of empirical moments of the distribution penalized in the loss function\n",
    "\n",
    "lambda_contractive_decoder = 0.  # Weight for contractive decoder in the loss function\n",
    "delta_decoder = 2  # Theshold parameter for outlyers of the decoder Frobenius norm\n",
    "#djnpm = \"mean\"  # Decoder Jacobian norm penalization mode (not used yet). Could be \"mean\" or \"max\"\n",
    "\n",
    "lambda_contractive_encoder = 0.  # Weight for contractive encoder\n",
    "delta_encoder = 0.  # Theshold parameter for the encoder Frobenius norm\n",
    "#ejnpm = \"max\"  # Encoder Jacobian norm penalization mode (not used yet). Could be \"mean\" or \"max\"\n",
    "\n",
    "eps = 0.0  # Regularization parameter for inverse of metric computation (involved in Scalar curvature computation)\n",
    "\n",
    "curvature_penalization_mode = \"mean\"  # Mode for curvature penalization\n",
    "if compute_curvature == True:  # Check if curvature computation is enabled\n",
    "    lambda_curv = 1e-1  # Weight for curvature regularization\n",
    "else:\n",
    "    lambda_curv = 0.  # No curvature regularization\n",
    "\n",
    "delta_curv = 0.1  # Delta parameter for curvature penalization ( meaningful only if \"curvature_penalization_mode == max\")\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr = 1e-3  # Learning rate for the optimizer\n",
    "num_epochs = 1# 20  # Number of epochs for training\n",
    "#num_batches = 15000  # Number of batches (can be uncommented if one needs to stop training for this num of batches)\n",
    "\n",
    "#curv_w_increase_rate = 0. # can be uncommented if one needs to increase curvature penalization weight during training\n",
    "\n",
    "# Hyperparameters for data loaders\n",
    "batch_size = 128  # Batch size \n",
    "split_ratio = 0.2  # Ratio for splitting the data into training and validation\n",
    "weight_decay = 0.  # Weight decay for the optimizer\n",
    "random_shuffling = False  # Flag to indicate whether to shuffle data randomly\n",
    "random_seed = 0  # Seed for random number generation\n",
    "Force_CPU = False  # Flag to force the use of CPU for computation\n",
    "\n",
    "# Set manual seed for reproducibility\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD sampling parameters\n",
    "# This parameters mean nothing if OOD_regime == False\n",
    "\n",
    "T_ood = 20 # 100 # period of OOD penalization\n",
    "n_ood = 5 # number of OOD samples per point\n",
    "sigma_ood = 2e-1 # sigma of OOD Gaussian samples: 2e-1 swissroll\n",
    "N_extr = 16 # 32 batch size of extremal curvature points\n",
    "r_ood = 1e-3 # 1e-2 decay factor\n",
    "OOD_w = lambda_curv # weight on curvature in OOD sampling\n",
    "start_ood = 0 # OOD starting batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('../') # have to go 1 level up\n",
    "import ricci_regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers in DataLoader\n",
    "num_workers = 11\n",
    "\n",
    "if set_name == \"MNIST\":\n",
    "    D = 784\n",
    "    k = 10 # number of classes\n",
    "    #MNIST_SIZE = 28\n",
    "    # MNIST Dataset\n",
    "    train_dataset = datasets.MNIST(root='../datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset  = datasets.MNIST(root='../datasets/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "    set_parameters = {\"k\" : k}\n",
    "elif set_name == \"MNIST01\":\n",
    "    D = 784\n",
    "    k = len(selected_labels) # number of classes\n",
    "    full_mnist_dataset = datasets.MNIST(root='../datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset  = datasets.MNIST(root='../datasets/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "    mask = (full_mnist_dataset.targets == -1) \n",
    "    for label in selected_labels:\n",
    "        mask = mask | (full_mnist_dataset.targets == label)\n",
    "    indices01 = torch.where(mask)[0]\n",
    "    #indices01 = torch.where((full_mnist_dataset.targets == 0) | (full_mnist_dataset.targets == 1))[0]\n",
    "    from torch.utils.data import Subset\n",
    "    train_dataset = Subset(full_mnist_dataset, indices01) # MNIST only with 0,1 indices\n",
    "\n",
    "    set_parameters = {\"k\" : k}\n",
    "        \n",
    "elif set_name == \"Synthetic\":\n",
    "    D = 784       #dimension\n",
    "    k = 3         # num of 2d planes in dim D\n",
    "    n = 6*(10**3) # num of points in each plane\n",
    "    shift_class = 0.0\n",
    "    var_class = 1.0\n",
    "    intercl_var = 0.1 # this has to be greater than 0.04\n",
    "    # this creates a gaussian, \n",
    "    # i.e.random shift \n",
    "    # proportional to the value of intercl_var\n",
    "    # Generate dataset\n",
    "    # via classes\n",
    "    torch.manual_seed(0) # reproducibility\n",
    "    my_dataset = ricci_regularization.SyntheticDataset(k=k,n=n,d=d,D=D,\n",
    "                                        shift_class=shift_class, intercl_var=intercl_var, var_class=var_class)\n",
    "\n",
    "    train_dataset = my_dataset.create\n",
    "    set_parameters = {\n",
    "    \"k\" : k,\n",
    "    \"n\" : n,\n",
    "    \"shift_class\" : shift_class,\n",
    "    \"var_class\" : var_class,\n",
    "    \"intercl_var\" : intercl_var\n",
    "    }\n",
    "elif set_name == \"Swissroll\":\n",
    "    D = 3\n",
    "    sr_noise = 1e-6\n",
    "    sr_numpoints = 18000 #k*n\n",
    "    train_dataset =  sklearn.datasets.make_swiss_roll(n_samples=sr_numpoints, noise=sr_noise,random_state=random_seed)\n",
    "    sr_points = torch.from_numpy(train_dataset[0]).to(torch.float32)\n",
    "    #sr_points = torch.cat((sr_points,torch.zeros(sr_numpoints,D-3)),dim=1)\n",
    "    sr_colors = torch.from_numpy(train_dataset[1]).to(torch.float32)\n",
    "    from torch.utils.data import TensorDataset\n",
    "    train_dataset = TensorDataset(sr_points,sr_colors)\n",
    "    set_parameters = {\n",
    "    \"sr_noise\" : sr_noise,\n",
    "    \"sr_numpoints\" : sr_numpoints\n",
    "    }\n",
    "\n",
    "m = len(train_dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(train_dataset, [m-int(m*split_ratio), int(m*split_ratio)])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=random_shuffling) # was true\n",
    "test_loader  = torch.utils.data.DataLoader(test_data , batch_size=batch_size)\n",
    "\n",
    "batches_per_epoch = len(train_loader)\n",
    "#start_ood = batches_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "if architecture_type == \"TorusConvAE\":\n",
    "    torus_ae = ricci_regularization.Architectures.TorusConvAE(x_dim=D, h_dim1= 512, h_dim2=256, z_dim=d,pixels=28)\n",
    "else:\n",
    "    torus_ae = ricci_regularization.Architectures.TorusAE(x_dim=D, h_dim1= 512, h_dim2=256, z_dim=d)\n",
    "    \n",
    "if Force_CPU == True:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Forced to use CPU.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available! Training will use GPU.\")\n",
    "elif torch.cuda.is_available() == False:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is NOT available! Using CPU.\")\n",
    "\n",
    "# Move the AE to the selected device\n",
    "torus_ae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weights_loaded == True:\n",
    "    PATH_weights_loaded = f'../nn_weights/{set_name}_exp{experiment_number-1}.pt'\n",
    "    torus_ae.load_state_dict(torch.load(PATH_weights_loaded))\n",
    "    torus_ae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(torus_ae.parameters(),lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curv_func(encoded_data, function=torus_ae.decoder_torus):\n",
    "    metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=function)\n",
    "    det_on_data = torch.det(metric_on_data)\n",
    "    Sc_on_data = ricci_regularization.Sc_jacfwd_vmap(encoded_data,\n",
    "                                           function=function)\n",
    "    N = metric_on_data.shape[0]\n",
    "    Integral_of_Sc = (1/N)*(torch.sqrt(det_on_data)*torch.square(Sc_on_data)).sum()\n",
    "    #Integral_of_Sc = (1/N)*(torch.sqrt(det_on_data)*(Sc_on_data**4)).sum()\n",
    "    return Integral_of_Sc\n",
    "\"\"\"\n",
    "# minimizing |g-I|_F\n",
    "def curv_func(encoded_data, function=torus_ae.decoder_torus):\n",
    "    metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=function)\n",
    "    N = metric_on_data.shape[0]\n",
    "    func = (1/N)*(metric_on_data-torch.eye(d)).norm(dim=(1,2)).sum()\n",
    "    return func\n",
    "\"\"\"\n",
    "    \n",
    "# Loss = MSE + uniform_loss + curv_loss\n",
    "#  where the uniform_loss uses modulis of Fourier modes, of the empirical distribution.\n",
    "#  This requires batch size to be in the range of CLT.\n",
    "#\n",
    "# Inputs:\n",
    "#   recon_data: reconstructed data via decoder\n",
    "#   data: original data\n",
    "#   z: latent variable\n",
    "def loss_function(recon_data, data, z, decoder, compute_curvature = False, diagnostic_mode = False):\n",
    "    # explain \n",
    "    MSE = F.mse_loss(recon_data, data.view(-1, D), reduction='mean')\n",
    "    # Splits sines and cosines\n",
    "    z_sin = z[:, 0:d]\n",
    "    z_cos = z[:, d:2*d]\n",
    "    #\n",
    "    # Compute empirical first mode\n",
    "    mode1 = torch.mean( z, dim = 0)\n",
    "    mode1 = torch.sum( mode1*mode1 )\n",
    "    #\n",
    "    # Compute empirical second mode\n",
    "    mode2_1 = torch.mean( 2*z_cos*z_cos-1, dim = 0)\n",
    "    mode2_1 = torch.sum( mode2_1*mode2_1)\n",
    "    mode2_2 = torch.mean( 2*z_sin*z_cos, dim = 0)\n",
    "    mode2_2 = torch.sum( mode2_2*mode2_2 )\n",
    "    mode2 = mode2_1 + mode2_2\n",
    "    #\n",
    "    unif_loss = mode1 + mode2\n",
    "    # Compute empirical third mode\n",
    "    if num_moments > 2:\n",
    "        mode3_1 = torch.mean( 4*z_cos**3-3*z_cos, dim = 0)\n",
    "        mode3_1 = torch.sum( mode3_1*mode3_1)\n",
    "        mode3_2 = torch.mean( z_sin*(8*z_cos**3-4*z_cos), dim = 0)\n",
    "        mode3_2 = torch.sum( mode3_2*mode3_2 )\n",
    "        mode3 = mode3_1 + mode3_2\n",
    "        unif_loss += mode3\n",
    "    # Compute empirical fourth mode\n",
    "    if num_moments > 3:\n",
    "        mode4_1 = torch.mean( 8*z_cos**4-8*z_cos**2+1, dim = 0)\n",
    "        mode4_1 = torch.sum( mode4_1*mode4_1)\n",
    "        mode4_2 = torch.mean( z_sin*(16*z_cos**4-12*z_cos**2+1), dim = 0)\n",
    "        mode4_2 = torch.sum( mode4_2*mode4_2 )\n",
    "        mode4 = mode4_1 + mode4_2\n",
    "        unif_loss += mode4\n",
    "    dict_losses = {\n",
    "        \"MSE\": MSE,\n",
    "        \"uniform_loss\": unif_loss,\n",
    "    }\n",
    "    \n",
    "    if compute_curvature == True:\n",
    "        encoded_points_no_grad = torus_ae.encoder2lifting(data.view(-1, D)).detach()\n",
    "        #curv_loss = curv_func(encoded_points_no_grad,function=torus_ae.decoder_torus)\n",
    "\n",
    "        metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_points_no_grad,\n",
    "                                           function=decoder)\n",
    "        det_on_data = torch.det(metric_on_data)\n",
    "        Sc_on_data = ricci_regularization.Sc_jacfwd_vmap(encoded_points_no_grad,\n",
    "                                           function=decoder,eps=eps)\n",
    "        \n",
    "        if curvature_penalization_mode == \"mean\":\n",
    "            curv_loss = (torch.sqrt(det_on_data)*torch.square(Sc_on_data)).mean()\n",
    "        elif curvature_penalization_mode == \"max\":\n",
    "            curv_outlyers = torch.nn.ReLU()(torch.sqrt(det_on_data)*torch.square(Sc_on_data) - delta_curv)\n",
    "            curv_loss = (curv_outlyers).max()\n",
    "        \n",
    "        dict_losses[\"curvature_loss\"] = curv_loss\n",
    "        if diagnostic_mode == True:\n",
    "            curv_squared_mean = (torch.square(Sc_on_data)).mean()\n",
    "            curv_squared_max = (torch.square(Sc_on_data)).max()\n",
    "            dict_losses[\"curv_squared_mean\"] = curv_squared_mean\n",
    "            dict_losses[\"curv_squared_max\"] = curv_squared_max\n",
    "    if diagnostic_mode == True:\n",
    "        if compute_curvature == False:\n",
    "            encoded_points_no_grad = torus_ae.encoder2lifting(data.view(-1, D)).detach()\n",
    "            metric_on_data = ricci_regularization.metric_jacfwd_vmap(encoded_points_no_grad,function=decoder)\n",
    "            det_on_data = torch.det(metric_on_data)    \n",
    "        #regularization term\n",
    "        #eps = 0.01\n",
    "        g_inv_train_batch = torch.linalg.inv(metric_on_data + eps*torch.eye(d).to(device))\n",
    "        g_inv_norm_train_batch = torch.linalg.matrix_norm(g_inv_train_batch)\n",
    "        g_inv_norm_mean = torch.mean(g_inv_norm_train_batch)\n",
    "        g_inv_norm_max = torch.max(g_inv_norm_train_batch)\n",
    "        g_det_mean = det_on_data.mean()\n",
    "        g_det_max = det_on_data.max()\n",
    "        g_det_min = det_on_data.min()\n",
    "\n",
    "        decoder_jac_norm = torch.func.vmap(torch.trace)(metric_on_data)\n",
    "        decoder_jac_norm_mean = decoder_jac_norm.mean()\n",
    "        decoder_jac_norm_max = decoder_jac_norm.max()\n",
    "        dict_losses[\"g_inv_norm_mean\"] = g_inv_norm_mean\n",
    "        dict_losses[\"g_inv_norm_max\"] = g_inv_norm_max\n",
    "        dict_losses[\"g_det_mean\"] = g_det_mean\n",
    "        dict_losses[\"g_det_max\"] = g_det_max\n",
    "        dict_losses[\"g_det_min\"] = g_det_min\n",
    "        \n",
    "        # decoder jac\n",
    "        dict_losses[\"decoder_jac_norm_mean\"] = decoder_jac_norm_mean\n",
    "        dict_losses[\"decoder_jac_norm_max\"] = decoder_jac_norm_max\n",
    "\n",
    "        outlyers_decoder_norm = torch.nn.ReLU()(decoder_jac_norm - delta_decoder)\n",
    "        dict_losses[\"decoder_contractive_loss\"] = (outlyers_decoder_norm).max()\n",
    "\n",
    "        # encoder jac. THIS is VERY suboptimal for large D!!! Redo with  \n",
    "        metric_array_encoder = ricci_regularization.metric_jacrev_vmap(data.view(-1, D),\n",
    "                                                                       function=torus_ae.encoder_torus,\n",
    "                                                                       latent_space_dim=D)\n",
    "        encoder_jac_norm = torch.func.vmap(torch.trace)(metric_array_encoder)\n",
    "        encoder_jac_norm_mean = encoder_jac_norm.mean()\n",
    "        encoder_jac_norm_max = encoder_jac_norm.max()\n",
    "\n",
    "        dict_losses[\"encoder_jac_norm_mean\"] = encoder_jac_norm_mean\n",
    "        dict_losses[\"encoder_jac_norm_max\"] = encoder_jac_norm_max\n",
    "        outlyers_encoder_norm = torch.nn.ReLU()(encoder_jac_norm - delta_encoder)\n",
    "        #num_outlyers = torch.nonzero(outlyers_encoder_norm).flatten().shape[0]\n",
    "        #dict_losses[\"encoder_contractive_loss\"] = (1/num_outlyers)*(outlyers_encoder_norm).sum()\n",
    "        dict_losses[\"encoder_contractive_loss\"] = (outlyers_encoder_norm).max()\n",
    "\n",
    "    return dict_losses\n",
    "\"\"\"\n",
    "dict_losses = {\n",
    "        \"MSE\": MSE,\n",
    "        \"uniform_loss\": unif_loss,\n",
    "        \"curvature_loss\":curv_loss,\n",
    "        \"curv_squared_mean\":curv_squared_mean,\n",
    "        \"curv_squared_max\":curv_squared_max,\n",
    "        \"g_inv_norm_mean\":g_inv_norm_mean,\n",
    "        \"g_inv_norm_max\":g_inv_norm_max,\n",
    "        \"g_det_mean\":g_det_mean,\n",
    "        \"g_det_max\":g_det_max,\n",
    "        \"decoder_jac_norm_mean\": decoder_jac_norm_mean,\n",
    "        \"decoder_jac_norm_max\": decoder_jac_norm_max\n",
    "    }\n",
    "\"\"\"  \n",
    "\n",
    "# OOD initialization\n",
    "extreme_curv_value_tensor = None\n",
    "extreme_curv_points_tensor = None\n",
    "if OOD_regime == True:\n",
    "    first_batch,_ = next(iter(train_loader))\n",
    "    first_batch = first_batch.to(device)\n",
    "    extreme_curv_points_tensor = torus_ae.encoder2lifting(first_batch.view(-1,D)[:N_extr]).detach()\n",
    "    extreme_curv_points_tensor.to(device)         \n",
    "\n",
    "    extreme_curv_value_tensor = ricci_regularization.Sc_jacfwd_vmap(extreme_curv_points_tensor, function=torus_ae.decoder_torus,eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1,batch_idx = 0,dict_loss_arrays={},diagnostic_mode = False, compute_curvature = False,\n",
    "          extreme_curv_points_tensor = extreme_curv_points_tensor, \n",
    "          extreme_curv_value_tensor = extreme_curv_value_tensor):\n",
    "    if batch_idx == 0:\n",
    "        dict_loss_arrays = {}\n",
    "    torus_ae.train()\n",
    "    print(\"Epoch %d\"%epoch)\n",
    "    t = tqdm( train_loader, desc=\"Train\", position=0 )\n",
    "    \n",
    "    for (data, labels) in t:\n",
    "        #data = data.cuda()\n",
    "        #data = data.cpu()\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward\n",
    "        \"\"\"\n",
    "        if architecture_type == \"TorusConvAE\":\n",
    "            recon_batch, z = decoder(encoder(data)) , encoder(data)\n",
    "        else:\n",
    "            recon_batch, z = torus_ae(data)\n",
    "        \"\"\"\n",
    "        recon_batch, z = torus_ae(data)\n",
    "        #mse_loss, uniform_loss, curvature_loss, g_inv_loss, curvature_squared = loss_function(recon_batch, data, z,decoder=torus_ae.decoder_torus)\n",
    "        \n",
    "        dict_losses = loss_function(recon_batch, data, z,\n",
    "                                    decoder=torus_ae.decoder_torus,\n",
    "                                    diagnostic_mode=diagnostic_mode, \n",
    "                                    compute_curvature=compute_curvature)\n",
    "        mse_loss = dict_losses[\"MSE\"]\n",
    "        uniform_loss = dict_losses[\"uniform_loss\"]\n",
    "        loss = mse_w*mse_loss + unif_w*uniform_loss \n",
    "\n",
    "        #relu_function = torch.nn.ReLU()\n",
    "        #decoder_penalty = relu_function( dict_losses[f\"decoder_jac_norm_{djnpm}\"]- delta_decoder)\n",
    "        #encoder_penalty = relu_function( dict_losses[f\"encoder_jac_norm_{ejnpm}\"]- delta_encoder)\n",
    "        if diagnostic_mode == True:\n",
    "            encoder_contractive_loss = dict_losses[\"encoder_contractive_loss\"]\n",
    "            decoder_contractive_loss = dict_losses[\"decoder_contractive_loss\"]\n",
    "            loss = loss + + lambda_contractive_decoder*decoder_contractive_loss + lambda_contractive_encoder*encoder_contractive_loss#lambda_contractive_encoder*encoder_penalty\n",
    "\n",
    "\n",
    "        \n",
    "        if (compute_curvature == True): \n",
    "            curvature_loss = dict_losses[\"curvature_loss\"]\n",
    "            loss = loss + lambda_curv*curvature_loss\n",
    "        else:\n",
    "            curvature_loss_mean_per_epoch = \"nan\"\n",
    "        \n",
    "        # OOD regime (optional)\n",
    "        if OOD_regime == True:\n",
    "            extreme_curv_points_tensor, extreme_curv_value_tensor = ricci_regularization.find_extreme_curvature_points(data_batch=data,\n",
    "                                extreme_curv_points_tensor=extreme_curv_points_tensor,\n",
    "                                extreme_curv_value_tensor=extreme_curv_value_tensor,batch_idx=batch_idx,\n",
    "                                encoder=torus_ae.encoder2lifting,decoder=torus_ae.decoder_torus,r_ood=r_ood,\n",
    "                                N_extr=N_extr,output_dim=D)\n",
    "            \n",
    "            if (batch_idx % T_ood == 0) & (batch_idx >= start_ood):\n",
    "                OOD_curvature_loss = ricci_regularization.OODTools.curv_loss_on_OOD_samples(extreme_curv_points_tensor=extreme_curv_points_tensor,\n",
    "                                                                                            decoder=torus_ae.decoder_torus,\n",
    "                                                                                            sigma_ood=sigma_ood,n_ood=n_ood,N_extr=N_extr,\n",
    "                                                                                            latent_space_dim=d)\n",
    "                if diagnostic_mode == True:\n",
    "                    print(\"Curvature functional at OOD points\", OOD_curvature_loss)\n",
    "                loss = OOD_w * OOD_curvature_loss\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #appending losses per batch to loss arrays\n",
    "        if batch_idx == 0:\n",
    "            #dict_loss_arrays[\"decoder_penalty\"] = []\n",
    "            #dict_loss_arrays[\"encoder_penalty\"] = []\n",
    "            for key in dict_losses.keys():\n",
    "                dict_loss_arrays[key] = []\n",
    "        #dict_loss_arrays[\"decoder_penalty\"].append(decoder_penalty.item())\n",
    "        #dict_loss_arrays[\"encoder_penalty\"].append(decoder_penalty.item())\n",
    "        for key in dict_losses.keys():\n",
    "            dict_loss_arrays[key].append(dict_losses[key].item())\n",
    "        \n",
    "        # Progress bar\n",
    "        batch_idx += 1\n",
    "        MSE_mean_per_epoch = np.array(dict_loss_arrays[\"MSE\"])[-batches_per_epoch:].mean()\n",
    "        uniform_loss_mean_per_epoch = np.array(dict_loss_arrays[\"uniform_loss\"])[-batches_per_epoch:].mean()    \n",
    "        if compute_curvature == True:\n",
    "            curvature_loss_mean_per_epoch = np.array(dict_loss_arrays[\"curvature_loss\"])[-batches_per_epoch:].mean()\n",
    "        else:\n",
    "            curvature_loss_mean_per_epoch = \"nan\"\n",
    "        #decoder_penalty_mean_per_epoch = np.array(dict_loss_arrays[\"decoder_contractive_loss\"])[-batches_per_epoch:].mean()\n",
    "        \n",
    "        # Losses to be printed online in diagnostic_mode\n",
    "        if diagnostic_mode == True:\n",
    "            encoder_contractive_loss_mean_per_epoch = np.array(dict_loss_arrays[\"encoder_contractive_loss\"])[-batches_per_epoch:].mean()\n",
    "            t.set_description_str(desc=f\"MSE:{MSE_mean_per_epoch}, Uniform:{uniform_loss_mean_per_epoch}, Encoder_penalty:{encoder_contractive_loss_mean_per_epoch}, Curvature:{curvature_loss_mean_per_epoch}.\\n\")\n",
    "        \n",
    "        # Losses to be printed online in compute_curvature mode\n",
    "        if compute_curvature == True:\n",
    "            t.set_description_str(desc=f\"MSE:{MSE_mean_per_epoch}, Uniform:{uniform_loss_mean_per_epoch}, Curvature:{curvature_loss_mean_per_epoch}.\\n\")\n",
    "        else:\n",
    "            t.set_description_str(desc=f\"MSE:{MSE_mean_per_epoch}, Uniform:{uniform_loss_mean_per_epoch}\")\n",
    "        #if batch_idx > num_batches:\n",
    "        #    break\n",
    "    #end for\n",
    "    return batch_idx, dict_loss_arrays\n",
    "\n",
    "def test(mse_loss_array=[], uniform_loss_array=[], curvature_loss_array = [],g_inv_loss_array=[],curvature_squared_array=[]):\n",
    "    torus_ae.eval()\n",
    "    with torch.no_grad():\n",
    "        t = tqdm( test_loader, desc=\"Test\", position=1 )\n",
    "        for data, _ in t:\n",
    "            data = data.cpu()\n",
    "            recon_batch, z = torus_ae(data)\n",
    "            dict_losses = loss_function(recon_batch, data, z,decoder=torus_ae.decoder_torus)\n",
    "            if compute_curvature == True:\n",
    "                curvature_loss = dict_losses[\"curvature_loss\"]\n",
    "            else:\n",
    "                curvature_loss = torch.zeros(1)\n",
    "        \n",
    "            mse_loss_array.append( dict_losses[\"MSE\"].item() ) \n",
    "            uniform_loss_array.append( dict_losses[\"uniform_loss\"].item() )\n",
    "            curvature_loss_array.append(curvature_loss.item())\n",
    "    print(f\"Test losses. \\nMSE:{np.array(mse_loss_array).mean()}, Uniform:{np.array(uniform_loss_array).mean()}, Curvature:{np.array(curvature_loss_array).mean()}.\\n\")\n",
    "    return mse_loss_array, uniform_loss_array, curvature_loss_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx=0\n",
    "dict_loss_arrays = {}\n",
    "#diagnostic_mode = False\n",
    "\n",
    "# Launch\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  torus_ae.to(device)\n",
    "  batch_idx, dict_loss_arrays = train(epoch=epoch,batch_idx=batch_idx,dict_loss_arrays=dict_loss_arrays,compute_curvature=compute_curvature,\n",
    "                                                 diagnostic_mode=diagnostic_mode)\n",
    "  if diagnostic_mode == True :\n",
    "    dict2print = ricci_regularization.PlottingTools.translate_dict(dict2print=dict_loss_arrays, include_curvature_plots=compute_curvature,eps=eps)\n",
    "    ricci_regularization.PlottingTools.plotsmart(dict2print)\n",
    "  #else:\n",
    "  #  ricci_regularization.PlottingTools.plotfromdict(dict_of_losses=dict_loss_arrays)\n",
    "      \n",
    "  # update curvature weight\n",
    "  #lambda_curv = lambda_curv * 10**(curv_w_increase_rate)\n",
    "    \n",
    "  if (set_name in [\"MNIST\",\"MNIST01\"]): #& (architecture_type == \"TorusAE\"):\n",
    "    ricci_regularization.PlottingTools.plot_ae_outputs_selected(test_dataset=test_dataset,\n",
    "                                                       encoder=torus_ae.cpu().encoder2lifting,\n",
    "                                                       decoder=torus_ae.cpu().decoder_torus,selected_labels=selected_labels)\n",
    "  #test() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test losses, $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_R_squared_losses(data_loader=test_loader):\n",
    "    len_test_loader = len(test_loader)\n",
    "    mse_loss = 0\n",
    "    curv_loss = 0\n",
    "    unif_loss = 0\n",
    "    input_dataset_list = []\n",
    "    torus_ae.to(device)\n",
    "    for (data, labels) in test_loader:\n",
    "        data = data.to(device)\n",
    "        input_dataset_list.append(data.cpu())\n",
    "        input = data\n",
    "        recon = torus_ae(data)[0]\n",
    "        z = torus_ae(data)[1]\n",
    "        enc = torus_ae.encoder2lifting(data.view(-1,D))\n",
    "        dict_losses = loss_function(recon, input, z,\n",
    "                                    decoder=torus_ae.decoder_torus, \n",
    "                                    compute_curvature=True,diagnostic_mode=False)\n",
    "        mse_loss += dict_losses[\"MSE\"].cpu().detach()/len_test_loader\n",
    "        unif_loss += dict_losses[\"uniform_loss\"].cpu().detach()/len_test_loader\n",
    "        curv_loss += dict_losses[\"curvature_loss\"].cpu().detach()/len_test_loader\n",
    "    input_dataset_tensor = torch.cat(input_dataset_list).view(-1,D)\n",
    "    var = torch.var(input_dataset_tensor.flatten())\n",
    "    R_squared = 1 - mse_loss/var\n",
    "    return mse_loss, unif_loss, curv_loss, R_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_unif, test_curv, test_R_squared = compute_R_squared_losses(test_loader)\n",
    "#train_mse, train_unif, train_curv, train_R_squared, train_decoder_penalty = compute_R_squared_losses(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Train losses:\\nmse:{train_mse}, unif_loss:{train_unif}, decoder_penalty:{train_decoder_penalty}, curv_loss:{train_curv}\")\n",
    "#print(f\"R_squared: {train_R_squared.item():.4f}\")\n",
    "#print(f\"Test losses:\\nmse:{test_mse}\")\n",
    "print(f\"Test losses:\\nmse:{test_mse}, unif_loss:{test_unif}, curv_loss:{test_curv}\")\n",
    "print(f\"R_squared: {test_R_squared.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "from torcheval.metrics import R2Score\n",
    "R_squared = R2Score()#(multioutput=\"raw_values\")\n",
    "input = input_dataset_tensor.flatten()\n",
    "target = recon_dataset_tensor.flatten()\n",
    "R_squared.update(input, target)\n",
    "R_squared.compute()#.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weights_saved == True:\n",
    "    PATH_ae = f'../nn_weights/{set_name}_exp{experiment_number}.pt'\n",
    "    torch.save(torus_ae.state_dict(), PATH_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss ploting\n",
    "if diagnostic_mode == True:\n",
    "    #fig,axes = ricci_regularization.PlottingTools.plotsmart(dict2print)\n",
    "    fig,axes = ricci_regularization.PlottingTools.PlotSmartConvolve(dict2print)\n",
    "else:\n",
    "    fig,axes = ricci_regularization.PlottingTools.plotfromdict(dict_loss_arrays)\n",
    "if violent_saving == True:\n",
    "    fig.savefig(f\"{Path_pictures}/losses_exp{experiment_number}.pdf\",bbox_inches='tight',format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig,axes = ricci_regularization.PlottingTools.plot9losses(mse_loss_array,curvature_loss_array,g_inv_meanperbatch_array)\n",
    "if violent_saving == True:\n",
    "    fig.savefig(f\"{Path_pictures}/9losses_exp{experiment_number}.pdf\",bbox_inches='tight',format=\"pdf\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torus latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspiration for torus_ae.encoder2lifting\n",
    "\"\"\"\n",
    "def circle2anglevectorized(zLatentTensor,d = d):\n",
    "    cosphi = zLatentTensor[:, 0:d]\n",
    "    sinphi = zLatentTensor[:, d:2*d]\n",
    "    phi = torch.acos(cosphi)*torch.sgn(torch.asin(sinphi))\n",
    "    return phi\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zlist = []\n",
    "torus_ae.cpu()\n",
    "colorlist = []\n",
    "enc_list = []\n",
    "input_dataset_list = []\n",
    "recon_dataset_list = []\n",
    "for (data, labels) in tqdm( train_loader, position=0 ):\n",
    "#for (data, labels) in train_loader:\n",
    "    input_dataset_list.append(data)\n",
    "    recon_dataset_list.append(torus_ae(data)[0])\n",
    "    #zlist.append(torus_ae(data)[1])\n",
    "    enc_list.append(torus_ae.encoder2lifting(data.view(-1,D)))\n",
    "    colorlist.append(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = torch.cat(input_dataset_list)\n",
    "recon_dataset = torch.cat(recon_dataset_list)\n",
    "encoded_points = torch.cat(enc_list)\n",
    "encoded_points_no_grad = encoded_points.detach()/math.pi\n",
    "color_array = torch.cat(colorlist).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "if set_name == \"Swissroll\":\n",
    "    my_cmap = \"jet\"\n",
    "else:\n",
    "    my_cmap = ricci_regularization.PlottingTools.discrete_cmap(k, 'jet')\n",
    "plt.scatter(encoded_points_no_grad[:,0],encoded_points_no_grad[:,1], c=color_array-1, marker='o', edgecolor='none', cmap=my_cmap)\n",
    "\n",
    "if set_name in [\"Synthetic\",\"MNIST\",\"MNIST01\"]:\n",
    "    plt.colorbar(ticks=range(k),orientation=\"vertical\")\n",
    "plt.grid(True)\n",
    "if violent_saving == True:\n",
    "    plt.savefig(f\"{Path_pictures}/latent_space_exp{experiment_number}.pdf\",bbox_inches='tight',format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_config = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"experiment_number\": experiment_number,\n",
    "    \"dataset\":\n",
    "    {\n",
    "        \"name\": set_name,\n",
    "        \"parameters\": set_parameters,\n",
    "        \"selected_labels\": selected_labels\n",
    "    },\n",
    "    \"architecture\" :\n",
    "    {\n",
    "        \"name\":architecture_type,\n",
    "        \"input_dim\": D,\n",
    "        \"latent_dim\": d\n",
    "    },\n",
    "    \"optimization_parameters\": \n",
    "    {\n",
    "\t    \"learning_rate\": lr,\n",
    "\t    \"batch_size\": batch_size,\n",
    "        \"split_ratio\": split_ratio,\n",
    "\t    \"num_epochs\": num_epochs,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"random_shuffling\":random_shuffling,\n",
    "        \"random_seed\": random_seed,\n",
    "        \"device\": device.type\n",
    "    },\n",
    "    \"losses\":\n",
    "    {\n",
    "\t    \"mse_w\": mse_w,\n",
    "\t    \"unif_w\": unif_w,\n",
    "        \"Number of moments used\": num_moments,\n",
    "\t    \"lambda_curv\": lambda_curv,\n",
    "        \"delta_curv\": delta_curv,\n",
    "        \"curvature_penalization_mode\": curvature_penalization_mode,\n",
    "        \"g_inv regularization eps\": eps,\n",
    "        \"lambda_contractive_encoder\": lambda_contractive_encoder,\n",
    "        \"delta_encoder\" : delta_encoder,\n",
    "        \"lambda_contractive_decoder\" : lambda_contractive_decoder,\n",
    "        \"delta_decoder\" : delta_decoder,\n",
    "#        \"decoder_jac_norm_penalization_mode \" : djnpm,\n",
    "#        \"encoder_jac_norm_penalization_mode \" : ejnpm,\n",
    "        \"diagnostic_mode\": diagnostic_mode,\n",
    "        \"compute_curvature\": compute_curvature\n",
    "    },\n",
    "    \"OOD_parameters\": \n",
    "    {\n",
    "        \"OOD_regime\": OOD_regime,\n",
    "        \"start_ood\":start_ood,\n",
    "        \"T_ood\":T_ood,\n",
    "        \"n_ood\":n_ood,\n",
    "        \"sigma_ood\":sigma_ood,\n",
    "        \"N_extr\":N_extr,\n",
    "        \"r_ood\": r_ood,\n",
    "        \"OOD_w\":OOD_w\n",
    "    },\n",
    "    \"training_results_on_test_data\":\n",
    "    {\n",
    "        \"R^2\": test_R_squared.item(),\n",
    "        \"mse_loss\": test_mse.item(),\n",
    "        \"unif_loss\": test_unif.item(),\n",
    "        \"curv_loss\": test_curv.item()\n",
    "    },\n",
    "    \"Path_pictures\": Path_pictures,\n",
    "    \"Path_weights\": Path_weights,\n",
    "    \"Path_experiments\": Path_experiments,\n",
    "    \"weights_saved_at\": PATH_ae\n",
    "}\n",
    "if weights_loaded == True:\n",
    "    json_config[\"weights_loaded_from\"] = PATH_weights_loaded\n",
    "# Save dictionary to JSON file\n",
    "with open(f'{Path_experiments}/{experiment_name}exp{experiment_number}.json', 'w') as json_file:\n",
    "    json.dump(json_config, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
