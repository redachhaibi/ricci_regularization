{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal imports\n",
    "import torch, yaml, os, json\n",
    "import ricci_regularization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from ricci_regularization import RiemannianKmeansTools\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_setup_number = 0\n",
    "# experiment setup\n",
    "K = 3 # number of clusters\n",
    "N = 10 # number of points to be clustered\n",
    "\n",
    "# Select labels to make subset of mnist data points to be clustered\n",
    "selected_labels = [1,5,8]\n",
    "\n",
    "mode = \"Schauder\" \n",
    "#mode = \"Interpolation_points\" # alternative option\n",
    "\n",
    "# specific parameters \n",
    "n_max = 3  # Schauder basis complexity (only for Schauder)\n",
    "step_count = 50  # Number of interpolation steps (for both methods)\n",
    "\n",
    "# optimization parameters\n",
    "beta = 1.e-2 # Frechet mean learning rate #beta is learning_rate_frechet_mean (outer loop)\n",
    "learning_rate = 0.5e-3 # learning_rate_geodesics (inner loop)\n",
    "num_iter_outer = 50 # number of Frechet mean updates (outer loop)\n",
    "num_iter_inner = 10 # number of geodesics refinement interations per 1 Frechet mean update (inner loop)\n",
    "\n",
    "# decision boundary parameters\n",
    "neighbours_number = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the pretrained AE + creatong directory for results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_experiment = '../experiments/MNIST_Setting_1_config.yaml'\n",
    "with open(Path_experiment, 'r') as yaml_file:\n",
    "    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "    \n",
    "experiment_name = yaml_config[\"experiment\"][\"name\"]\n",
    "Path_pictures = \"../experiments/\" + yaml_config[\"experiment\"][\"name\"] + f\"/K_means_setup_{k_means_setup_number}\"\n",
    "# Check and create directories based on configuration\n",
    "if not os.path.exists(Path_pictures):  # Check if the picture path does not exist\n",
    "    os.mkdir(Path_pictures)  # Create the directory for plots if not yet created\n",
    "    print(f\"Created directory: {Path_pictures}\")  # Print directory creation feedback\n",
    "else:\n",
    "    print(f\"Directiry already exists: {Path_pictures}\")\n",
    "\n",
    "# Load data loaders based on YAML configuration\n",
    "dict = ricci_regularization.DataLoaders.get_dataloaders(\n",
    "    dataset_config=yaml_config[\"dataset\"],\n",
    "    data_loader_config=yaml_config[\"data_loader_settings\"],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "print(\"Experiment results loaded successfully.\")\n",
    "# Loading data\n",
    "train_loader = dict[\"train_loader\"]\n",
    "test_loader = dict[\"test_loader\"]\n",
    "test_dataset = dict.get(\"test_dataset\")  # Assuming 'test_dataset' is a key returned by get_dataloaders\n",
    "print(\"Data loaders created successfully.\")\n",
    "\n",
    "# Loading the pre-tained AE\n",
    "torus_ae, Path_ae_weights = ricci_regularization.DataLoaders.get_tuned_nn(config=yaml_config)\n",
    "print(\"AE weights loaded successfully.\")\n",
    "print(\"AE weights loaded from\", Path_ae_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking dataset to be clusterized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting the dataset that we want to cluster\n",
    "# we use some random N points of the test dataset that we will cluster\n",
    "# This could be done differently, e.g. by simply picking random points\n",
    "D = yaml_config[\"architecture\"][\"input_dim\"]\n",
    "d = yaml_config[\"architecture\"][\"latent_dim\"]\n",
    "\n",
    "# Extract data and ground truth labels of the subset\n",
    "all_data = test_dataset.data\n",
    "all_labels = test_dataset.targets\n",
    "mask = torch.isin(all_labels, torch.tensor(selected_labels)) # mask will be used to chose only labels in selected_labels\n",
    "# Filter dataset\n",
    "data_filtered = all_data[mask]\n",
    "labels_filtered = all_labels[mask]\n",
    "torch.manual_seed(0)\n",
    "indices = torch.randperm(len(data_filtered))[:N]  # Randomly shuffle and pick first N\n",
    "mnist_subset = data_filtered[indices]\n",
    "ground_truth_labels = labels_filtered[indices]\n",
    "\n",
    "# meaningless alternative \n",
    "#data = test_dataset.data\n",
    "#subset_indices = list(range(N))\n",
    "#mnist_subset = torch.utils.data.Subset(data, subset_indices)\n",
    "\n",
    "# constructing dataloader for the mnist_subset\n",
    "dataset_batch_size = 128\n",
    "dataloader = torch.utils.data.DataLoader(mnist_subset, batch_size=dataset_batch_size, shuffle=False)\n",
    "# encoding into latent space\n",
    "torus_ae.cpu()\n",
    "torus_ae.eval()\n",
    "\n",
    "# Encode samples into latent space\n",
    "encoded_points = []\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    for images in dataloader:\n",
    "#        print(images.shape)\n",
    "        latent = torus_ae.encoder2lifting( (images.reshape(-1, D)).to(torch.float32) )  # Pass images through the encoder\n",
    "        encoded_points.append(latent)\n",
    "encoded_points = torch.cat(encoded_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering poins to choose N of them with labels in selected_labels\n",
    "#clusters can be unbalanced\n",
    "list_encoded_data_filtered = []\n",
    "list_labels_filtered = []\n",
    "for data,label in train_loader:\n",
    "    mask_batch = torch.isin(label, torch.tensor(selected_labels)) # mask will be used to chose only labels in selected_labels\n",
    "    data_filtered = data[mask_batch]\n",
    "    labels_filtered = label[mask_batch]\n",
    "    enc_images = torus_ae.encoder2lifting(data_filtered.reshape(-1, D)).detach()\n",
    "    list_encoded_data_filtered.append(enc_images)\n",
    "    list_labels_filtered.append(labels_filtered)\n",
    "    #print(labels_filtered)\n",
    "all_encoded_data_filtered = torch.cat(list_encoded_data_filtered)\n",
    "all_labels_filtered = torch.cat(list_labels_filtered)\n",
    "#randomly picking N points with selected labels\n",
    "torch.manual_seed(0)\n",
    "indices = torch.randperm(len(all_encoded_data_filtered))[:N]  # Randomly shuffle and pick first N\n",
    "encoded_points = all_encoded_data_filtered[indices]\n",
    "ground_truth_labels = all_labels_filtered[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manifold plot\n",
    "RiemannianKmeansTools.manifold_plot_selected_labels(all_encoded_data_filtered,\n",
    "            all_labels_filtered,selected_labels,\n",
    "            saving_folder=Path_pictures, plot_title=\"Manifold plot for all points with selected labels\",\n",
    "            file_saving_name=\"Manifold_plot_selescted_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot for points to cluster\n",
    "RiemannianKmeansTools.manifold_plot_selected_labels(encoded_points,\n",
    "            ground_truth_labels,selected_labels,\n",
    "            saving_folder=Path_pictures, plot_title=\"Encoded Points Colored by Ground Truth Labels\",\n",
    "            file_saving_name=\"ground_truth_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting parameters to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_centroids = RiemannianKmeansTools.initialize_centers(encoded_points, K, N) \n",
    "current_centroids = torch.clone(initial_centroids) \n",
    "\n",
    "if mode == \"Interpolation_points\":\n",
    "    geodesic_solver = None\n",
    "    # Initialize geodesic segments\n",
    "    parameters_of_geodesics = RiemannianKmeansTools.construct_interpolation_points_on_segments_connecting_centers2encoded_data(\n",
    "            encoded_points, \n",
    "            initial_centroids, \n",
    "            num_aux_points = step_count)\n",
    "elif mode == \"Schauder\":\n",
    "    geodesic_solver = ricci_regularization.Schauder.NumericalGeodesics(n_max, step_count)\n",
    "    # Get Schauder basis\n",
    "    N_max = geodesic_solver.schauder_bases[\"zero_boundary\"][\"N_max\"]\n",
    "    basis = geodesic_solver.schauder_bases[\"zero_boundary\"][\"basis\"]\n",
    "    # Define parameters (batch_size × N_max × dim)\n",
    "    parameters_of_geodesics = torch.zeros((N, K, N_max, d), requires_grad=True)\n",
    "init_parameters = torch.clone(parameters_of_geodesics) # save initial segments\n",
    "# Set optimizer params\n",
    "parameters = torch.nn.Parameter(parameters_of_geodesics) # Wrap as a parameter\n",
    "\n",
    "optimizer = torch.optim.SGD([parameters], lr=learning_rate)\n",
    "\n",
    "cluster_index_of_each_point = None\n",
    "geodesics2nearestcentroids = None\n",
    "\n",
    "#losses\n",
    "intra_class_variance_history = []\n",
    "intra_class_variance_by_cluster_history = []\n",
    "geodesics2nearestcentroids_lengths_history = []\n",
    "geodesics2nearestcentroids_lengths_by_cluster_history = []\n",
    "\n",
    "norm_Frechet_mean_gradient_history = []\n",
    "\n",
    "\n",
    "# visualizing initialization (optional)\n",
    "plt.title(\"K-means initialization\")\n",
    "plt.scatter(encoded_points[:,0],encoded_points[:,1], label = \"encoded data\")\n",
    "plt.scatter(initial_centroids[:,0], initial_centroids[:,1], c=\"red\", label = \"initial_centroids\", marker='*', s = 60)\n",
    "plt.xlim(-torch.pi, torch.pi)\n",
    "plt.ylim(-torch.pi, torch.pi)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------------------\n",
    "# Riemannian K-means Algorithm\n",
    "# ----------------------------\n",
    "# Outer loop \n",
    "for iter_outer in range(num_iter_outer):\n",
    "    # Inner loop (refining geodesics)\n",
    "    for iter_inner in range(num_iter_inner):\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        # Compute the loss\n",
    "        energies_of_geodesics = RiemannianKmeansTools.compute_energy(\n",
    "                mode = mode, \n",
    "                parameters_of_geodesics=parameters, \n",
    "                end_points = [encoded_points, current_centroids],\n",
    "                decoder = torus_ae.decoder_torus,\n",
    "                geodesic_solver = geodesic_solver,\n",
    "                reduction=\"none\")\n",
    "        loss_geodesics = energies_of_geodesics.sum()\n",
    "        # Backpropagation: compute gradients\n",
    "        loss_geodesics.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Store the loss value\n",
    "    # end inner loop\n",
    "\n",
    "    # compute a vector of length of all geodesics shape (N,K)\n",
    "    lengths_of_geodesics = RiemannianKmeansTools.compute_lengths(\n",
    "            mode = mode,\n",
    "            parameters_of_geodesics=parameters,\n",
    "            end_points = [encoded_points, current_centroids],\n",
    "            decoder = torus_ae.decoder_torus,\n",
    "            geodesic_solver = geodesic_solver,\n",
    "            reduction=\"none\") \n",
    "    \n",
    "    if mode == \"Interpolation_points\":\n",
    "        geodesic_curve = RiemannianKmeansTools.geodesics_from_parameters_interpolation_points(\n",
    "                parameters_of_geodesics = parameters_of_geodesics, \n",
    "                end_points = [encoded_points, current_centroids])\n",
    "    elif mode == \"Schauder\":\n",
    "        geodesic_curve = RiemannianKmeansTools.geodesics_from_parameters_schauder(\n",
    "                geodesic_solver = geodesic_solver, \n",
    "                parameters_of_geodesics = parameters_of_geodesics, \n",
    "                end_points = [encoded_points, current_centroids])\n",
    "\n",
    "    # retrieve the class membership of each point by finding the closest cluster centroid \n",
    "    cluster_index_of_each_point = torch.argmin(lengths_of_geodesics, dim=1) # shape (N)\n",
    "    batch_indices = torch.arange(N) # this is needed, since   geodesic_curve[:, cluster_index_of_each_point, :, :] will produce a tensor of shape (N,N,step_count,d)\n",
    "    # pick only geodesics connecting points to cluster relevant centroids where the points are assigned\n",
    "    geodesics2nearestcentroids = geodesic_curve[batch_indices, cluster_index_of_each_point, :, :].detach() # shape (N,step_count,d)\n",
    "\n",
    "    # v is the direction to move the cluster centroids # shape (N,d)\n",
    "    v = geodesics2nearestcentroids[:,-1,:] - geodesics2nearestcentroids[:,-2,:]\n",
    "    v = v / v.norm(dim=1).unsqueeze(-1) # find the last segments of the geod shape (N,d)\n",
    "    \n",
    "    # Compute weighted Frechet mean gradient for each cluster\n",
    "    weighted_v = lengths_of_geodesics[:, 0].unsqueeze(-1) * v  # Shape: (N, d)\n",
    "    # Create a one-hot encoding of the cluster indices\n",
    "    one_hot_clusters = torch.nn.functional.one_hot(cluster_index_of_each_point, num_classes=K).float()  # Shape: (N, K)\n",
    "    # Compute the gradients for each cluster\n",
    "    Frechet_mean_gradient = one_hot_clusters.T @ weighted_v  # Shape: (K, d)\n",
    "    # Update cluster centroids\n",
    "    with torch.no_grad():\n",
    "        current_centroids += - beta * Frechet_mean_gradient  # Update all centroids simultaneously\n",
    "\n",
    "    # Compute average Frechet mean gradient norm among the K clusters on step iter_outer \n",
    "    average_Frechet_mean_gradient_norm = (Frechet_mean_gradient.norm(dim=1).mean()).item()\n",
    "    # Append to norm history\n",
    "    norm_Frechet_mean_gradient_history.append(average_Frechet_mean_gradient_norm)\n",
    "\n",
    "    # saving the lengths of geodesics2nearestcentroids\n",
    "    geodesics2nearestcentroids_lengths = lengths_of_geodesics[batch_indices, cluster_index_of_each_point]\n",
    "    geodesics2nearestcentroids_lengths_history.append( geodesics2nearestcentroids_lengths.detach().sum().item() )\n",
    "    # -------------------\n",
    "    # could be done also:\n",
    "    # Expand cluster_index_of_each_point to index into v and lengths_of_geodesics\n",
    "    # cluster_index_of_each_point_expanded = cluster_index_of_each_point.unsqueeze(-1).expand(-1, v.size(-1)) # old line\n",
    "    # cluster_index_of_each_point_expanded = cluster_index_of_each_point.unsqueeze(-1).expand(N,K) # shape (N,K)\n",
    "    # geodesics2nearestcentroids_lengths = torch.gather(lengths_of_geodesics,1,cluster_index_of_each_point_expanded)[:,0]\n",
    "    # the problem of torch.gather is that tensor and indices have to be same size. we need to expand index tensor\n",
    "    # ---------------------\n",
    "\n",
    "    # save intra-class variance\n",
    "    intra_class_variance = energies_of_geodesics[batch_indices, cluster_index_of_each_point]\n",
    "    intra_class_variance_history.append( intra_class_variance.detach().sum().item() )\n",
    "\n",
    "    #compute the sum of geodesic length for each cluster\n",
    "    #scatter_add_ is the reverse of torch.gather\n",
    "    length_of_geodesics2nearestcentroids_by_cluster = torch.zeros(K, dtype=geodesics2nearestcentroids_lengths.dtype)\n",
    "    length_of_geodesics2nearestcentroids_by_cluster.scatter_add_(0, cluster_index_of_each_point, geodesics2nearestcentroids_lengths)    \n",
    "    geodesics2nearestcentroids_lengths_by_cluster_history.append(length_of_geodesics2nearestcentroids_by_cluster.unsqueeze(0))\n",
    "\n",
    "    #compute the Intra-class variance, i.e. sum of geodesic energy for each cluster\n",
    "    #scatter_add_ is the reverse of torch.gather\n",
    "    intra_class_variance_by_cluster = torch.zeros(K, dtype=geodesics2nearestcentroids_lengths.dtype)\n",
    "    intra_class_variance_by_cluster.scatter_add_(0, cluster_index_of_each_point, intra_class_variance)    \n",
    "    intra_class_variance_by_cluster_history.append(intra_class_variance_by_cluster.unsqueeze(0))\n",
    "\n",
    "#timing\n",
    "end_time = time.time()\n",
    "algorithm_execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting losses\n",
    "# In this cell: \n",
    "# norm_Frechet_mean_gradient_history\n",
    "# geodesics2nearestcentroids_lengths_history\n",
    "# loss_history \n",
    "# are arrays or tensors\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure with 1 row and 3 columns\n",
    "\n",
    "# Plot norm_Frechet_mean_gradient_history\n",
    "axes[0].plot(norm_Frechet_mean_gradient_history, marker='o', markersize=3, label='Frechet mean update history')\n",
    "axes[0].set_title(r'Averege norm of centroid shifts $\\frac{1}{k}\\sum_{j=1}^k \\| \\sum_{i:x_i\\in C_j} l_i v_i \\|_2$ ')\n",
    "axes[0].set_xlabel('Outer loop iterations')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot geodesics2nearestcentroids lengths by cluster\n",
    "# Generate a color palette with distinct colors\n",
    "colors = plt.cm.jet(torch.linspace(0, 1, K))  # Use a colormap (e.g., 'viridis')\n",
    "\n",
    "lengths_of_geodesics2nearestcentroids_concatenated = torch.cat((geodesics2nearestcentroids_lengths_by_cluster_history), dim=0).detach()\n",
    "for i in range(K):\n",
    "    axes[1].plot(lengths_of_geodesics2nearestcentroids_concatenated[:, i],marker='o',markersize=3,\n",
    "                 label=f'Cluster {i} geodesics length', color=colors[i])\n",
    "    axes[1].set_xlabel('Outer Loop Iterations')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "\n",
    "# Plot geodesics2nearestcentroids_lengths_history\n",
    "axes[1].plot(geodesics2nearestcentroids_lengths_history, marker='o', markersize=3, label='All clusters geodesics length', color='green')\n",
    "axes[1].set_title('Length of geodesics to nearest centroids')\n",
    "axes[1].set_xlabel('Outer loop iterations')\n",
    "axes[1].legend()\n",
    "\n",
    "intra_class_variance_concatenated = torch.cat((intra_class_variance_by_cluster_history), dim=0).detach()\n",
    "for i in range(K):\n",
    "    axes[2].plot(intra_class_variance_concatenated[:, i],marker='o',markersize=3,\n",
    "                 label=f'Cluster {i} variance', color=colors[i])\n",
    "    axes[2].set_xlabel('Outer Loop Iterations')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].legend()\n",
    "\n",
    "# Plot geodesics2nearestcentroids_lengths_history\n",
    "axes[2].plot(intra_class_variance_history, marker='o', markersize=3, label='All clusters geodesics length', color='green')\n",
    "axes[2].set_title('Intra-cass variance and variances within each cluster')\n",
    "axes[2].set_xlabel('Outer loop iterations')\n",
    "axes[2].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{Path_pictures}/kmeans_losses.pdf\",bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"Interpolation_points\":\n",
    "    geodesic_curve = RiemannianKmeansTools.geodesics_from_parameters_interpolation_points(\n",
    "        parameters_of_geodesics,\n",
    "        end_points = [encoded_points, current_centroids])\n",
    "elif mode == \"Schauder\":\n",
    "    geodesic_curve = RiemannianKmeansTools.geodesics_from_parameters_schauder(\n",
    "        geodesic_solver, \n",
    "        parameters_of_geodesics, \n",
    "        end_points = [encoded_points, current_centroids])\n",
    "\n",
    "RiemannianKmeansTools.plot_octopus(\n",
    "    geodesic_curve.detach(), \n",
    "    memberships = cluster_index_of_each_point,\n",
    "    saving_folder=Path_pictures,suffix=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"centroid shifts:\\n\", (initial_centroids -  current_centroids))\n",
    "average_cluster_center_shift_norm = (current_centroids - initial_centroids).detach().norm(dim = 1).mean()\n",
    "print(\"Average centroid's shift:\", average_cluster_center_shift_norm.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RiemannianKmeansTools.plot_knn_decision_boundary(encoded_points,labels4coloring=ground_truth_labels, \n",
    "        saving_folder=Path_pictures, file_saving_name=\"Decision_boundary_ground_truth_labels\",\n",
    "        neighbours_number=neighbours_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RiemannianKmeansTools.plot_knn_decision_boundary(encoded_points,labels4coloring=cluster_index_of_each_point, \n",
    "        saving_folder=Path_pictures, file_saving_name=\"Decision_boundary_Riemannian_k_means_labels\",\n",
    "        neighbours_number=neighbours_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define experiment parameters\n",
    "params = {\n",
    "    \"K\": K,  # Number of clusters\n",
    "    \"N\": N,  # Number of points to be clustered\n",
    "    \"selected_labels\": selected_labels,  # Labels used for clustering\n",
    "    \"mode\": mode,  # Can be \"Schauder\" or \"Interpolation_points\"\n",
    "    \n",
    "    # Specific parameters\n",
    "    \"n_max\": n_max,  # Schauder basis complexity\n",
    "    \"step_count\": step_count,  # Number of interpolation steps\n",
    "    \n",
    "    # Optimization parameters\n",
    "    \"beta\": beta,  # Frechet mean learning rate\n",
    "    \"learning_rate\": learning_rate,  # Learning rate for geodesics\n",
    "    \"num_iter_outer\": num_iter_outer,  # Number of Frechet mean updates\n",
    "    \"num_iter_inner\": num_iter_inner,  # Number of geodesic refinement iterations\n",
    "    \"time_secs\": algorithm_execution_time, # Computed using time\n",
    "    \"ground_truth_labels\": ground_truth_labels.tolist(),\n",
    "    \"Riemannian_k_means_labels\": cluster_index_of_each_point.tolist(),\n",
    "    \"encoded_points\": encoded_points.tolist()\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(Path_pictures+\"/params.json\", \"w\") as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(f\"Parameters saved to {Path_pictures}/params.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
