{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal imports\n",
    "import torch, yaml, os, json\n",
    "import ricci_regularization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from ricci_regularization import RiemannianKmeansTools\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_setup_number = 5\n",
    "periodicity_mode = True\n",
    "pretrained_AE_setting_name = 'MNIST_Setting_3_exp1'\n",
    "Path_AE_config = f'../experiments/{pretrained_AE_setting_name}_config.yaml'\n",
    "with open(Path_AE_config, 'r') as yaml_file:\n",
    "    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "selected_labels = yaml_config[\"dataset\"][\"selected_labels\"]\n",
    "K = len(selected_labels) # number of clusters\n",
    "N = 300 # number of points to be clustered\n",
    "\n",
    "# Select labels to make subset of mnist data points to be clustered\n",
    "random_seed_picking_points = 2 #k_means_setup_number\n",
    "\n",
    "mode = \"Schauder\" \n",
    "#mode = \"Interpolation_points\" # alternative option\n",
    "\n",
    "# specific parameters \n",
    "n_max = 7  # Schauder basis complexity (only for Schauder)\n",
    "step_count = 100  # Number of interpolation steps (for both methods)\n",
    "\n",
    "# optimization parameters\n",
    "beta = 2.e-4 # Frechet mean learning rate #beta is learning_rate_frechet_mean (outer loop)\n",
    "learning_rate = 0.1e-4 # learning_rate_geodesics (inner loop)\n",
    "num_iter_outer = 50 # number of Frechet mean updates (outer loop)\n",
    "num_iter_inner = 10 # number of geodesics refinement interations per 1 Frechet mean update (inner loop)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the pretrained AE + creatong directory for results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#experiment_name = yaml_config[\"experiment\"][\"name\"]\n",
    "Path_pictures = \"../experiments/\" + pretrained_AE_setting_name + f\"/K_means_setup_{k_means_setup_number}\"\n",
    "# Check and create directories based on configuration\n",
    "if not os.path.exists(Path_pictures):  # Check if the picture path does not exist\n",
    "    os.mkdir(Path_pictures)  # Create the directory for plots if not yet created\n",
    "    print(f\"Created directory: {Path_pictures}\")  # Print directory creation feedback\n",
    "else:\n",
    "    print(f\"Directiry already exists: {Path_pictures}\")\n",
    "\n",
    "# Load data loaders based on YAML configuration\n",
    "dict = ricci_regularization.DataLoaders.get_dataloaders(\n",
    "    dataset_config=yaml_config[\"dataset\"],\n",
    "    data_loader_config=yaml_config[\"data_loader_settings\"],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "print(\"Experiment results loaded successfully.\")\n",
    "# Loading data\n",
    "train_loader = dict[\"train_loader\"]\n",
    "test_loader = dict[\"test_loader\"]\n",
    "test_dataset = dict.get(\"test_dataset\")  # Assuming 'test_dataset' is a key returned by get_dataloaders\n",
    "print(\"Data loaders created successfully.\")\n",
    "\n",
    "# Loading the pre-tained AE\n",
    "torus_ae, Path_ae_weights = ricci_regularization.DataLoaders.get_tuned_nn(config=yaml_config)\n",
    "print(\"AE weights loaded successfully.\")\n",
    "print(\"AE weights loaded from\", Path_ae_weights)\n",
    "torus_ae.cpu()\n",
    "torus_ae.eval()\n",
    "print(\"AE sent to cpu and eval mode activated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking dataset to be clusterized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting the dataset that we want to cluster\n",
    "# we use some random N points of the test dataset that we will cluster\n",
    "# This could be done differently, e.g. by simply picking random points\n",
    "D = yaml_config[\"architecture\"][\"input_dim\"]\n",
    "d = yaml_config[\"architecture\"][\"latent_dim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering poins to choose N of them with labels in selected_labels\n",
    "#clusters can be unbalanced\n",
    "list_encoded_data_filtered = []\n",
    "list_labels_filtered = []\n",
    "for data,label in train_loader:\n",
    "    mask_batch = torch.isin(label, torch.tensor(selected_labels)) # mask will be used to chose only labels in selected_labels\n",
    "    data_filtered = data[mask_batch]\n",
    "    labels_filtered = label[mask_batch]\n",
    "    enc_images = torus_ae.encoder2lifting(data_filtered.reshape(-1, D)).detach()\n",
    "    list_encoded_data_filtered.append(enc_images)\n",
    "    list_labels_filtered.append(labels_filtered)\n",
    "    #print(labels_filtered)\n",
    "all_encoded_data_filtered = torch.cat(list_encoded_data_filtered)\n",
    "all_labels_filtered = torch.cat(list_labels_filtered)\n",
    "#randomly picking N points with selected labels\n",
    "torch.manual_seed(random_seed_picking_points)\n",
    "indices = torch.randperm(len(all_encoded_data_filtered))[:N]  # Randomly shuffle and pick first N\n",
    "encoded_points = all_encoded_data_filtered[indices]\n",
    "ground_truth_labels = all_labels_filtered[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manifold plot\n",
    "RiemannianKmeansTools.manifold_plot_selected_labels(all_encoded_data_filtered,\n",
    "            all_labels_filtered,selected_labels,\n",
    "            saving_folder=Path_pictures, plot_title=\"Manifold plot for all points with selected labels\",\n",
    "            file_saving_name=\"Manifold_plot_selected_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot for points to cluster\n",
    "RiemannianKmeansTools.manifold_plot_selected_labels(encoded_points,\n",
    "            ground_truth_labels,selected_labels,\n",
    "            saving_folder=Path_pictures, plot_title=\"Encoded Points Colored by Ground Truth Labels\",\n",
    "            file_saving_name=\"ground_truth_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting parameters to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_centroids = RiemannianKmeansTools.initialize_centers(encoded_points, K, N) \n",
    "current_centroids = torch.clone(initial_centroids) \n",
    "\n",
    "if mode == \"Interpolation_points\":\n",
    "    geodesic_solver = None\n",
    "    # Initialize geodesic segments\n",
    "    parameters_of_geodesics = RiemannianKmeansTools.construct_interpolation_points_on_segments_connecting_centers2encoded_data(\n",
    "            encoded_points, \n",
    "            initial_centroids, \n",
    "            num_aux_points = step_count)\n",
    "elif mode == \"Schauder\":\n",
    "    geodesic_solver = ricci_regularization.Schauder.NumericalGeodesics(n_max, step_count)\n",
    "    # Get Schauder basis\n",
    "    N_max = geodesic_solver.schauder_bases[\"zero_boundary\"][\"N_max\"]\n",
    "    basis = geodesic_solver.schauder_bases[\"zero_boundary\"][\"basis\"]\n",
    "    # Define parameters (batch_size × N_max × dim)\n",
    "    parameters_of_geodesics = torch.zeros((N, K, N_max, d), requires_grad=True)\n",
    "init_parameters = torch.clone(parameters_of_geodesics) # save initial segments\n",
    "# Set optimizer params\n",
    "parameters = torch.nn.Parameter(parameters_of_geodesics) # Wrap as a parameter\n",
    "\n",
    "optimizer = torch.optim.SGD([parameters], lr=learning_rate)\n",
    "\n",
    "cluster_index_of_each_point = None\n",
    "geodesics_to_nearest_centroids = None\n",
    "\n",
    "#losses\n",
    "history = []\n",
    "\n",
    "\n",
    "# visualizing initialization (optional)\n",
    "plt.title(\"K-means initialization\")\n",
    "plt.scatter(encoded_points[:,0],encoded_points[:,1], label = \"encoded data\")\n",
    "plt.scatter(initial_centroids[:,0], initial_centroids[:,1], c=\"red\", label = \"initial_centroids\", marker='*', s = 60)\n",
    "plt.xlim(-torch.pi, torch.pi)\n",
    "plt.ylim(-torch.pi, torch.pi)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing\n",
    "start_time = time.time()\n",
    "# sending the nn to selected device (usually it should be cuda)\n",
    "torus_ae.to(device)\n",
    "# ----------------------------\n",
    "# Riemannian K-means Algorithm\n",
    "# ----------------------------\n",
    "# Outer loop \n",
    "t = tqdm(range(num_iter_outer), desc=\"Outer Loop iteration: 0\")\n",
    "for iter_outer in t:    \n",
    "    # Inner loop (refining geodesics)\n",
    "    for iter_inner in range(num_iter_inner):\n",
    "#for iter_outer in range(num_iter_outer):\n",
    "    # Inner loop (refining geodesics)\n",
    "#    for iter_inner in range(num_iter_inner):\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        # Compute the loss\n",
    "        energies_of_geodesics = RiemannianKmeansTools.compute_energy(\n",
    "                mode = mode, \n",
    "                parameters_of_geodesics=parameters, \n",
    "                end_points = [encoded_points, current_centroids],\n",
    "                decoder = torus_ae.decoder_torus,\n",
    "                geodesic_solver = geodesic_solver,\n",
    "                reduction=\"none\", device=device, \n",
    "                periodicity_mode=periodicity_mode)\n",
    "        loss_geodesics = energies_of_geodesics.sum()\n",
    "        # Backpropagation: compute gradients\n",
    "        loss_geodesics.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Store the loss value\n",
    "    # end inner loop\n",
    "    energies_of_geodesics = energies_of_geodesics.cpu()\n",
    "    # compute geodesic_curve of shape (N,K,step_count,d)\n",
    "    # compute a vector of length of all geodesics shape (N,K)\n",
    "    with torch.no_grad():\n",
    "        geodesic_curve, lengths_of_geodesics = RiemannianKmeansTools.compute_lengths(\n",
    "                mode = mode,\n",
    "                parameters_of_geodesics=parameters,\n",
    "                end_points = [encoded_points, current_centroids],\n",
    "                decoder = torus_ae.decoder_torus,\n",
    "                geodesic_solver = geodesic_solver,\n",
    "                reduction=\"none\", device=device, \n",
    "                periodicity_mode=periodicity_mode,\n",
    "                return_geodesic_curve=True) \n",
    "    lengths_of_geodesics = lengths_of_geodesics.cpu() # shape (N,K)\n",
    "    geodesic_curve = geodesic_curve.cpu()\n",
    "\n",
    "    # retrieve the class membership of each point by finding the closest cluster centroid \n",
    "    cluster_index_of_each_point = torch.argmin(lengths_of_geodesics, dim=1) # shape (N)\n",
    "    batch_indices = torch.arange(N) # this is needed, since   geodesic_curve[:, cluster_index_of_each_point, :, :] will produce a tensor of shape (N,N,step_count,d)\n",
    "    # pick only geodesics connecting points to cluster relevant centroids where the points are assigned\n",
    "    geodesics_to_nearest_centroids = geodesic_curve[batch_indices, cluster_index_of_each_point, :, :].detach() # shape (N,step_count,d)\n",
    "\n",
    "    # v is the direction to move the cluster centroids # shape (N,d)\n",
    "    v = geodesics_to_nearest_centroids[:,-1,:] - geodesics_to_nearest_centroids[:,-2,:]\n",
    "    v = v / v.norm(dim=1).unsqueeze(-1) # find the last segments of the geod shape (N,d)\n",
    "    \n",
    "    # Compute weighted Frechet mean gradient for each cluster\n",
    "    weighted_v = lengths_of_geodesics[:, 0].unsqueeze(-1) * v  # Shape: (N, d)\n",
    "    # Create a one-hot encoding of the cluster indices\n",
    "    one_hot_clusters = torch.nn.functional.one_hot(cluster_index_of_each_point, num_classes=K).float()  # Shape: (N, K)\n",
    "    # Compute the gradients for each cluster\n",
    "    Frechet_mean_gradient = one_hot_clusters.T @ weighted_v  # Shape: (K, d)\n",
    "    # Update cluster centroids\n",
    "    with torch.no_grad():\n",
    "        current_centroids += - beta * Frechet_mean_gradient  # Update all centroids simultaneously\n",
    "\n",
    "    # Compute average Frechet mean gradient norm among the K clusters on step iter_outer \n",
    "    average_Frechet_mean_gradient_norm = (Frechet_mean_gradient.norm(dim=1).mean()).item()\n",
    "\n",
    "    # saving the lengths of geodesics_to_nearest_centroids\n",
    "    geodesics_to_nearest_centroids_lengths = lengths_of_geodesics[batch_indices, cluster_index_of_each_point]\n",
    "    \n",
    "    # save intra-class variance\n",
    "    intraclass_variance = (1/N) * energies_of_geodesics[batch_indices, cluster_index_of_each_point]\n",
    "    \n",
    "    #compute the sum of geodesic length for each cluster\n",
    "    #scatter_add_ is the reverse of torch.gather\n",
    "    length_of_geodesics_to_nearest_centroids_by_cluster = torch.zeros(K, dtype=geodesics_to_nearest_centroids_lengths.dtype)\n",
    "    length_of_geodesics_to_nearest_centroids_by_cluster.scatter_add_(0, cluster_index_of_each_point, geodesics_to_nearest_centroids_lengths)    \n",
    "    \n",
    "    #compute the Intra-class variance, i.e. sum of geodesic energy for each cluster\n",
    "    #scatter_add_ is the reverse of torch.gather\n",
    "    intraclass_variance_by_cluster = torch.zeros(K, dtype=geodesics_to_nearest_centroids_lengths.dtype)\n",
    "    intraclass_variance_by_cluster.scatter_add_(0, cluster_index_of_each_point, intraclass_variance)    \n",
    "    \n",
    "    history_item = {\n",
    "        \"intraclass_variance\"                              : intraclass_variance.detach().sum().numpy(),\n",
    "        \"intraclass_variance_by_cluster\"                   : intraclass_variance_by_cluster.unsqueeze(0).detach().numpy(), \n",
    "        \"norm_Frechet_mean_gradient\"                       : average_Frechet_mean_gradient_norm,\n",
    "        \"geodesics_to_nearest_centroids_lengths\"           : geodesics_to_nearest_centroids_lengths.detach().sum().numpy(),\n",
    "        \"geodesics_to_nearest_centroids_lengths_by_cluster\": length_of_geodesics_to_nearest_centroids_by_cluster.unsqueeze(0).detach().numpy()\n",
    "    }\n",
    "    history.append( history_item )\n",
    "    t.set_description(f\"Outer Loop iteration: {iter_outer+1}, Centroid gradient norm:{average_Frechet_mean_gradient_norm:.4f}, Total geodesic energy:{loss_geodesics:.4f}\")  # Update description dynamically\n",
    "#timing\n",
    "end_time = time.time()\n",
    "algorithm_execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Frechet_mean_gradient_history = []\n",
    "geodesics_to_nearest_centroids_lengths_by_cluster_history = []\n",
    "geodesics_to_nearest_centroids_lengths_history = []\n",
    "intraclass_variance_by_cluster_history = []\n",
    "intraclass_variance_history = []\n",
    "for i in range(len(history)):\n",
    "    norm_Frechet_mean_gradient_history.append(history[i][\"norm_Frechet_mean_gradient\"])\n",
    "    geodesics_to_nearest_centroids_lengths_by_cluster_history.append(history[i][\"geodesics_to_nearest_centroids_lengths_by_cluster\"])\n",
    "    geodesics_to_nearest_centroids_lengths_history.append(history[i][\"geodesics_to_nearest_centroids_lengths\"])\n",
    "    intraclass_variance_by_cluster_history.append(history[i][\"intraclass_variance_by_cluster\"])\n",
    "    intraclass_variance_history.append(history[i][\"intraclass_variance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting losses\n",
    "# In this cell: \n",
    "# norm_Frechet_mean_gradient_history\n",
    "# geodesics_to_nearest_centroids_lengths_history\n",
    "# loss_history \n",
    "# are arrays or tensors\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure with 1 row and 3 columns\n",
    "\n",
    "# Plot norm_Frechet_mean_gradient_history\n",
    "axes[0].plot(norm_Frechet_mean_gradient_history, marker='o', markersize=3) \n",
    "axes[0].set_title('Average norm of gradients of centroids')\n",
    "axes[0].set_xlabel('Outer loop iterations')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "# Plot geodesics_to_nearest_centroids lengths by cluster\n",
    "# Generate a color palette with distinct colors\n",
    "colors = plt.cm.jet(torch.linspace(0, 1, K))  # Use a colormap (e.g., 'viridis')\n",
    "\n",
    "#lengths_of_geodesics_to_nearest_centroids_concatenated = torch.cat((geodesics_to_nearest_centroids_lengths_by_cluster_history), dim=0).detach()\n",
    "lengths_of_geodesics_to_nearest_centroids_concatenated = np.concatenate(geodesics_to_nearest_centroids_lengths_by_cluster_history)\n",
    "for i in range(K):\n",
    "    axes[1].plot(lengths_of_geodesics_to_nearest_centroids_concatenated[:, i],marker='o',markersize=3,\n",
    "                 label=f'Lengths of geodesics in cluster {i}', color=colors[i])\n",
    "    axes[1].set_xlabel('Outer Loop Iterations')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "\n",
    "# Plot geodesics_to_nearest_centroids_lengths_history\n",
    "axes[1].plot(geodesics_to_nearest_centroids_lengths_history, marker='o', markersize=3, \n",
    "             label='Lengths of geodesics in all clusters', color='green')\n",
    "axes[1].set_title('Lengths of geodesics to nearest centroids')\n",
    "axes[1].set_xlabel('Outer loop iterations')\n",
    "axes[1].legend(loc= 'upper right')\n",
    "\n",
    "intraclass_variance_concatenated = np.concatenate(intraclass_variance_by_cluster_history)\n",
    "#torch.cat((intraclass_variance_by_cluster_history), dim=0).detach()\n",
    "for i in range(K):\n",
    "    axes[2].plot(intraclass_variance_concatenated[:, i],marker='o',markersize=3,\n",
    "                 label=f'Variance of geodesics of cluster {i} ', color=colors[i])\n",
    "    axes[2].set_xlabel('Outer Loop Iterations')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].legend()\n",
    "\n",
    "# Plot geodesics_to_nearest_centroids_lengths_history\n",
    "axes[2].plot(intraclass_variance_history, marker='o', markersize=3,\n",
    "             label='Intra-class variance', color='green')\n",
    "axes[2].set_title('Intra-class variances')\n",
    "axes[2].set_xlabel('Outer loop iterations')\n",
    "axes[2].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{Path_pictures}/kmeans_losses.pdf\",bbox_inches='tight', format=\"pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Final values of losses:')\n",
    "print('-----------------------')\n",
    "print(f'Intra-class variance: {intraclass_variance_history[-1]:.3f}')\n",
    "print(f'Lengths of geodesics to nearest centroids: {geodesics_to_nearest_centroids_lengths_history[-1]:.3f}')\n",
    "print(f'Centroid gradient average norm: {norm_Frechet_mean_gradient_history[-1]:.3f}')\n",
    "print(f'Centroid shift average norm: {beta*norm_Frechet_mean_gradient_history[-1]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"Interpolation_points\":\n",
    "    geodesic_curve = RiemannianKmeansTools.geodesics_from_parameters_interpolation_points(\n",
    "        parameters_of_geodesics,\n",
    "        end_points = [encoded_points, current_centroids])\n",
    "\n",
    "elif mode == \"Schauder\":\n",
    "    geodesic_curve = RiemannianKmeansTools.geodesics_from_parameters_schauder(\n",
    "        geodesic_solver, \n",
    "        parameters_of_geodesics, \n",
    "        end_points = [encoded_points, current_centroids],periodicity_mode=periodicity_mode)\n",
    "RiemannianKmeansTools.plot_octopus(\n",
    "    geodesic_curve.detach(), \n",
    "    memberships = cluster_index_of_each_point,\n",
    "    saving_folder=Path_pictures,suffix=0, periodicity_mode=periodicity_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total centroid shifts during training:\\n\", (initial_centroids -  current_centroids))\n",
    "average_cluster_center_shift_norm = (current_centroids - initial_centroids).detach().norm(dim = 1).mean()\n",
    "print(\"Average norm of these shifts:\", average_cluster_center_shift_norm.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving parameters of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment parameters\n",
    "params = {\n",
    "    \"K\": K,  # Number of clusters\n",
    "    \"N\": N,  # Number of points to be clustered\n",
    "    \"selected_labels\": selected_labels,  # Labels used for clustering\n",
    "    \"mode\": mode,  # Can be \"Schauder\" or \"Interpolation_points\"\n",
    "    \n",
    "    # Specific parameters\n",
    "    \"n_max\": n_max,  # Schauder basis complexity\n",
    "    \"step_count\": step_count,  # Number of interpolation steps\n",
    "    \n",
    "    # Optimization parameters\n",
    "    \"beta\": beta,  # Frechet mean learning rate\n",
    "    \"learning_rate\": learning_rate,  # Learning rate for geodesics\n",
    "    \"num_iter_outer\": num_iter_outer,  # Number of Frechet mean updates\n",
    "    \"num_iter_inner\": num_iter_inner,  # Number of geodesic refinement iterations\n",
    "    \"time_secs\": algorithm_execution_time, # Computed using time\n",
    "    \"ground_truth_labels\": ground_truth_labels.tolist(),\n",
    "    \"Riemannian_k_means_labels\": cluster_index_of_each_point.tolist(),\n",
    "    \"encoded_points\": encoded_points.tolist()\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(Path_pictures+\"/params.json\", \"w\") as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(f\"Parameters saved to {Path_pictures}/params.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving separetely all the optimized geodesics, shape (N, K, step_count, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save additional info\n",
    "torch.save(geodesic_curve, Path_pictures+\"/geodesic_curve.pt\")\n",
    "print(f\"Discretized geodesic curves saved to {Path_pictures}/geodesic_curve.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
