{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "device = torch.device(\"cpu\")\n",
    "import torch\n",
    "import torch.func as TF\n",
    "import torch.autograd.functional as AF\n",
    "import torch.autograd as TA\n",
    "from functorch import jacrev,jacfwd\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Tutorial\n",
    "\n",
    "Introducing the Decoder used in ConvAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, \n",
    "        unflattened_size=(32, 3, 3))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, \n",
    "            stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
    "            padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
    "            padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "decoder = Decoder(encoded_space_dim = 2,fc2_input_dim=128)\n",
    "# Send to device\n",
    "decoder.to(device) \n",
    "\n",
    "# Load the parameters of the trained decoder without curvature in Loss func\n",
    "PATH_dec = '../nn_weights/decoder_conv_autoenc.pt'\n",
    "decoder.load_state_dict(torch.load(PATH_dec))\n",
    "\n",
    "# Switch to eval mode\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dot                            # [D], [D] -> []\n",
    "batched_dot = torch.func.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n",
    "x, y = torch.randn(2, 5), torch.randn(2, 5)\n",
    "a = batched_dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sum(x*y,dim = 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example of vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vectorized = TF.vmap(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vectorized(torch.rand(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. vmap for computing the metric using finite differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us take a uniform grid on the latent space. Note that here d=2. The bounds for the grid can be taken from 3 sigma rule. \n",
    "#We will take 2 sigmas however\n",
    "numsteps =10\n",
    "\n",
    "def make_grid(numsteps):\n",
    "    \n",
    "    xs = torch.linspace(-1.5, 1.5, steps = numsteps)\n",
    "    ys = torch.linspace(-1.5, 1.5, steps = numsteps)\n",
    "    #uniform_grid = torch.cartesian_prod(xs,ys)\n",
    "\n",
    "    # true grid starts from left bottom corner. x is the first to increase\n",
    "    tgrid = torch.cartesian_prod(ys, xs)\n",
    "    tgrid = tgrid.roll(1,1)\n",
    "    return tgrid\n",
    "\n",
    "grid = make_grid(numsteps)\n",
    "numsteps = int(np.sqrt(grid.shape[0]))\n",
    "    \n",
    "hx = float(abs((grid[numsteps**2 - 1] - grid[0])[0]))/(numsteps - 1)\n",
    "hy = float(abs((grid[numsteps**2 - 1] - grid[0])[1]))/(numsteps - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing metric at one point with finite differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical metric computation\n",
    "def metric_num_smart(u, hx=hx, hy=hy): #this gives metric at point u with steps hx and hy\n",
    "    u = u.reshape(-1,2)\n",
    "\n",
    "    dx = torch.tensor([[0.0 + hx, 0.0]])\n",
    "    dy = torch.tensor([[0.0, 0.0 + hy]])\n",
    "    dpsi_over_dx = (decoder(u + dx) - decoder(u - dx))/(2*hx)\n",
    "    dpsi_over_dy = (decoder(u + dy) - decoder(u - dy))/(2*hy)\n",
    "\n",
    "    dpsi_over_dx = torch.flatten(dpsi_over_dx).view(784,1)\n",
    "    dpsi_over_dy = torch.flatten(dpsi_over_dy).view(784,1)\n",
    "    \n",
    "    dpsi = torch.cat((dpsi_over_dx,dpsi_over_dy),dim=-1)\n",
    "    my_metric = torch.matmul(dpsi.T,dpsi)\n",
    "    return my_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical metric computation\n",
    "def metric_num(u, hx=hx, hy=hy): #this gives metric at point u with steps hx and hy\n",
    "    u = u.reshape(-1,2)\n",
    "    #dx = torch.tensor([[0.0 + hx, 0.0]])\n",
    "    # u -> p and so on...\n",
    "    dpsidx = (decoder(u + torch.tensor([[0.0 + hx, 0.0]])) - decoder(u + torch.tensor([[0.0 - hx, 0.0]])))/(2*hx)\n",
    "    dpsidy = (decoder(u + torch.tensor([[0.0, 0.0 + hy]])) - decoder(u + torch.tensor([[0.0, 0.0 - hy]])))/(2*hy)\n",
    "    #my_metric = torch.tensor([[(dpsidx*dpsidx).sum(),(dpsidx*dpsidy).sum()],\n",
    "    #                         [(dpsidx*dpsidy).sum(),(dpsidy*dpsidy).sum()]])\n",
    "    #torch.dot(dpsidx,dpsidx)\n",
    "    #return my_metric\n",
    "    #dpsidx - > dpsi_over_dx\n",
    "    dpsidx = torch.flatten(dpsidx)\n",
    "    dpsidy = torch.flatten(dpsidy)\n",
    "    #print(dpsidx.shape)\n",
    "    g11 = torch.dot(dpsidx,dpsidx)\n",
    "    g12 = torch.dot(dpsidx,dpsidy)\n",
    "    g22 = torch.dot(dpsidy,dpsidy)\n",
    "    #dpsi = torch.cat((dpsidx,dpsidy),dim=-1)\n",
    "    #print(dpsi.shape)\n",
    "    #my_metric = torch.matmul(dpsi.T,dpsi)\n",
    "    #my_metric = torch.tensor([[g11,g12],\n",
    "    #                         [g12,g22]])\n",
    "    #return my_metric\n",
    "    #return decoder(u)\n",
    "    return g11, g12, g12, g22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1e-5 #step\n",
    "#u = torch.tensor([[0.,0.],[0.,0.],[0.,0.]]) # point\n",
    "u = torch.tensor([0.,0.])\n",
    "with torch.no_grad():\n",
    "    print(metric_num(u,h,h))\n",
    "    print(metric_num_smart(u,h,h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized numerical computation with vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_num_vectorized = TF.vmap(metric_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100*100 # number of points\n",
    "with torch.no_grad():\n",
    "    mertic_numerical_list = metric_num_vectorized(torch.rand(N,2))\n",
    "    #print(mertic_numerical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the array of metrics\n",
    "def turn_metric_to_tensor(mertic_numerical_list):\n",
    "    N = len(mertic_numerical_list[0])\n",
    "    metric_as_tensor = torch.cat(mertic_numerical_list)\n",
    "    metric_as_tensor = metric_as_tensor.reshape(-1,N)\n",
    "    #print(torch.equal(metric_as_tensor[1], metric_as_tensor[2])) # check symmetry\n",
    "    metric_as_tensor = metric_as_tensor.reshape(2,2,-1)\n",
    "    metric_as_tensor = metric_as_tensor.T\n",
    "    #metric_as_tensor [:5] # see first 5 metric matrices\n",
    "    return metric_as_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_num_smart_vectorized = TF.vmap(metric_num_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100*100 # number of points\n",
    "with torch.no_grad():\n",
    "    mertic_numerical_smart_list = metric_num_smart_vectorized(torch.rand(N,2))\n",
    "    #print(mertic_numerical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mertic_numerical_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mertic_numerical_smart_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking  f.d. on a grid with torch.vmap VS using torch.roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric on a grid\n",
    "\n",
    "def g(grid):\n",
    "    numsteps = int(np.sqrt(grid.shape[0]))\n",
    "    \n",
    "    hx = float(abs((grid[numsteps**2 - 1] - grid[0])[0]))/(numsteps - 1)\n",
    "    hy = float(abs((grid[numsteps**2 - 1] - grid[0])[1]))/(numsteps - 1)\n",
    "    \n",
    "    latent = grid\n",
    "    latent = latent.to(device)\n",
    "    psi = decoder(latent)\n",
    "    psi_next_x =  psi.roll(-1,0)\n",
    "    psi_prev_x =  psi.roll(1,0)\n",
    "    psi_next_y =  psi.roll(-numsteps,0)\n",
    "    psi_prev_y =  psi.roll(numsteps,0)\n",
    "    \n",
    "    dpsidx = (psi_next_x - psi_prev_x)/(2*hx)\n",
    "    dpsidy = (psi_next_y - psi_prev_y)/(2*hy)\n",
    "    \n",
    "    metric = torch.cat(((dpsidx*dpsidx).sum((1,2,3)),(dpsidx*dpsidy).sum((1,2,3)),(dpsidx*dpsidy).sum((1,2,3)),(dpsidy*dpsidy).sum((1,2,3))),0)\n",
    "    metric = metric.view(4, numsteps*numsteps)\n",
    "    metric = metric.transpose(0, 1)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the grid of metric\n",
    "tgrid = make_grid(numsteps)\n",
    "\n",
    "with torch.no_grad():\n",
    "    metric_torchroll = g(tgrid)\n",
    "    metric_torchroll = metric_torchroll.view(numsteps*numsteps, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgrid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_torchroll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    metric_torchvmap_list = metric_num_vectorized(tgrid)\n",
    "metric_torchvmap = turn_metric_to_tensor(metric_torchvmap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(metric_torchvmap,metric_torchroll) #errors on the border for torch.roll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(metric_torchroll-metric_torchvmap) # errors on the border for torch.roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no border\n",
    "metric_torchvmap_no_border = metric_torchvmap.view(numsteps,numsteps,2,2)[1:-1,1:-1]\n",
    "metric_torchroll_no_border = metric_torchroll.view(numsteps,numsteps,2,2)[1:-1,1:-1]\n",
    "#Newfrob = metric.norm(dim=(1,2)).view(numsteps,numsteps)\n",
    "#Newfrob = Newfrob[1:-1,1:-1].transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"L1 error:\", float(torch.max(metric_torchroll_no_border - metric_torchvmap_no_border))) # no error in L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len((metric_torchroll_no_border - metric_torchvmap_no_border).flatten())\n",
    "print(\"MSE:\",float(((metric_torchroll_no_border - metric_torchvmap_no_border)**2).sum()/size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsteps = 10\n",
    "tgrid = make_grid(numsteps=numsteps)\n",
    "\n",
    "#with_torchroll = Timer(stmt=\"g(tgrid)\", globals=globals())\n",
    "#with_vmap = Timer(stmt=\"metric_num_vectorized(tgrid)\", globals=globals())\n",
    "\n",
    "with_torchroll_timer = timeit.timeit(stmt=\"g(tgrid)\",number=100,globals=globals())\n",
    "with_vmap_timer = timeit.timeit(stmt=\"metric_num_smart_vectorized(tgrid)\",number=100, globals=globals())\n",
    "\n",
    "print(\"using torch.roll:\", with_torchroll_timer)\n",
    "print(\"using vmap:\",with_vmap_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numstep_array = np.linspace(10,300,30).astype(int)\n",
    "numstep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_time_roll =[]\n",
    "computation_time_vmap =[]\n",
    "\n",
    "for i in numstep_array:\n",
    "    numsteps = i\n",
    "    tgrid = make_grid(numsteps=numsteps)\n",
    "\n",
    "    with_torchroll_timer = timeit.timeit(stmt=\"g(tgrid)\",number=1,globals=globals())\n",
    "    with_vmap_timer = timeit.timeit(stmt=\"metric_num_vectorized(tgrid)\",number=1, globals=globals())\n",
    "\n",
    "    computation_time_roll.append(with_torchroll_timer)\n",
    "    computation_time_vmap.append(with_vmap_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_time_vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numstep_array[::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(computation_time_roll,label=\"toch.roll\")\n",
    "plt.plot(computation_time_vmap,label=\"vmap\")\n",
    "plt.xticks(numstep_array[::3]/10, labels=numstep_array[::3])\n",
    "plt.title(\"Comparison of torch.roll and vmap performance\")\n",
    "plt.xlabel(\"Linear size of the grid\")\n",
    "plt.ylabel(\"Time in seconds\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Selecting a method for automatic differentiation using autograd.grad\n",
    "\n",
    "Conclusion is to use jacfwd + vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([3.,5.],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.grad(f(input)[0],input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(tensor):\n",
    "    return torch.sum(tensor*tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.grad(g(input),input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd.functional.jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(0.)\n",
    "\n",
    "AF.jacobian(torch.sin, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(10)\n",
    "input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF.jacobian(torch.sin, input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input.reshape(-1,2)\n",
    "decoder(input).shape\n",
    "input.shape\n",
    "AF.jacobian(decoder, input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing autograd with vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "input = torch.rand(10,2).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.grad(decoder(input)[0,0,0,0],input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing autograd.jacobian with vmap is unsupported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_auto_jacobian(input):\n",
    "    input = input.reshape(-1,2)\n",
    "    decoder(input).shape\n",
    "    return AF.jacobian(decoder, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_auto_jacobian(torch.rand(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_auto_jacobian_vectorized = TF.vmap(decoder_auto_jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_auto_jacobian_vectorized(torch.rand(10,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jacrev and jacfwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5)\n",
    "jacobian = jacrev(torch.sin)(x)\n",
    "expected = torch.diag(torch.cos(x))\n",
    "assert torch.allclose(jacobian, expected)\n",
    "print(jacobian)\n",
    "print(jacobian.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jacrev+vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_auto_jacrev(input):\n",
    "    input = input.reshape(-1,2)\n",
    "    decoder(input).shape\n",
    "    return jacrev(decoder)(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_auto_jacrev(torch.rand(5,2)).shape #it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_auto_jacrev_vectorized = TF.vmap(decoder_auto_jacrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_auto_jacrev_vectorized(torch.rand(10,2)).shape # it works!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about bigger stuff??\n",
    "N = 1000\n",
    "decoder_auto_jacrev_vectorized(torch.rand(N,2)).shape # it works in 7.2 secs!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jacfwd + vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_auto_jacfwd(input):\n",
    "    input = input.reshape(-1,2)\n",
    "    return jacfwd(decoder)(input)\n",
    "decoder_auto_jacfwd_vectorized = TF.vmap(decoder_auto_jacfwd)\n",
    "decoder_auto_jacfwd_vectorized(torch.rand(10,2)).shape # it works!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about bigger stuff??\n",
    "N = 10000\n",
    "decoder_auto_jacfwd_vectorized(torch.rand(N,2)).shape # it works in 0.1 secs!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: Reda was right the jacfwd+vmap seems to be the thing we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Metric using jacfwd + vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = torch.rand(1,2)\n",
    "input = torch.zeros(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_jacfwd(input):\n",
    "    input = input.reshape(-1,2)\n",
    "    jac = jacfwd(decoder)(input)\n",
    "    jac = jac.reshape(-1,2)\n",
    "    metric = torch.matmul(jac.T,jac)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. 1. Comparing with finite differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_jacfwd(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 1e-2\n",
    "result = metric_num(input, precision, precision)\n",
    "torch.tensor( result ).view(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2. Metric with jacfwd + vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_jacfwd_vectorized = TF.vmap(metric_jacfwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numstep_array = np.linspace(10,210,num=5).astype(int)\n",
    "numstep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#computation_time_roll =[]\n",
    "computation_time_vmap =[]\n",
    "computation_time_jacfwd =[]\n",
    "\n",
    "for i in numstep_array:\n",
    "    numsteps = i\n",
    "    tgrid = make_grid(numsteps=numsteps)\n",
    "\n",
    "    #with_torchroll_timer = timeit.timeit(stmt=\"g(tgrid)\",number=1,globals=globals())\n",
    "    with_vmap_timer = timeit.timeit(stmt=\"metric_num_vectorized(tgrid)\",number=1, globals=globals())\n",
    "    with_jacfwd_timer = timeit.timeit(stmt=\"metric_jacfwd_vectorized(tgrid)\",number=1,globals=globals())\n",
    "\n",
    "    computation_time_vmap.append(with_vmap_timer)\n",
    "    computation_time_jacfwd.append(with_jacfwd_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(computation_time_jacfwd,label=\"vmap+jacfwd\")\n",
    "plt.plot(computation_time_vmap,label=\"Finite differences using vmap\")\n",
    "plt.xticks((numstep_array-10)/50, labels=numstep_array)\n",
    "plt.title(\"Comparison of jacfwd and numerical vmap performance\")\n",
    "plt.xlabel(\"Linear size of the grid\")\n",
    "plt.ylabel(\"Time in seconds\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why jacrev is not an option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_jacrev(input):\n",
    "    input = input.reshape(-1,2)\n",
    "    jac = jacrev(decoder)(input)\n",
    "    jac = jac.reshape(-1,2)\n",
    "    metric = torch.matmul(jac.T,jac)\n",
    "    return metric\n",
    "metric_jacrev_vectorized = TF.vmap(metric_jacrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_jacrev_vectorized(torch.rand(1600,2)).shape # this is not computable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numstep_array = np.linspace(10,30,num=6).astype(int)\n",
    "numstep_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computation_time_roll =[]\n",
    "computation_time_jacrev =[]\n",
    "computation_time_jacfwd =[]\n",
    "\n",
    "for i in numstep_array:\n",
    "    numsteps = i\n",
    "    tgrid = make_grid(numsteps=numsteps)\n",
    "\n",
    "    #with_torchroll_timer = timeit.timeit(stmt=\"g(tgrid)\",number=1,globals=globals())\n",
    "    with_jacrev_timer = timeit.timeit(stmt=\"metric_jacrev_vectorized(tgrid)\",number=1, globals=globals())\n",
    "    with_jacfwd_timer = timeit.timeit(stmt=\"metric_jacfwd_vectorized(tgrid)\",number=1,globals=globals())\n",
    "\n",
    "    computation_time_jacrev.append(with_jacrev_timer)\n",
    "    computation_time_jacfwd.append(with_jacfwd_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(computation_time_jacfwd,label=\"vmap+jacfwd\")\n",
    "plt.plot(computation_time_jacrev,label=\"vmap+jacrev\")\n",
    "plt.xticks((numstep_array-10)/4, labels=numstep_array)\n",
    "plt.title(\"Comparison of jacfwd and jacrev\")\n",
    "plt.xlabel(\"Linear size of the grid\")\n",
    "plt.ylabel(\"Time in seconds\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Frobenius norm of the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsteps = 200\n",
    "tgrid = make_grid(numsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.1. With jacfwd+vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsteps = 100\n",
    "tgrid = make_grid(numsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_on_grid_jacfwd = metric_jacfwd_vectorized(tgrid).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.linspace(-1.5, 1.5, steps = numsteps)\n",
    "ys = torch.linspace(-1.5, 1.5, steps = numsteps)\n",
    "\n",
    "# Fast computation of Frobenious norm on the grid without borders\n",
    "Newfrob1 = metric_on_grid_jacfwd.norm(dim=(1,2)).view(numsteps,numsteps)\n",
    "Newfrob1 = Newfrob1[1:-1,1:-1].transpose(0,1)\n",
    "#Heat map of the frobenius norm\n",
    "h = plt.contourf(xs[1:-1], ys[1:-1], Newfrob1)\n",
    "plt.title('Heatmap of the Frobenius norm of the metric')\n",
    "plt.xlabel( \"x coordinate\")\n",
    "plt.ylabel( \"y coordinate\")\n",
    "plt.axis('scaled')\n",
    "plt.colorbar(label=\"Frobenius norm of the metric\")\n",
    "#plt.xlim(-1.5 + mean[0], 1.5 + mean[0])\n",
    "#plt.ylim(-1.5 + mean[1], 1.5 + mean[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.2. With f.d. + torch.roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsteps = 200\n",
    "tgrid = make_grid(numsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    metric_torchroll = g(tgrid)\n",
    "    metric_torchroll = metric_torchroll.view(numsteps*numsteps, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.linspace(-1.5, 1.5, steps = numsteps)\n",
    "ys = torch.linspace(-1.5, 1.5, steps = numsteps)\n",
    "\n",
    "# Fast computation of Frobenious norm on the grid without borders\n",
    "Newfrob2 = metric_torchroll.norm(dim=(1,2)).view(numsteps,numsteps)\n",
    "#Newfrob2 = metric_torchroll.view(numsteps,numsteps,2,2)[1:-1,1:-1].norm(dim=(2,3)).transpose(0,1)\n",
    "\n",
    "Newfrob2 = Newfrob2[1:-1,1:-1].transpose(0,1)\n",
    "#Heat map of the frobenius norm\n",
    "h = plt.contourf(xs[1:-1], ys[1:-1],1e+4*Newfrob2)\n",
    "plt.title('Heatmap of the Frobenius norm of the metric')\n",
    "plt.xlabel( \"x coordinate\")\n",
    "plt.ylabel( \"y coordinate\")\n",
    "plt.axis('scaled')\n",
    "plt.colorbar(label=\"Frobenius norm of the metric\")\n",
    "#plt.xlim(-1.5 + mean[0], 1.5 + mean[0])\n",
    "#plt.ylim(-1.5 + mean[1], 1.5 + mean[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat map of the frobenius norm\n",
    "h = plt.contourf(xs[1:-1], ys[1:-1],100*abs(Newfrob1-Newfrob2)/Newfrob1)\n",
    "plt.title('Heatmap of relative error')\n",
    "plt.xlabel( \"x coordinate\")\n",
    "plt.ylabel( \"y coordinate\")\n",
    "plt.axis('scaled')\n",
    "plt.colorbar(label=\"Relative error for the Frobenius norm of the metric\")\n",
    "#plt.xlim(-1.5 + mean[0], 1.5 + mean[0])\n",
    "#plt.ylim(-1.5 + mean[1], 1.5 + mean[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the error is ~10% for 200x200 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_on_grid_jacfwd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Higher order derivatives using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5)\n",
    "jacobian = jacrev(torch.sin)(x)\n",
    "expected = torch.diag(torch.cos(x))\n",
    "assert torch.allclose(jacobian, expected)\n",
    "print(jacobian)\n",
    "print(jacobian.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacfun = jacrev(torch.square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_der = jacrev(jacfun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_der(torch.tensor([1.,3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacfun(torch.tensor(0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
