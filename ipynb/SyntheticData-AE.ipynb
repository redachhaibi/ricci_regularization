{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for dataset\n",
    "D = 784       #dimension\n",
    "k = 3         # num of 2d planes in dim D\n",
    "n = 6*(10**3) # num of points in each plane\n",
    "shift_class = 0\n",
    "\n",
    "\n",
    "# Hyperparameters for data loaders\n",
    "batch_size  = 16\n",
    "split_ratio = 0.2\n",
    "\n",
    "# Set manual seed for reproducibility\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ricci_regularization\n",
    "\n",
    "# Generate dataset\n",
    "train_dataset = ricci_regularization.generate_dataset(D, k, n, shift_class=shift_class)\n",
    "\n",
    "m = len(train_dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(train_dataset, [int(m-m*split_ratio), int(m*split_ratio)])\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(test_data , batch_size=batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test_data[:][0] will give the vectors of data without labels from the test part of the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Declaration of AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "cuda_on = torch.cuda.is_available()\n",
    "if cuda_on:\n",
    "    device  = torch.device(\"cuda\") \n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        #self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512, bias=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 256, bias=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 128, bias=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, hidden_dim, bias=True),\n",
    "        )\n",
    "        self.kl = 0 # For compatibility\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        # out = torch.relu(out)\n",
    "        #out = torch.sin(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        #out = F.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        #self.decoder = nn.Linear(hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128, bias=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 256, bias=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 512, bias=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, output_dim, bias=True),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.decoder(x)\n",
    "        #out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr         = 2e-5\n",
    "momentum   = 0.8\n",
    "num_epochs = 5\n",
    "batches_per_plot = 50\n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "### Initialize the two networks\n",
    "d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical\n",
    "#model = Autoencoder(hidden_dim=hidden_dim)\n",
    "encoder = Encoder(input_dim=784, hidden_dim=d)\n",
    "decoder = Decoder(hidden_dim=d, output_dim=784)\n",
    "\n",
    "# VAE\n",
    "#encoder = VariationalEncoder(input_dim=784, hidden_dim=d, cuda=cuda_on)\n",
    "#decoder = Decoder(hidden_dim=d, output_dim=784)\n",
    "\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.RMSprop(params_to_optimize, lr=lr, momentum=momentum, weight_decay=0.0)\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from https://gist.github.com/jakevdp/91077b0cae40f8f8244a\n",
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\"\"\"\n",
    "\n",
    "    # Note that if base_cmap is a string or None, you can simply do\n",
    "    #    return plt.cm.get_cmap(base_cmap, N)\n",
    "    # The following works for string, None, or a colormap instance:\n",
    "\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_plot(encoder, data, batch_idx):\n",
    "\n",
    "    labels = data[:][1]\n",
    "    data   = data[:][0]\n",
    "\n",
    "    # Encode\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.view(-1,28*28) # reshape the img\n",
    "        data = data.to(device)\n",
    "        encoded_data = encoder(data)\n",
    "\n",
    "    # Record codes\n",
    "    latent = encoded_data.cpu().numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter( latent[:,0], latent[:,1], c=labels, alpha=0.5, marker='o', edgecolor='none', cmap=discrete_cmap(k, 'jet'))\n",
    "    plt.title( f'''Latent space for test data in AE at batch {batch_idx}''')\n",
    "    plt.colorbar(ticks=range(k))\n",
    "    axes = plt.gca()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches per epoch\n",
    "print( \"Reality check of batch splitting: \")\n",
    "print( \"-- Batches per epoch\", len(train_loader) )\n",
    "print( \"batch size:\", batch_size )\n",
    "print( \"product: \", len(train_loader)*batch_size )\n",
    "print( \"-- To be compared to:\", (1.0-split_ratio)*n*k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diz_loss = {'train_loss':[],'mse_loss':[]}\n",
    "diz_loss = {'train_loss':[]}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "   # Set train mode for both the encoder and the decoder\n",
    "   encoder.train()\n",
    "   decoder.train()\n",
    "   mse_loss = []\n",
    "   \n",
    "   batch_idx = 0\n",
    "   # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "   for image_batch, _ in train_loader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "      #shaping the images properly\n",
    "      image_batch = image_batch.view(-1,28*28)\n",
    "      # Move tensor to the proper device\n",
    "      image_batch = image_batch.to(device)\n",
    "      # True batch size\n",
    "      true_batch_size = image_batch.shape[0]\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Front-propagation\n",
    "      # -- Encode data\n",
    "      encoded_data = encoder(image_batch)\n",
    "      # -- Decode data\n",
    "      decoded_data = decoder(encoded_data)\n",
    "      # --Evaluate loss\n",
    "      #loss = loss_fn(decoded_data, image_batch)\n",
    "      loss = torch.sum( (decoded_data-image_batch)**2 )/true_batch_size\n",
    "\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      # Print batch loss\n",
    "      print('\\t Partial train loss (single batch): %f' % (loss.data))\n",
    "    \n",
    "      #print('\\t partial train loss (single batch): {:.6} \\t curv_loss {:.6} \\t mse {:.6}'.format(loss.data, new_loss, only_mse.data))\n",
    "      \n",
    "      mse_loss.append(float(loss.detach().cpu().numpy()))\n",
    "\n",
    "      # Plot      \n",
    "      if (batch_idx % batches_per_plot == 0):\n",
    "            plot = point_plot(encoder, test_data, batch_idx)\n",
    "            plot.show()\n",
    "       # end if\n",
    "\n",
    "      batch_idx += 1\n",
    "   # end for\n",
    "   \n",
    "   train_info = mse_loss\n",
    "   train_loss = np.mean(train_info)\n",
    "   \n",
    "   print('\\n EPOCH {}/{} \\t train loss {}'.format(epoch + 1, num_epochs, train_loss))\n",
    "   #plot = point_plot(test_data.cpu())\n",
    "   #plot.show()\n",
    "   \n",
    "   diz_loss['train_loss'].append(train_info)\n",
    "   \n",
    "diz_loss['train_loss'] = np.array(diz_loss['train_loss']).flatten()\n",
    "#diz_loss['mse_loss'] = np.array(diz_loss['mse_loss']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(diz_loss['train_loss'], label='Train_loss')\n",
    "#plt.semilogy(diz_loss['train_loss'] - diz_loss['mse_loss'], label='Curv_loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print( \"Computing latent variables for train dataset\" )\n",
    "\n",
    "inference_batch_size    = 1024\n",
    "inference_train_loader  = torch.utils.data.DataLoader( train_data , batch_size=inference_batch_size)\n",
    "\n",
    "# Fetch data for plot \n",
    "latent_variables = []\n",
    "label_variables  = []\n",
    "batch_idx = 0\n",
    "for (data, labels) in tqdm( inference_train_loader, position=0 ):\n",
    "    batch_idx += 1\n",
    "    data = data.unsqueeze(0).to(device)\n",
    "    # Encode\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.view(-1,28*28) # reshape the img\n",
    "        encoded_data = encoder(data)\n",
    "    # Record codes\n",
    "    encoded_data = encoded_data.cpu().numpy()\n",
    "    labels = labels.numpy()\n",
    "    latent_variables.extend( encoded_data )\n",
    "    label_variables.extend( labels)\n",
    "#\n",
    "print(\"Reality check:\")\n",
    "print(latent_variables[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_array = np.array( latent_variables )\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter( code_array[:,0], code_array[:,1], c=label_variables, alpha=0.5, marker='o', edgecolor='none', cmap=discrete_cmap(k, 'jet'))\n",
    "plt.title( \"Latent space for train data in AE\")\n",
    "plt.colorbar(ticks=range(k))\n",
    "axes = plt.gca()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Computing latent variables for test dataset\" )\n",
    "\n",
    "inference_batch_size   = 1024\n",
    "inference_test_loader  = torch.utils.data.DataLoader(test_data , batch_size=inference_batch_size)\n",
    "\n",
    "# Fetch data for plot \n",
    "latent_variables = []\n",
    "label_variables  = []\n",
    "batch_idx = 0\n",
    "for (data, labels) in tqdm( inference_test_loader, position=0 ):\n",
    "    batch_idx += 1\n",
    "    data = data.unsqueeze(0).to(device)\n",
    "    # Encode\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.view(-1,28*28) # reshape the img\n",
    "        encoded_data = encoder(data)\n",
    "    # Record codes\n",
    "    encoded_data = encoded_data.cpu().numpy()\n",
    "    labels = labels.numpy()\n",
    "    latent_variables.extend( encoded_data )\n",
    "    label_variables.extend( labels)\n",
    "#\n",
    "print(\"Reality check:\")\n",
    "print(latent_variables[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_array = np.array( latent_variables )\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter( code_array[:,0], code_array[:,1], c=label_variables, alpha=0.5, marker='o', edgecolor='none', cmap=discrete_cmap(k, 'jet'))\n",
    "plt.title( \"Latent space for test data in AE\")\n",
    "plt.colorbar(ticks=range(k))\n",
    "axes = plt.gca()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Plotting with plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_samples     = []\n",
    "encoded_samples_raw = []\n",
    "#for sample in tqdm(train_dataset):\n",
    "for sample in tqdm(test_data):\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        img = img.view(-1,28*28) # reshape the img\n",
    "        encoded_img  = encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    encoded_sample['label'] = label\n",
    "    encoded_sample_raw = np.array( [encoded_img[0], encoded_img[1], label] )\n",
    "    encoded_samples.append(encoded_sample)\n",
    "    encoded_samples_raw.append( encoded_sample_raw )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "encoded_samples_df = pd.DataFrame(encoded_samples)\n",
    "encoded_samples_df\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.scatter(encoded_samples_df, x='Enc. Variable 0', y='Enc. Variable 1', \n",
    "           color=encoded_samples_df.label.astype(str), opacity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
