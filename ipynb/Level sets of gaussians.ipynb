{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Hyperpameters: set and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#experiment_name = \"vae_klw=1e-5\"\n",
    "#experiment_name = \"swissroll_curv_w=1e-3\"\n",
    "experiment_name = \"current_experiment\"\n",
    "#experiment_name = \"1Gaussians_in_D=3_lr=4e-5\"\n",
    "\n",
    "violent_saving = True # if False it will not save plots\n",
    "# here you can choose a path for saving the pictures \n",
    "Path_pictures = f\"/home/alazarev/CodeProjects/Experiments/{experiment_name}\"\n",
    "#os.mkdir(Path_pictures) # needs to be commented once the folder for plots is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for dataset\n",
    "set_name = \"Synthetic\"\n",
    "#where_to_compute_curv = \"random\"\n",
    "where_to_compute_curv = \"batch\"\n",
    "\n",
    "#set_name = \"Swissroll\"\n",
    "sr_noise = 0.05\n",
    "sr_numpoints = 18000 #k*n\n",
    "\n",
    "D = 784       #dimension\n",
    "#D = 3 # for swissroll\n",
    "d = 2         # latent space dimension\n",
    "k = 3         # num of 2d planes in dim D\n",
    "n = 6*(10**3) # num of points in each plane\n",
    "shift_class = 0\n",
    "var_class = 1 # variation of each Gaussian\n",
    "intercl_var = 0.1 # this creates a Gaussian, \n",
    "# i.e.random shift \n",
    "# proportional to the value of intercl_var\n",
    "# initially 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klw = 0 # AE mode on\n",
    "\n",
    "klw = 5e-4 #VAE mode on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for data loaders\n",
    "batch_size  = 32 # was 32 initially\n",
    "split_ratio = 0.2\n",
    "\n",
    "# Set manual seed for reproducibility\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_w = 1.0\n",
    "curv_w = 0.0 #weight on curvature\n",
    "compute_curvature = False\n",
    "\n",
    "### Define the loss function\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr         = 4e-5 #initially 4e-5 for synthetic\n",
    "momentum   = 0.8 #initially 0.8\n",
    "num_epochs = 10 #initially 40\n",
    "batches_per_plot = 600 #initially 200 \n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal imports\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# adding path to the set generating package\n",
    "import sys\n",
    "sys.path.append('../') # have to go 1 level up\n",
    "import ricci_regularization as RR\n",
    "#import torchvision\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I*. Choosing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_name == \"Synthetic\":\n",
    "    # Generate dataset \n",
    "\n",
    "    # old style\n",
    "    # train_dataset = ricci_regularization.generate_dataset(D, k, n, shift_class=shift_class, intercl_var=intercl_var)\n",
    "\n",
    "    # via classes\n",
    "    torch.manual_seed(0) # reproducibility\n",
    "    my_dataset = RR.SyntheticDataset(k=k,n=n,d=d,D=D,\n",
    "                                        shift_class=shift_class,\n",
    "                                        var_class = var_class, \n",
    "                                        intercl_var=intercl_var)\n",
    "\n",
    "    train_dataset = my_dataset.create\n",
    "elif set_name == \"Swissroll\":\n",
    "    train_dataset =  sklearn.datasets.make_swiss_roll(n_samples=sr_numpoints, noise=sr_noise)\n",
    "    sr_points = torch.from_numpy(train_dataset[0]).to(torch.float32)\n",
    "    #sr_points = torch.cat((sr_points,torch.zeros(sr_numpoints,D-3)),dim=1)\n",
    "    sr_colors = torch.from_numpy(train_dataset[1]).to(torch.float32)\n",
    "    from torch.utils.data import TensorDataset\n",
    "    train_dataset = TensorDataset(sr_points,sr_colors) \n",
    "\n",
    "m = len(train_dataset)\n",
    "train_data, test_data = torch.utils.data.random_split(train_dataset, [int(m-m*split_ratio), int(m*split_ratio)])\n",
    "test_loader  = torch.utils.data.DataLoader(test_data , batch_size=batch_size)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# test_data[:][0] will give the vectors of data without labels from the test part of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:][0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. AE declatation and initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "cuda_on = torch.cuda.is_available()\n",
    "if cuda_on:\n",
    "    device  = torch.device(\"cuda\") \n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sine AE\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 128)\n",
    "        self.linear4 = nn.Linear(128, hidden_dim)\n",
    "        #self.activation = nn.ReLU()\n",
    "        self.activation = torch.sin\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear3(y)\n",
    "        y = self.activation(y)\n",
    "        out = self.linear4(y)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 512)\n",
    "        self.linear4 = nn.Linear(512, output_dim)\n",
    "        self.activation = torch.sin\n",
    "        #self.activation = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear3(y)\n",
    "        y = self.activation(y)\n",
    "        out = self.linear4(y)\n",
    "        #out = self.activation(out)\n",
    "        #out = torch.sigmoid(y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# initial structure\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 128)\n",
    "        self.linear4 = nn.Linear(128, hidden_dim)\n",
    "        #self.activation = nn.ReLU()\n",
    "        self.activation = torch.sin\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear3(y)\n",
    "        y = self.activation(y)\n",
    "        out = self.linear4(y)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 512)\n",
    "        self.linear4 = nn.Linear(512, output_dim)\n",
    "        #self.activation = nn.ReLU()\n",
    "        self.activation = torch.sin\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.linear3(y)\n",
    "        y = self.activation(y)\n",
    "        out = self.linear4(y)\n",
    "        out = self.activation(out)\n",
    "        #out = torch.sigmoid(y)\n",
    "        return out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# linear map\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #self.activation = nn.ReLU()\n",
    "        self.activation = torch.sin\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        #out = self.activation(out)\n",
    "        return out\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, z):\n",
    "        z = self.linear1(z)\n",
    "        #z = torch.nn.functional.relu(self.linear1(z))\n",
    "        #z = torch.sigmoid(self.linear2(z))\n",
    "        #return z.reshape((-1, 1, 28, 28))\n",
    "        return z\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, cuda=True):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.linear2 = nn.Linear(512, hidden_dim)\n",
    "        self.linear3 = nn.Linear(512, hidden_dim)\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        if cuda:\n",
    "            self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "            self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = torch.sin(self.linear1(x)) \n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially D=784, d=2\n",
    "# AE/VAE switch\n",
    "if klw > 0:\n",
    "    encoder = VariationalEncoder(input_dim=784, hidden_dim=d, cuda=cuda_on)\n",
    "else:\n",
    "    encoder = Encoder(input_dim=D, hidden_dim=d)\n",
    "decoder = Decoder(hidden_dim=d, output_dim=D)\n",
    "\n",
    "# ClassicalAE\n",
    "#encoder = Encoder(input_dim=D, hidden_dim=d)\n",
    "#decoder = Decoder(hidden_dim=d, output_dim=D)\n",
    "\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.RMSprop(params_to_optimize, lr=lr, momentum=momentum, weight_decay=0.0)\n",
    "\n",
    "#optimizer = torch.optim.Adam(params_to_optimize, lr=lr,weight_decay=0.0)\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PATH_enc = '../nn_weights/encoder_sin_curv=10.pt'\n",
    "encoder.load_state_dict(torch.load(PATH_enc))\n",
    "encoder.eval()\n",
    "PATH_dec = '../nn_weights/decoder_sin_curv=10.pt'\n",
    "decoder.load_state_dict(torch.load(PATH_dec))\n",
    "decoder.eval()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choice of curvature functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Func(encoded_data):\n",
    "    metric_on_data = RR.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=decoder)\n",
    "    det_on_data = torch.det(metric_on_data)\n",
    "    Sc_on_data = RR.Sc_jacfwd_vmap(encoded_data,\n",
    "                                           function=decoder)\n",
    "    N = metric_on_data.shape[0]\n",
    "    Integral_of_Sc = (1/N)*(torch.sqrt(det_on_data)*torch.square(Sc_on_data)).sum()\n",
    "    return Integral_of_Sc\n",
    "\n",
    "\"\"\"\n",
    "# minimizing |g-I|_F\n",
    "def Func(encoded_data):\n",
    "    metric_on_data = RR.metric_jacfwd_vmap(encoded_data,\n",
    "                                           function=decoder)\n",
    "    N = metric_on_data.shape[0]\n",
    "    func = (1/N)*(metric_on_data-torch.eye(d)).norm(dim=(1,2)).sum()\n",
    "    return func\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Plotting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from https://gist.github.com/jakevdp/91077b0cae40f8f8244a\n",
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\"\"\"\n",
    "\n",
    "    # Note that if base_cmap is a string or None, you can simply do\n",
    "    return plt.cm.get_cmap(base_cmap, N)\n",
    "    # The following works for string, None, or a colormap instance:\n",
    "\"\"\"\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_plot(encoder, data, batch_idx, colormap = 'jet',draw_grid = True,figsize = (8, 6)):\n",
    "\n",
    "    labels = data[:][1]\n",
    "    data   = data[:][0]\n",
    "\n",
    "    # Encode\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.view(-1,D) # reshape the img\n",
    "        data = data.to(device)\n",
    "        encoded_data = encoder(data)\n",
    "\n",
    "    # Record codes\n",
    "    latent = encoded_data.cpu().numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    if set_name == \"Swissroll\":\n",
    "        plt.scatter( latent[:,0], latent[:,1], c=labels, alpha=0.5, marker='o', edgecolor='none', cmap=colormap)\n",
    "    else:\n",
    "        plt.scatter( latent[:,0], latent[:,1], c=labels, alpha=0.5, marker='o', edgecolor='none', cmap=discrete_cmap(k, colormap))\n",
    "        #plt.scatter( latent[:,0], latent[:,1], c=labels, alpha=0.5, marker='o')\n",
    "        plt.colorbar(ticks=range(k),orientation='vertical',shrink = 0.7)\n",
    "    \n",
    "    plt.title( f'''Latent space for test data in AE at batch {batch_idx}''')\n",
    "    axes = plt.gca()\n",
    "    plt.grid(draw_grid)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches per epoch\n",
    "print( \"Reality check of batch splitting: \")\n",
    "print( \"-- Batches per epoch\", len(train_loader) )\n",
    "print( \"batch size:\", batch_size )\n",
    "print( \"product: \", len(train_loader)*batch_size )\n",
    "print( \"-- To be compared to:\", (1.0-split_ratio)*n*k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches per epoch\n",
    "print( \"Reality check of batch splitting: \")\n",
    "print( \"-- Batches per epoch\", len(test_loader) )\n",
    "print( \"batch size:\", batch_size )\n",
    "print( \"product: \", len(test_loader)*batch_size )\n",
    "print( \"-- To be compared to:\", split_ratio*n*k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 0 # to enumerate the plots\n",
    "batch_idx = 0\n",
    "batches_per_epoch = len(train_loader)\n",
    "\n",
    "mse_loss = []\n",
    "kl_loss = []\n",
    "curv_loss = []\n",
    "test_mse_loss_list = []\n",
    "test_curv_loss_list = []\n",
    "\n",
    "# to iterate though the batches of test data \n",
    "# simoultanuousely with train data\n",
    "iter_test_loader = iter(test_loader)\n",
    "      \n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "   # Set train mode for both the encoder and the decoder\n",
    "   encoder.train()\n",
    "   decoder.train()\n",
    "   \n",
    "   \n",
    "   # Iterate the dataloader: no need  for the label\n",
    "   # values, this is unsupervised learning\n",
    "   for image_batch, _ in train_loader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "      #shaping the images properly\n",
    "      image_batch = image_batch.view(-1,D)\n",
    "      # Move tensor to the proper device\n",
    "      image_batch = image_batch.to(device)\n",
    "      # True batch size\n",
    "      true_batch_size = image_batch.shape[0]\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Front-propagation\n",
    "      # -- Encode data\n",
    "      encoded_data = encoder(image_batch)\n",
    "      # -- Decode data\n",
    "      decoded_data = decoder(encoded_data)\n",
    "      # --Evaluate loss\n",
    "      mse_loss_batch = torch.sum( (decoded_data-image_batch)**2 )/true_batch_size\n",
    "      \n",
    "      if compute_curvature == True:\n",
    "         if where_to_compute_curv == \"batch\":\n",
    "            curvature_train_batch = Func(encoded_data)\n",
    "         elif where_to_compute_curv == \"random\":\n",
    "            curvature_train_batch = Func(2*torch.rand(batch_size,2)-1)\n",
    "      else:\n",
    "         curvature_train_batch = 0.0\n",
    "      \n",
    "      loss = mse_w*mse_loss_batch + curv_w*curvature_train_batch\n",
    "      \n",
    "      # if VAE mode is on\n",
    "      if klw > 0:\n",
    "          kl_loss_batch = encoder.kl \n",
    "          loss += klw*kl_loss_batch\n",
    "          kl_loss.append(kl_loss_batch.data)\n",
    "      else:\n",
    "          kl_loss_batch = 0.0\n",
    "\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      # Print batch loss\n",
    "      #print('\\t MSE loss per batch (single batch): %f' % (mse_loss_batch.data))\n",
    "      #print('\\t Total loss per batch (single batch): %f' % (loss.data))\n",
    "\n",
    "      if batch_idx % len(test_loader):\n",
    "         iter_test_loader = iter(test_loader)\n",
    "      test_images = next(iter_test_loader)[0].view(-1,D).to(device)\n",
    "      encoded_test_data = encoder(test_images)\n",
    "      decoded_test_data = decoder(encoded_test_data)\n",
    "\n",
    "      # True test_batch size\n",
    "      true_test_batch_size = test_images.shape[0]\n",
    "      with torch.no_grad():\n",
    "         test_mse_loss = torch.sum( (decoded_test_data - test_images)**2 )/true_test_batch_size\n",
    "         test_mse_loss_list.append(test_mse_loss.detach().cpu().numpy())\n",
    "         if compute_curvature == True:\n",
    "            test_curv_loss = Func(encoded_test_data)\n",
    "            test_curv_loss_list.append(test_curv_loss.detach().cpu().numpy())\n",
    "         # end if\n",
    "      # end with\n",
    "      #print('\\t test MSE loss per batch (single batch): %f' % (test_mse_loss.data))\n",
    "      #print('\\t partial train loss (single batch): {:.6} \\t curv_loss {:.6} \\t mse {:.6}'.format(loss.data, new_loss, only_mse.data))\n",
    "      \n",
    "      mse_loss.append(float(mse_loss_batch.detach().cpu().numpy()))\n",
    "      if compute_curvature == True:\n",
    "         curv_loss.append(float(curvature_train_batch.detach().cpu().numpy()))\n",
    "\n",
    "      # Plot and compute test loss      \n",
    "      if (batch_idx % batches_per_plot == 0):\n",
    "         #test loss\n",
    "\n",
    "         #plotting\n",
    "         plot = point_plot(encoder, test_data, batch_idx)\n",
    "         if violent_saving == True:\n",
    "            plot.savefig('../plots/pointplots_in_training_testdata/pp{0}.eps'.format(num_plots),format='eps')\n",
    "         num_plots += 1\n",
    "         plot.show()\n",
    "\n",
    "         # plotting losses\n",
    "         if batch_idx>0:\n",
    "            fig, ax1 = plt.subplots()\n",
    "\n",
    "            ax1.set_xlabel('Batches')\n",
    "            ax1.set_ylabel('MSE')\n",
    "            ax1.semilogy(mse_loss, label='train_MSE_loss', color='tab:orange')\n",
    "            ax1.semilogy(test_mse_loss_list, label='test_MSE_loss', color='tab:red')\n",
    "            \n",
    "            ax1.tick_params(axis='y')\n",
    "            plt.legend(loc='lower left')\n",
    "\n",
    "            if compute_curvature == True:\n",
    "\n",
    "               ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "               ax2.set_ylabel('Curvature')  # we already handled the x-label with ax1\n",
    "               ax2.semilogy(curv_loss, label='train_Curv_loss',color='tab:olive')\n",
    "               ax2.semilogy(test_curv_loss_list, label='test_Curv_loss', color='tab:green')\n",
    "               \n",
    "               ax2.tick_params(axis='y')\n",
    "               plt.legend(loc='lower right')\n",
    "            # end if\n",
    "            fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "            plt.show()\n",
    "            # end if\n",
    "       # end if\n",
    "      batch_idx += 1\n",
    "   # end for\n",
    "   #print('\\n EPOCH {}/{}. \\t Average values over epoch:\\n MSE loss: {}, Curvature loss: {}'.format(epoch + 1, num_epochs, np.mean(mse_loss[-batches_per_epoch:]),np.mean(curv_loss[-batches_per_epoch:])))\n",
    "   print(f'\\n EPOCH {epoch + 1}/{num_epochs}. \\t Average values over epoch:\\n MSE loss: {np.mean(mse_loss[-batches_per_epoch:])}, Curvature loss: {np.mean(curv_loss[-batches_per_epoch:])}, KL loss: {np.mean(kl_loss[-batches_per_epoch:])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of MSE over the datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose data\n",
    "data_for_plot = test_data\n",
    "\n",
    "latent = encoder(data_for_plot[:][0].squeeze()).detach()\n",
    "labels = data_for_plot[:][1]\n",
    "init_data = data_for_plot[:][0]\n",
    "recon_data = decoder(encoder(init_data).detach())\n",
    "abs_error_array = (recon_data-init_data).squeeze()\n",
    "mse_array = abs_error_array.norm(dim=1)**2\n",
    "mse_array = mse_array.detach()\n",
    "\n",
    "plt.scatter( latent[:,0], latent[:,1], c=mse_array, alpha=0.5, marker='o', edgecolor='none', cmap='jet',norm=matplotlib.colors.LogNorm())\n",
    "plt.colorbar(label=\"squared l2 norm errors\")\n",
    "plt.title(\"Heatmap of the squared errors in l2 norm over the data\\n points encoded into the latent space in logscale\")\n",
    "if violent_saving == True:\n",
    "    plt.savefig(f'{Path_pictures}/logscale_MSE_heatmap.eps',bbox_inches='tight',format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative and absolute errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data = train_data[:][0]\n",
    "recon_data = decoder(encoder(init_data).detach())\n",
    "abs_error_array = (recon_data-init_data).squeeze()\n",
    "errors_l2 = abs_error_array.norm(dim=1)\n",
    "errors_l1 = (torch.abs(abs_error_array)).sum(dim=1)\n",
    "\n",
    "average_l1_error = errors_l1.mean()\n",
    "average_l2_error = errors_l2.mean()\n",
    "\n",
    "init_data_l1_norms = (torch.abs(init_data.squeeze())).sum(dim=1)\n",
    "init_data_l2_norms = (init_data.squeeze()).norm(dim=1)\n",
    "\n",
    "average_l1_norm = init_data_l1_norms.mean()\n",
    "average_l2_norm = init_data_l2_norms.mean()\n",
    "\n",
    "rel_l1_errors = errors_l1/init_data_l1_norms\n",
    "rel_l2_errors = errors_l2/init_data_l2_norms\n",
    "\n",
    "average_relative_error_l1 = rel_l1_errors.mean()\n",
    "average_relative_error_l2 = (rel_l2_errors.mean())\n",
    "\n",
    "\n",
    "print(f\"average l_1 relative error is: {average_relative_error_l1*100:.4f}%\")\n",
    "print(f\"average l_1 absolute error is: {average_l1_error}\")\n",
    "print(f\"average l_1 norm of initial {D} dimensional data is: {average_l1_norm}\")\n",
    "\n",
    "print(f\"average l_2 relative error is: {average_relative_error_l2*100:.4f}%\")\n",
    "print(f\"average l_2 absolute error is: {average_l2_error}\")\n",
    "print(f\"average l_2 norm of initial {D} dimensional data is: {average_l2_norm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Histogram of squared euclidean norms\\n of reconstruction errors over train data\")\n",
    "ax.hist((errors_l2**2).detach(),bins=round(math.sqrt(errors_l2.shape[0])))\n",
    "#fig.text(0.0,-0.10, f\"MSE:{(errors_l2**2).mean().item():.4f} \\nSet params: n={n}, k={k}, d={d}, D={D}, $\\sigma$={var_class}, $\\sigma_{{I}}$={intercl_var}.\")\n",
    "fig.text(0.0,-0.10, f\"MSE:{(errors_l2**2).mean().item():.4f}\")\n",
    "#plot = plt.show()\n",
    "if violent_saving == True:\n",
    "    fig.savefig(f'{Path_pictures}/reconstruction_errors.eps',bbox_inches='tight',format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Histogram of squared euclidean norms \\n of all points of the swiss roll\")\n",
    "plt.hist(train_data[:][0].norm(dim=1)**2,bins=round(math.sqrt(n)))\n",
    "plt.xlabel(\"squared l2 norm\")\n",
    "plt.ylabel(\"number of points\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 17}) # makes all fonts on the plot be 24\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Batches')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.semilogy(mse_loss, label='train_MSE_loss', color='tab:orange')\n",
    "ax1.semilogy(test_mse_loss_list, label='test_MSE_loss', color='tab:red')\n",
    "\n",
    "ax1.tick_params(axis='y')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "ax2.set_ylabel('Curvature')  # we already handled the x-label with ax1\n",
    "ax2.semilogy(curv_loss, label='train_Curv_loss',color='tab:olive')\n",
    "ax2.semilogy(test_curv_loss_list, label='test_Curv_loss', color='tab:green')\n",
    "\n",
    "ax2.tick_params(axis='y')\n",
    "plt.legend(loc='upper right')\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#fig.text(0.05,-0.25,f'The dataset has {k*n} points originated \\nby {k} Gaussian(s). \\nAverage losses over the last epoch: \\nMSE loss: {np.mean(mse_loss[-batches_per_epoch:]):.3f}, \\nCurvature loss: {np.mean(curv_loss[-batches_per_epoch:]):.3f}')\n",
    "if set_name == \"Swissroll\":\n",
    "    fig.text(0.05,-0.25,f\"Set params: n={sr_numpoints}, noise={sr_noise}. \\nAverage losses over the last epoch: \\nMSE loss: {np.mean(mse_loss[-batches_per_epoch:]):.3f}, \\nCurvature loss: {np.mean(curv_loss[-batches_per_epoch:]):.3f}\")\n",
    "else:    \n",
    "    fig.text(0.05,-0.25,f\"Set params: n={n}, k={k}, d={d}, D={D}, $\\sigma$={var_class}, $\\sigma_{{I}}$={intercl_var}. \\nAverage losses over the last epoch: \\nMSE loss: {np.mean(mse_loss[-batches_per_epoch:]):.3f}, \\nCurvature loss: {np.mean(curv_loss[-batches_per_epoch:]):.3f}\")\n",
    "str_lambda_recon = \"$\\lambda_{recon}$\"\n",
    "str_lambda_curv = \"$\\lambda_{curv}$\"\n",
    "plt.title(f\"Params: lr={lr}, batch_size={batch_size},\\n {str_lambda_recon}={mse_w}, {str_lambda_curv}={curv_w}\")\n",
    "#plt.title(\"Params: lr={0}, batch_size={1},\\n $\\lambda_r$={2}, $\\lambda_c$={3},{4}\".format(lr,batch_size,mse_w,curv_w,str1))\n",
    "if violent_saving == True:\n",
    "    plt.savefig(f'{Path_pictures}/losses.eps',bbox_inches='tight',format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = plt.scatter( latent[:,0], latent[:,1], c=labels, alpha=0.5, marker='o', edgecolor='none', cmap=discrete_cmap(k, 'jet'))\n",
    "#plot.colorbar(ticks=range(k))\n",
    "scale = 0.8\n",
    "plot = point_plot(encoder,test_data,batch_idx,colormap='jet',draw_grid=False,figsize = (8*scale, 6*scale))\n",
    "#plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams.update({'font.size': 20}) # makes all fonts on the plot be 24\n",
    "if set_name == \"Swissroll\":\n",
    "    plot.title( f'Latent space of the AE for the swiss roll')\n",
    "    #plot.text(-1.4,-1.4, f\"Set params: n={sr_numpoints}, noise={sr_noise}.\")\n",
    "else:    \n",
    "    plot.title( f'Latent space of the VAE for the\\n Synthetic dataset')\n",
    "    #plot.text(-1.4,-1.4, f\"Set params: n={n}, k={k}, d={d}, D={D}, $\\sigma$={var_class}, $\\sigma_{{I}}$={intercl_var}.\")\n",
    "if violent_saving == True:\n",
    "    plot.savefig(f'{Path_pictures}/VAE_latent_space.eps',bbox_inches='tight',format='eps')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Level sets of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rotation matrices \\phi_j and shifts y_j \n",
    "# from the set construction\n",
    "phi = my_dataset.rotations\n",
    "shifts = my_dataset.shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disc of circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "num_circles = 5 # circles per plane\n",
    "numpoints = 100 #points per circle\n",
    "maxrad = 3\n",
    "#radius_array = maxrad*np.sqrt(np.linspace(0,1,num_circles))\n",
    "radius_array = maxrad*np.linspace(0,1,num_circles)\n",
    "\n",
    "plt.title( \"Canonical version of disc\" )\n",
    "theta_array  = torch.linspace(0, 1-1/100, 100)\n",
    "x = torch.cos( 2*torch.pi*theta_array )\n",
    "y = torch.sin( 2*torch.pi*theta_array )\n",
    "for r in radius_array:\n",
    "    r = r.item() # extracting the value of r\n",
    "    plt.scatter( r*x, r*y, c=theta_array, marker='.', cmap='hsv', alpha=0.5*(2-r/maxrad) )\n",
    "# end for \n",
    "if violent_saving == True:\n",
    "    plt.savefig(f'{Path_pictures}/Canonical_disk.eps',bbox_inches='tight',format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk with colormap by polar angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plane_idx = 0\n",
    "#num_circles = 40 # circles per plane\n",
    "#numpoints = 100 #points per circle\n",
    "#maxrad = 3\n",
    "#radius_array = maxrad*np.sqrt(np.linspace(0,1,num_circles))\n",
    "\n",
    "for plane_idx in range(k):\n",
    "    plt.title( f'''Embedding of {num_circles} circles of radius up to {maxrad} in \\n the latent space in plane # {plane_idx} with {str_lambda_curv}={curv_w}''')\n",
    "    for rad in radius_array:\n",
    "        theta = torch.linspace(0.,2*torch.pi*(numpoints-1)/numpoints,\n",
    "                           numpoints)\n",
    "        #print(\"theta:\",theta)\n",
    "        x_array = rad*torch.cos(theta).unsqueeze(0).T\n",
    "        #print(\"x:\",x_array)\n",
    "        y_array = rad*torch.sin(theta).unsqueeze(0).T\n",
    "        #print(\"y\",y_array)\n",
    "        circle = torch.cat((x_array,y_array),dim=-1)\n",
    "        #circle = design_circle(100, rad=(j+1)*2)\n",
    "        #plot.scatter(x_array,y_array)\n",
    "        circle_in_D = torch.matmul(phi[plane_idx],circle.T).T+shifts[plane_idx].squeeze()\n",
    "        #plt.scatter(circle_in_D[:,0],circle_in_D[:,1])\n",
    "        encoded_circle = encoder(circle_in_D).detach()\n",
    "        #plt.scatter(encoded_circle[:,0],encoded_circle[:,1])\n",
    "        plt.scatter(encoded_circle[:,0],encoded_circle[:,1],c=theta, marker='.', cmap='hsv', alpha=0.5*(2-rad/maxrad))\n",
    "        plt.grid(True)\n",
    "        \n",
    "        #print(rad)\n",
    "    ax = plt.gca()\n",
    "    # to make equal axis scales\n",
    "    #ax.set_aspect('equal') \n",
    "    #plt.figure(figsize=(8, 8))\n",
    "    if violent_saving == True:\n",
    "        plt.savefig(f'{Path_pictures}/circle_in_plane#{plane_idx}_byangle.eps',bbox_inches='tight',format='eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plane_idx = 0\n",
    "num_circles = 5 # circles per plane\n",
    "numpoints = 100 #points per circle\n",
    "maxrad = 3\n",
    "\n",
    "radius_array = maxrad*np.sqrt(np.linspace(0,1,num_circles))\n",
    "\n",
    "for plane_idx in range(k):\n",
    "    plt.title( f'''Embedding of {num_circles} circles of radius up to {maxrad} in \\n the latent space in plane # {plane_idx} with $\\lambda_{{curv}}=${curv_w}''')\n",
    "    for rad in radius_array:\n",
    "        theta = torch.linspace(0.,2*torch.pi*(numpoints-1)/numpoints,\n",
    "                           numpoints)\n",
    "        x_array = rad*torch.cos(theta).unsqueeze(0).T\n",
    "        y_array = rad*torch.sin(theta).unsqueeze(0).T\n",
    "        circle = torch.cat((x_array,y_array),dim=-1)\n",
    "        circle_in_D = torch.matmul(phi[plane_idx],circle.T).T+shifts[plane_idx].squeeze()\n",
    "        encoded_circle = encoder(circle_in_D).detach()\n",
    "        plt.scatter(encoded_circle[:,0],encoded_circle[:,1],s=15)\n",
    "        plt.grid(True)\n",
    "    #ax = plt.gca()\n",
    "    # to make equal axis scales\n",
    "    #ax.set_aspect('equal') \n",
    "    #plt.figure(figsize=(8, 8))\n",
    "    if violent_saving == True:\n",
    "        plt.savefig(f'{Path_pictures}/circle_in_plane#{plane_idx}_bycolor.eps',bbox_inches='tight',format='eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plane_idx = 0\n",
    "num_circles = 5 # circles per plane\n",
    "numpoints = 100 #points per circle\n",
    "maxrad = 3*math.sqrt(var_class)\n",
    "\n",
    "radius_array = maxrad*np.sqrt(np.linspace(0,1,num_circles))\n",
    "\n",
    "for plane_idx in range(k):\n",
    "    #plt.title( f'''Embedding of {num_circles} circles of radius up to {maxrad} in each \\n of the {k} planes in the latent space with \\n penalty on frobenius norm of $G-I$ equal to  {curv_w}''')\n",
    "    plt.title( f'''Embedding of {num_circles} circles of radius up to {maxrad} in each \\n of the {k} planes in the latent space''')\n",
    "    for rad in radius_array:\n",
    "        theta = torch.linspace(0.,2*torch.pi*(numpoints-1)/numpoints,\n",
    "                           numpoints)\n",
    "        x_array = rad*torch.cos(theta).unsqueeze(0).T\n",
    "        y_array = rad*torch.sin(theta).unsqueeze(0).T\n",
    "        circle = torch.cat((x_array,y_array),dim=-1)\n",
    "        circle_in_D = torch.matmul(phi[plane_idx],circle.T).T+shifts[plane_idx].squeeze()\n",
    "        encoded_circle = encoder(circle_in_D).detach()\n",
    "        \n",
    "        # color by rad\n",
    "        plt.scatter(encoded_circle[:,0],encoded_circle[:,1],marker='.',cmap='jet',s=20)\n",
    "        #print(3*rad*np.ones(numpoints))\n",
    "        # colorby polar angle\n",
    "        #plt.scatter(encoded_circle[:,0],encoded_circle[:,1],c=theta, marker='.', cmap='hsv', alpha=1-rad/maxrad)\n",
    "        plt.grid(True)\n",
    "    #ax = plt.gca()\n",
    "    # to make equal axis scales\n",
    "    #ax.set_aspect('equal') \n",
    "    #plt.figure(figsize=(8, 8))\n",
    "if violent_saving == True:\n",
    "    plt.savefig(f'{Path_pictures}/circles_color_by_radius.png',bbox_inches='tight',format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plane_idx in range(k):\n",
    "    #plt.title( f'''Embedding of {num_circles} circles of radius up to {maxrad} in each \\n of the {k} planes in the latent space with \\n penalty on frobenius norm of $G-I$ equal to  {curv_w}''')\n",
    "    plt.title( f'''Embedding of {num_circles} circles of radius up to {maxrad} in each \\n of the {k} planes in the latent space''')\n",
    "    for rad in radius_array:\n",
    "        theta = torch.linspace(0.,2*torch.pi*(numpoints-1)/numpoints,\n",
    "                           numpoints)\n",
    "        x_array = rad*torch.cos(theta).unsqueeze(0).T\n",
    "        y_array = rad*torch.sin(theta).unsqueeze(0).T\n",
    "        circle = torch.cat((x_array,y_array),dim=-1)\n",
    "        circle_in_D = torch.matmul(phi[plane_idx],circle.T).T+shifts[plane_idx].squeeze()\n",
    "        encoded_circle = encoder(circle_in_D).detach()\n",
    "        \n",
    "        # color by rad\n",
    "        #plt.scatter(encoded_circle[:,0],encoded_circle[:,1])\n",
    "        # colorby polar angle\n",
    "        plt.scatter(encoded_circle[:,0],encoded_circle[:,1],c=theta, marker='.', cmap='hsv', alpha=1-rad/maxrad)\n",
    "        plt.grid(True)\n",
    "    #ax = plt.gca()\n",
    "    # to make equal axis scales\n",
    "    #ax.set_aspect('equal') \n",
    "    #plt.figure(figsize=(8, 8))\n",
    "if violent_saving == True:\n",
    "    plt.savefig(f'{Path_pictures}/circles_color_by_angle.eps',bbox_inches='tight',format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "torch.manual_seed(0)\n",
    "samples_over_latent_space = torch.rand(N,2)-1.0\n",
    "\n",
    "#visualize samples\n",
    "plot.scatter(samples_over_latent_space[:,0],samples_over_latent_space[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_at_samples = RR.metric_jacfwd_vmap(samples_over_latent_space,function=decoder).detach()\n",
    "eigenvalues = torch.linalg.eigvalsh(metric_at_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_eigenvalues = eigenvalues[:,0]\n",
    "max_eigenvalues = eigenvalues[:,1]\n",
    "all_eigenvalues = torch.cat((min_eigenvalues,max_eigenvalues),dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.title(f\"Histogram of eigenvalues of metric $G$ evaluated \\n at {N} samples uniformly distributed \\n over the latent space with penalty \\n {curv_w} on Frobenius norm of $G-I$\")\n",
    "plt.hist(all_eigenvalues, bins=round(math.sqrt(N)))\n",
    "plt.xlabel(\"All eigenvalues of metric\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.title(f\"Histogram of minimal eigenvalues of metric $G$ evaluated at {N} \\n samples uniformly distributed over the latent space \")\n",
    "plt.hist(min_eigenvalues, bins=round(math.sqrt(N)))\n",
    "plt.xlabel(\"Min eigenvalues of metric\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(max_eigenvalues, bins=round(math.sqrt(N)))\n",
    "plt.title(f\"Histogram of maximal eigenvalues of metric $G$ evaluated at {N} \\n samples uniformly distributed over the latent space \")\n",
    "plt.xlabel(\"Max eigenvalues of metric\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_over_latent_space = (encoder(train_data[:][0]).detach()).squeeze()\n",
    "samples_over_latent_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps of det(G) and tr(G) and histograms of errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# on grid\n",
    "grid_on_ls = RR.make_grid(100,xsize=2,ysize=2,xcenter=-0.0,ycenter=0.0)\n",
    "#samples_over_latent_space = grid_on_ls\n",
    "\n",
    "# random\n",
    "# samples_over_latent_space = 2*torch.rand(N,2)-1.0\n",
    "\n",
    "# on train_data\n",
    "\n",
    "samples_over_latent_space = (encoder(train_data[:][0]).detach()).squeeze()\n",
    "\n",
    "#visualize samples\n",
    "plt.title(\"Visualized samples\")\n",
    "plt.scatter(samples_over_latent_space[:,0],samples_over_latent_space[:,1],marker=\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RR.metric_jacfwd(torch.tensor([-0.2,0.0]), function=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_at_samples = RR.metric_jacfwd_vmap(samples_over_latent_space,function=decoder).detach()\n",
    "eigenvalues = torch.linalg.eigvalsh(metric_at_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_eigenvalues = eigenvalues[:,0]\n",
    "max_eigenvalues = eigenvalues[:,1]\n",
    "all_eigenvalues = torch.cat((min_eigenvalues,max_eigenvalues),dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_trace = min_eigenvalues+max_eigenvalues\n",
    "metric_det = min_eigenvalues*max_eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.hist(np.array(metric_det),bins=round(math.sqrt(metric_det.shape[0])))\n",
    "plt.title(\"Metric determinants histogram over train data points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.hist(np.array(metric_trace),bins=round(math.sqrt(metric_trace.shape[0])))\n",
    "plt.title(\"Metric traces histogram over train data points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.func as TF\n",
    "grid_on_ls = RR.make_grid(100,xsize=2,ysize=2,xcenter=-0.0,ycenter=0.0)\n",
    "metric_on_grid = RR.metric_jacfwd_vmap(grid_on_ls,function=decoder)\n",
    "metric_det_on_grid = torch.det(metric_on_grid)\n",
    "metric_trace_on_grid = TF.vmap(torch.trace)(metric_on_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR.draw_scalar_on_grid(metric_det_on_grid.view(100,100),plot_name=f\"det(G) with curv_w={curv_w}\",numsteps=100,xsize=2,ysize=2,xcenter=-0.0,ycenter=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR.draw_scalar_on_grid(metric_trace_on_grid.view(100,100),plot_name=f\"tr(G) curv_w={curv_w}\",numsteps=100,xsize=2,ysize=2,xcenter=-0.0,ycenter=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = train_data[:][0]\n",
    "#init_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = (decoder(encoder(init_points)) - init_points).norm(dim=(1,2))**2\n",
    "errors = errors/len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(errors.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.hist(np.array(errors.detach()),bins=round(math.sqrt(errors.shape[0])))\n",
    "plt.title(\"Reconstruction errors histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PATH_enc = '../nn_weights/encoder_synthetic_lr=6e-5.pt'\n",
    "torch.save(encoder.state_dict(), PATH_enc)\n",
    "PATH_dec = '../nn_weights/decoder_synthetic_lr=6e-5.pt'\n",
    "torch.save(decoder.state_dict(), PATH_dec)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
