{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, yaml\n",
    "import ricci_regularization\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_experiment = '../../experiments/MNIST_Setting_3_config.yaml'\n",
    "with open(Path_experiment, 'r') as yaml_file:\n",
    "#with open('../experiments/Synthetic_Setting_1/Synthetic_Setting_1_config.yaml', 'r') as yaml_file:\n",
    "#with open('../experiments/Swissroll_exp5_config.yaml', 'r') as yaml_file:\n",
    "    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "# Load data loaders based on YAML configuration\n",
    "# Load data loaders based on YAML configuration\n",
    "dict = ricci_regularization.DataLoaders.get_dataloaders(\n",
    "    dataset_config=yaml_config[\"dataset\"],\n",
    "    data_loader_config=yaml_config[\"data_loader_settings\"],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "print(\"Experiment results loaded successfully.\")\n",
    "train_loader = dict[\"train_loader\"]\n",
    "test_loader = dict[\"test_loader\"]\n",
    "test_dataset = dict.get(\"test_dataset\")  # Assuming 'test_dataset' is a key returned by get_dataloaders\n",
    "\n",
    "print(\"Data loaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torus_ae, Path_ae_weights = ricci_regularization.DataLoaders.get_tuned_nn(config=yaml_config,additional_path='../')\n",
    "\n",
    "print(\"AE weights loaded successfully.\")\n",
    "print(\"AE weights loaded from\", Path_ae_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to store the PNG images\n",
    "shots_folder_name = \"/generated_pics\"\n",
    "shots_folder_adress = '../../experiments/'+yaml_config[\"experiment\"][\"name\"]+ shots_folder_name\n",
    "if not os.path.exists(shots_folder_adress):\n",
    "    os.makedirs(shots_folder_adress)\n",
    "    print(\"A folder created for saved images to create a gif at:\", shots_folder_adress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "K = 2 # number of clusters\n",
    "data = test_dataset.data\n",
    "N = 7 #len(data)\n",
    "m = 10 # intermediate points on every geodesic\n",
    "D = yaml_config[\"architecture\"][\"input_dim\"]\n",
    "d = yaml_config[\"architecture\"][\"latent_dim\"]\n",
    "\n",
    "# Limit dataset to the first n samples\n",
    "subset_indices = list(range(N))\n",
    "mnist_subset = torch.utils.data.Subset(data, subset_indices)\n",
    "dataloader = torch.utils.data.DataLoader(mnist_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# encoding into latent space\n",
    "torus_ae.cpu()\n",
    "torus_ae.eval()\n",
    "\n",
    "# Encode samples into latent space\n",
    "encoded_points = []\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    for images in dataloader:\n",
    "#        print(images.shape)\n",
    "        latent = torus_ae.encoder2lifting( (images.reshape(-1, D)).to(torch.float32) )  # Pass images through the encoder\n",
    "        encoded_points.append(latent)\n",
    "encoded_points = torch.cat(encoded_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing initialization\n",
    "centers = ricci_regularization.initialize_centers(encoded_points, K, N)\n",
    "\n",
    "plt.scatter(encoded_points[:,0],encoded_points[:,1], label = \"encoded data\")\n",
    "plt.scatter(centers[:,0], centers[:,1], c=\"red\", label = \"centers\", marker='*', s = 60)\n",
    "plt.xlim(-torch.pi, torch.pi)\n",
    "plt.ylim(-torch.pi, torch.pi)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = ricci_regularization.connect_centers2encoded_data_with_segments(encoded_points, centers, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_regularization.plot_octopus(segments, saving_adress=shots_folder_adress, iter=0,silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function and optimizer parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energy(points_on_geodesics, decoder=torus_ae.decoder_torus, num_data_points = N, num_classes = K, num_aux_points = m, latent_dim = d):\n",
    "    #assert points_on_geodesics.shape == torch.Size([num_data_points, num_classes, num_aux_points, latent_dim])\n",
    "    decoded_points = decoder(points_on_geodesics)\n",
    "    computed_energy = (( decoded_points[:,:,1:,:] - decoded_points[:,:,:-1,:] ) ** 2 ).sum() # comute sum of Euclidean energies of all the curves in R^D\n",
    "    # make sure that optimization is parallelized\n",
    "    # Warning! the outpiut is the single scalar, i.e the sum of all the energies\n",
    "    return computed_energy\n",
    "\n",
    "def compute_lengths(points_on_geodesics, decoder=torus_ae.decoder_torus, num_data_points = N, num_classes = K, num_aux_points = m, latent_dim = d):\n",
    "    #assert segments.shape == torch.Size([num_data_points, num_classes, num_aux_points, latent_dim])\n",
    "    if points_on_geodesics.shape != torch.Size([num_data_points, num_classes, num_aux_points, latent_dim]):\n",
    "        points_on_geodesics = points_on_geodesics.unsqueeze(0)\n",
    "    decoded_points = decoder(points_on_geodesics)\n",
    "    tangent_vectors = decoded_points[:,:,1:,:] - decoded_points[:,:,:-1,:]\n",
    "    computed_lengths = torch.sqrt((tangent_vectors**2).sum(dim=(-2,-1))) # comute Euclidean compute_lengths of the curves in R^D\n",
    "    return computed_lengths\n",
    "loss_geodesics = compute_energy(points_on_geodesics=segments, decoder=torus_ae.decoder_torus)\n",
    "loss_geodesics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5e-3\n",
    "num_iter = 1500\n",
    "# Define parameters (for example, weights to optimize)\n",
    "segments = ricci_regularization.connect_centers2encoded_data_with_segments(encoded_points, centers, m) #initialize the segents between centers and data points\n",
    "init_segments = torch.clone(segments)\n",
    "segments = torch.nn.Parameter(segments) # Wrap as a parameter\n",
    "\n",
    "optimizer = torch.optim.SGD([segments], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner loop (refining geodesics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for iter_num in range(num_iter):\n",
    "    \n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "    # Compute the loss\n",
    "    loss_geodesics = compute_energy(points_on_geodesics=segments, decoder=torus_ae.decoder_torus)\n",
    "\n",
    "    # Backpropagation: compute gradients\n",
    "    loss_geodesics.backward()\n",
    "\n",
    "    # Zero out gradients for the first and last points (don't want them updated)\n",
    "    segments.grad[:, :, 0, :] = 0.  # First points along 'geodesics' (data_point)\n",
    "    segments.grad[:, :, -1, :] = 0.  # Last points along 'geodesics' (center)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store the loss value\n",
    "    loss_history.append(loss_geodesics.item())\n",
    "    print(f\"Iteration #{iter_num + 1}, loss: {loss_geodesics.item():.3f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_regularization.plot_octopus(segments.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the first and the last points did not move\n",
    "assert torch.equal( init_segments[:,:,0,:], segments[:,:,0,:])\n",
    "assert torch.equal( init_segments[:,:,-1,:], segments[:,:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer loop: Frechet mean update + membership update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1e-2 # Frechet mean learning rate\n",
    "lengths_of_geod = compute_lengths(segments, torus_ae.decoder_torus) # comute a vector of length of all geodesics shape (N,K)\n",
    "memberships = torch.argmin(lengths_of_geod, dim=1) # retrieve the class membership of each point by finding the closest cluster center shape (N)\n",
    "\n",
    "#batch_indices = torch.arange(N)\n",
    "#meaningful_geodesics = segments[batch_indices, memberships, :, :] # pick only geodesics connecting points to cluster centers where the points are assigned shape (N,m,d)\n",
    "meaningful_geodesics = segments[:, memberships, :, :] # pick only geodesics connecting points to cluster centers where the points are assigned shape (N,m,d)\n",
    "v = meaningful_geodesics[:,-1,:] - meaningful_geodesics[:,-2,:] #!!! think of weighted average between the last vector and some previous ones\n",
    "# Renormalization is numerically unstable. \n",
    "v = v / v.norm(dim=1).unsqueeze(-1) # find the last segments of the geod shape (N,d)\n",
    "# numerical stability?\n",
    "\n",
    "# Redo without the loop and comment all operations\n",
    "# update cluster centers with weight beta\n",
    "for i in range(K):\n",
    "    cluster_mask = memberships == i \n",
    "    v_i = v[cluster_mask] \n",
    "    l_i = lengths_of_geod[cluster_mask][:,0]\n",
    "    with torch.no_grad():\n",
    "        FM_gradient = torch.sum( l_i.unsqueeze(-1) * v_i, dim=0 ) # output is d dimensional vector\n",
    "        #centers[i] = centers[i] - beta * FM_gradient\n",
    "        segments[:, i, -1, :] += - beta * FM_gradient # update i-th cluster center ( only moving the very last point on a geodesic)\n",
    "        print(f\"\\nNorm of gradient of FM lossfor {i}-th cluster\", FM_gradient.norm().item())\n",
    "#centers = centers.detach()\n",
    "#print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_i.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"segments shape:\", segments.shape)\n",
    "print(\"memberships shape:\", memberships.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print \n",
    "print(\"lengths of geodesics\", compute_lengths(segments) )\n",
    "print(\"memberships\", memberships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_regularization.plot_octopus(segments.detach(),memberships,meaningful_geodesics.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version -1 without local charts update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5e-3 # Frechet mean learning rate\n",
    "learning_rate = 1e-3\n",
    "num_iter_outer = 200\n",
    "num_geod_iter = 10 # number of geodesics refinement interations per 1 FM update\n",
    "memberships = None\n",
    "meaningful_geodesics = None\n",
    "loss_history = []\n",
    "meaningful_geodesics_loss_history = []\n",
    "norm_FM_grad_history = []\n",
    "\n",
    "# Initialize geodesic segments\n",
    "centers = ricci_regularization.initialize_centers(encoded_points, K, N)\n",
    "segments = ricci_regularization.connect_centers2encoded_data_with_segments(encoded_points, centers, m) #initialize the segents between centers and data points\n",
    "init_segments = torch.clone(segments) # save initial segments\n",
    "# Set optimizer params:\n",
    "segments = torch.nn.Parameter(segments) # Wrap as a parameter\n",
    "\n",
    "optimizer = torch.optim.SGD([segments], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_regularization.plot_octopus(segments.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stopping criterium e.g. delta of energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer loop \n",
    "for iter_outer in range(num_iter_outer):\n",
    "    # Inner loop (refining geodesics)\n",
    "    for iter_inner in range(num_geod_iter):\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "        # Compute the loss\n",
    "        loss_geodesics = compute_energy(points_on_geodesics=segments, decoder=torus_ae.decoder_torus)\n",
    "\n",
    "        # Backpropagation: compute gradients\n",
    "        loss_geodesics.backward()\n",
    "\n",
    "        # Zero out gradients for the first and last points (don't want them updated)\n",
    "        segments.grad[:, :, 0, :] = 0.  # First points along 'geodesics'\n",
    "        segments.grad[:, :, -1, :] = 0.  # Last points along 'geodesics'\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss value\n",
    "        loss_history.append(loss_geodesics.item())\n",
    "        # saving plots\n",
    "        #ricci_regularization.plot_octopus(segments,memberships=memberships,meaningful_geodesics=meaningful_geodesics, \n",
    "        #             saving_adress=shots_folder_adress, iter=iter_outer*num_geod_iter + iter_inner,silent=True)\n",
    "        #print(f\"Iteration #{iter_inner + 1}, loss: {loss_geodesics.item():.3f}\")    \n",
    "    lengths_of_geod = compute_lengths(segments, torus_ae.decoder_torus) # comute a vector of length of all geodesics shape (N,K)\n",
    "    memberships = torch.argmin(lengths_of_geod, dim=1) # retrieve the class membership of each point by finding the closest cluster center shape (N)\n",
    "\n",
    "    batch_indices = torch.arange(N)\n",
    "    meaningful_geodesics = segments[batch_indices, memberships, :, :].detach() # pick only geodesics connecting points to cluster centers where the points are assigned shape (N,m,d)\n",
    "    \n",
    "    # saving the lengths of meaningful geodesics\n",
    "    meaningful_geodesics_lengths = compute_lengths(meaningful_geodesics)\n",
    "    meaningful_geodesics_loss_history.append( meaningful_geodesics_lengths.detach().mean().item() )\n",
    "\n",
    "    v = meaningful_geodesics[:,-1,:] - meaningful_geodesics[:,-2,:]\n",
    "    v = v / v.norm(dim=1).unsqueeze(-1) # find the last segments of the geod shape (N,d)\n",
    "    # update cluster centers with weight beta\n",
    "    for i in range(K):\n",
    "        average_FM_grad_norm = 0.\n",
    "        cluster_mask = memberships == i \n",
    "        v_i = v[cluster_mask] \n",
    "        l_i = lengths_of_geod[cluster_mask][:,0]\n",
    "        with torch.no_grad():\n",
    "            FM_gradient = torch.sum( l_i.unsqueeze(-1) * v_i, dim=0 )\n",
    "            segments[:, i, -1, :] += - beta * FM_gradient # update i-th cluster center ( only moving the very last point on a geodesic)\n",
    "            #print(f\"\\nNorm of gradient of FM lossfor {i}-th cluster\", FM_gradient.norm().item())\n",
    "        average_FM_grad_norm += FM_gradient.norm().item()/K\n",
    "        # !save all of cluster FM_grad seperately\n",
    "    norm_FM_grad_history.append(average_FM_grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming norm_FM_grad_history, meaningful_geodesics_loss_history, loss_history are arrays or tensors\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure with 1 row and 3 columns\n",
    "\n",
    "# Plot norm_FM_grad_history\n",
    "axes[0].plot(norm_FM_grad_history, marker='o', label='FM Grad History')\n",
    "axes[0].set_title('Averege shift of centers (Fréchet mean gradient norm)')\n",
    "axes[0].set_xlabel('Outer loop iterations')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot meaningful_geodesics_loss_history\n",
    "axes[1].plot(meaningful_geodesics_loss_history, marker='o', label='Geodesics Loss History', color='orange')\n",
    "axes[1].set_title('Meaningfull geodesics length')\n",
    "axes[1].set_xlabel('Outer loop iterations')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot loss_history\n",
    "axes[2].plot(loss_history, label='All geodesics length', color='green')\n",
    "axes[2].set_title('All geodesics length')\n",
    "axes[2].set_xlabel(f'All iterations: {num_geod_iter} inner  per outer loop iter')\n",
    "axes[2].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_regularization.plot_octopus(segments, memberships=memberships,meaningful_geodesics=meaningful_geodesics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GIF from the PNG images\n",
    "png_files = sorted([f for f in os.listdir(shots_folder_adress) if f.endswith('.png')])\n",
    "images = []\n",
    "for file in png_files:\n",
    "    image_path = os.path.join(shots_folder_adress, file)\n",
    "    images.append(imageio.imread(image_path))  # Read each PNG image\n",
    "\n",
    "# Create the GIF\n",
    "output_gif = \"output_animation.gif\"\n",
    "imageio.mimsave(shots_folder_adress + '/' + output_gif, images, duration=0.001)  # Adjust the duration for frame speed\n",
    "\n",
    "print(f\"GIF created and saved as {output_gif}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing PNGs and the output video name\n",
    "images_folder = shots_folder_adress\n",
    "output_video = \"output_video.avi\"\n",
    "\n",
    "# Set video parameters\n",
    "frame_rate = 30\n",
    "images = sorted([img for img in os.listdir(images_folder) if img.endswith(\".png\")])\n",
    "if not images:\n",
    "    raise ValueError(\"No PNG images found in the specified directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the first image to get dimensions\n",
    "first_image_path = os.path.join(images_folder, images[0])\n",
    "frame = cv2.imread(first_image_path)\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # You can use other codecs like 'mp4v'\n",
    "video = cv2.VideoWriter(output_video, fourcc, frame_rate, (width, height))\n",
    "\n",
    "for image in images:\n",
    "    img_path = os.path.join(images_folder, image)\n",
    "    frame = cv2.imread(img_path)\n",
    "    video.write(frame)\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Video saved as {output_video}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_cluster_center_shift_norm = (segments[0,:,-1,:] - init_segments[0,:,-1,:]).detach().norm(dim = 1).mean()\n",
    "print(\"Averega center's shift:\", average_cluster_center_shift_norm.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recentering local charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers = torch.tensor([[ 2., -2.5]])\n",
    "#        [-2.5458,  2.2106],\n",
    "#        [-1.0967,  2.2219]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_segments = ricci_regularization.connect_centers2encoded_data_with_segments(encoded_points, centers=new_centers, num_aux_points=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_regularization.plot_octopus(segments=new_segments)#, xlim=None, ylim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_updated_segments(segments):\n",
    "    # recognize the shape of segments:\n",
    "    N = segments.shape[0] # num data points\n",
    "    K = segments.shape[1] # num clusters\n",
    "    m = segments.shape[2] # num auxilliary points\n",
    "    d = segments.shape[3] # latent dimension\n",
    "    updated_segments = segments.clone()\n",
    "    # adapting segments to their local charts\n",
    "    for i in range(N): # this is very bad! REDO with a mask\n",
    "        for j in range(K):\n",
    "            for dim in range(d):\n",
    "                if torch.abs( segments[i,j,-1,dim] - segments[i,j,0,dim] ) > torch.pi:\n",
    "                    # choose direction of the point shift\n",
    "                    sign = torch.sgn( segments[i,j,-1,dim] - segments[i,j,0,dim] )\n",
    "                    shift = sign * 2 * torch.pi\n",
    "                    # shift the point \n",
    "                    updated_segments[i,j,0,dim] += shift\n",
    "    # Generate interpolation parameters (m values between 0 and 1)\n",
    "    t = torch.linspace(0, 1, steps=m).to(encoded_points.device).view(1, 1, m, 1)  # Shape: (1, 1, m, 1)\n",
    "\n",
    "    new_centers = segments[:,:,-1,:]\n",
    "    # Reshape encoded_points and centers for broadcasting\n",
    "    new_start_points = updated_segments[:,0,0,:].unsqueeze(1).unsqueeze(2)   # Shape: (n, 1, 1, d)\n",
    "    centers_expanded = new_centers.unsqueeze(2)        # Shape: (1, k, 1, d)\n",
    "\n",
    "    # Compute all intermediate points using linear interpolation\n",
    "    updated_segments = new_start_points + t * (centers_expanded - new_start_points)  # Shape: (n, k, m, d)\n",
    "    return updated_segments\n",
    "def mod_pi(segments): # only for plotting, local chart quiting has to be fixed\n",
    "    # Returns the coordinates of points in the initial local chart\n",
    "    return torch.remainder(segments + torch.pi, 2*torch.pi) - torch.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_segments = compute_updated_segments(new_segments).detach()\n",
    "#updated_segments_mod_pi = mod_pi(updated_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(new_segments[:,:,0,0], new_segments[:,:,0,1], c = 'green',zorder = 10, label = \"before shift\")\n",
    "plt.scatter(updated_segments[:,:,0,0], updated_segments[:,:,0,1], c = 'magenta', s = 100, label = \"after shift\")\n",
    "# add an arrow from not upd to upd segments\n",
    "plt.xlim(-2*torch.pi, 2*torch.pi)\n",
    "plt.ylim(-2*torch.pi, 2*torch.pi)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_regularization.plot_octopus(new_segments, xlim=2*torch.pi,ylim=2*torch.pi)\n",
    "# add the grid plotting option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_regularization.plot_octopus(updated_segments,xlim=2*torch.pi, ylim=2*torch.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_energy(new_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_energy(updated_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ricci_regularization.plot_octopus(updated_segments_mod_pi,xlim= torch.pi, ylim=torch.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_energy(updated_segments_mod_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_array = [ -2 * torch.pi, 0., 2 * torch.pi]\n",
    "segments_array = []\n",
    "for shift_x in shift_array:\n",
    "    for shift_y in shift_array:\n",
    "        segments_array.append(updated_segments + shift_x * torch.tensor([1.,0.]) + shift_y * torch.tensor([0.,1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segments in segments_array:\n",
    "    for i in range(N):\n",
    "            for j in range(K):\n",
    "                #if j == 0:\n",
    "                #    color = \"blue\"\n",
    "                #else:\n",
    "                color = \"orange\"\n",
    "                plt.plot(segments[i,j,:,0], segments[i,j,:,1],'-',marker='o', c = color, markersize=3)\n",
    "        # plot centers\n",
    "    centers = segments[0,:,-1,:]\n",
    "    # plot the datapoints (the starting points on all the geodesics, colored by memberships if specified):\n",
    "    plt.scatter(centers[:,0], centers[:,1], c=\"red\", label = \"centers\", marker='*', edgecolor='black', s = 170,zorder = 10)\n",
    "    plt.scatter(segments[:,0,0,0], segments[:,0,0,1], c=\"green\", label = \"centers\", marker='o', s = 30,zorder = 10)\n",
    "plt.xlim(-torch.pi, torch.pi)\n",
    "plt.ylim(-torch.pi, torch.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 0 (with periodicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5e-3 # Frechet mean learning rate\n",
    "learning_rate = 1e-3\n",
    "num_iter_outer = 1\n",
    "num_geod_iter = 20 # number of geodesics refinement interations per 1 FM update\n",
    "memberships = None\n",
    "meaningful_geodesics = None\n",
    "loss_history = []\n",
    "meaningful_geodesics_loss_history = []\n",
    "norm_FM_grad_history = []\n",
    "\n",
    "# Initialize geodesic segments\n",
    "centers = ricci_regularization.initialize_centers(encoded_points, K, N)\n",
    "segments = ricci_regularization.connect_centers2encoded_data_with_segments(encoded_points, centers, m) #initialize the segents between centers and data points\n",
    "init_segments = torch.clone(segments) # save initial segments\n",
    "# Set optimizer params:\n",
    "segments = torch.nn.Parameter(segments) # Wrap as a parameter\n",
    "\n",
    "optimizer = torch.optim.SGD([segments], lr=learning_rate)\n",
    "ricci_regularization.plot_octopus(segments.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer loop \n",
    "for iter_outer in range(num_iter_outer):\n",
    "    # update the segments to take into account periodicity\n",
    "    #segments = compute_updated_segments(segments)\n",
    "    # Inner loop (refining geodesics)\n",
    "    for iter_inner in range(num_geod_iter):\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "        # Compute the loss\n",
    "        loss_geodesics = compute_energy(points_on_geodesics=segments, decoder=torus_ae.decoder_torus)\n",
    "\n",
    "        # Backpropagation: compute gradients\n",
    "        loss_geodesics.backward()\n",
    "\n",
    "        # Zero out gradients for the first and last points (don't want them updated)\n",
    "        segments.grad[:, :, 0, :] = 0.  # First points along 'geodesics'\n",
    "        segments.grad[:, :, -1, :] = 0.  # Last points along 'geodesics'\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss value\n",
    "        loss_history.append(loss_geodesics.item())\n",
    "        #ricci_regularization.plot_octopus(segments,memberships=memberships,meaningful_geodesics=meaningful_geodesics, \n",
    "        #             saving_adress=shots_folder_adress, iter=iter_outer*num_geod_iter + iter_inner,silent=True)\n",
    "        #print(f\"Iteration #{iter_inner + 1}, loss: {loss_geodesics.item():.3f}\")    \n",
    "    lengths_of_geod = compute_lengths(segments, torus_ae.decoder_torus) # comute a vector of length of all geodesics shape (N,K)\n",
    "    memberships = torch.argmin(lengths_of_geod, dim=1) # retrieve the class membership of each point by finding the closest cluster center shape (N)\n",
    "\n",
    "    batch_indices = torch.arange(N)\n",
    "    meaningful_geodesics = segments[batch_indices, memberships, :, :].detach() # pick only geodesics connecting points to cluster centers where the points are assigned shape (N,m,d)\n",
    "    \n",
    "    # saving the lengths of meaningful geodesics\n",
    "    meaningful_geodesics_lengths = compute_lengths(meaningful_geodesics)\n",
    "    meaningful_geodesics_loss_history.append( meaningful_geodesics_lengths.detach().mean().item() )\n",
    "\n",
    "    v = meaningful_geodesics[:,-1,:] - meaningful_geodesics[:,-2,:]\n",
    "    v = v / v.norm(dim=1).unsqueeze(-1) # find the last segments of the geod shape (N,d)\n",
    "    # update cluster centers with weight beta\n",
    "    for i in range(K):\n",
    "        average_FM_grad_norm = 0.\n",
    "        cluster_mask = memberships == i \n",
    "        v_i = v[cluster_mask] \n",
    "        l_i = lengths_of_geod[cluster_mask][:,0]\n",
    "        with torch.no_grad():\n",
    "            FM_gradient = torch.sum( l_i.unsqueeze(-1) * v_i, dim=0 )\n",
    "            segments[:, i, -1, :] += - beta * FM_gradient # update i-th cluster center ( only moving the very last point on a geodesic)\n",
    "            #print(f\"\\nNorm of gradient of FM lossfor {i}-th cluster\", FM_gradient.norm().item())\n",
    "        average_FM_grad_norm += FM_gradient.norm().item()/K\n",
    "    norm_FM_grad_history.append(average_FM_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming norm_FM_grad_history, meaningful_geodesics_loss_history, loss_history are arrays or tensors\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure with 1 row and 3 columns\n",
    "\n",
    "# Plot norm_FM_grad_history\n",
    "axes[0].plot(norm_FM_grad_history, marker='o', label='FM Grad History')\n",
    "axes[0].set_title('Averege shift of centers (Fréchet mean gradient norm)')\n",
    "axes[0].set_xlabel('Outer loop iterations')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot meaningful_geodesics_loss_history\n",
    "axes[1].plot(meaningful_geodesics_loss_history, marker='o', label='Geodesics Loss History', color='orange')\n",
    "axes[1].set_title('Meaningfull geodesics length')\n",
    "axes[1].set_xlabel('Outer loop iterations')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot loss_history\n",
    "axes[2].plot(loss_history, label='All geodesics length', color='green')\n",
    "axes[2].set_title('All geodesics length')\n",
    "axes[2].set_xlabel(f'All iterations: {num_geod_iter} inner  per outer loop iter')\n",
    "axes[2].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# here bulshit starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimizer_parameters(learning_rate, max_iterations):\n",
    "    \"\"\"\n",
    "    Sets the parameters for the optimizer.\n",
    "    \n",
    "    Parameters:\n",
    "        learning_rate (float): The learning rate for gradient descent.\n",
    "        max_iterations (int): The maximum number of iterations for optimization.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Optimizer parameters.\n",
    "    \"\"\"\n",
    "    optimizer_params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_iterations': max_iterations\n",
    "    }\n",
    "    print(f\"Optimizer parameters set: {optimizer_params}\")\n",
    "    return optimizer_params\n",
    "\n",
    "def update_frechet_mean(data, memberships, K):\n",
    "    \"\"\"\n",
    "    Updates the cluster centers (Frechet mean update) using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "        data (torch.Tensor): Data points, shape (n_samples, n_features).\n",
    "        memberships (torch.Tensor): Membership array, shape (n_samples,).\n",
    "        K (int): Number of clusters.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Updated cluster centers, shape (K, n_features).\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    updated_centers = torch.zeros((K, n_features))\n",
    "    \n",
    "    for k in range(K):\n",
    "        cluster_mask = memberships == k  # Mask for points in cluster k\n",
    "        cluster_points = data[cluster_mask]\n",
    "        if cluster_points.size(0) > 0:\n",
    "            updated_centers[k] = cluster_points.mean(dim=0)\n",
    "    \n",
    "    print(f\"Updated Frechet means: {updated_centers}\")\n",
    "    return updated_centers\n",
    "\n",
    "def geodesic_update(data, centers, memberships, learning_rate):\n",
    "    \"\"\"\n",
    "    Refines geodesic approximations and updates the parameters using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "        data (torch.Tensor): Data points, shape (n_samples, n_features).\n",
    "        centers (torch.Tensor): Current cluster centers, shape (K, n_features).\n",
    "        memberships (torch.Tensor): Membership array, shape (n_samples,).\n",
    "        learning_rate (float): Learning rate for updates.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Updated geodesic approximations, shape (n_samples, K, n_features).\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    K = centers.shape[0]\n",
    "    \n",
    "    # Geodesic approximation: linear interpolation as a simple example\n",
    "    geodesics = torch.zeros((n_samples, K, n_features))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for k in range(K):\n",
    "            geodesics[i, k] = data[i] + learning_rate * (centers[k] - data[i])\n",
    "    \n",
    "    print(f\"Updated geodesics: {geodesics}\")\n",
    "    return geodesics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_geodesics(data, centers, m=20):\n",
    "    \"\"\"\n",
    "    Initializes geodesics connecting data points to cluster centers.\n",
    "    \n",
    "    Parameters:\n",
    "        data (torch.Tensor): Data points, shape (n_samples, n_features).\n",
    "        centers (torch.Tensor): Cluster centers, shape (K, n_features).\n",
    "        m (int): Number of intermediate points on each geodesic.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Points on geodesics, shape (n_samples, K, m, n_features).\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    K = centers.shape[0]\n",
    "    \n",
    "    # Initialize geodesics tensor\n",
    "    geodesics = torch.zeros((n_samples, K, m, n_features))\n",
    "    \n",
    "    # Generate geodesics\n",
    "    for i in range(n_samples):\n",
    "        for l in range(K):\n",
    "            geodesic_start = data[i]\n",
    "            geodesic_end = centers[l]\n",
    "            \n",
    "            # Generate m evenly spaced points along the straight-line geodesic\n",
    "            for j in range(m):\n",
    "                t = j / (m - 1)  # Normalized position along the geodesic [0, 1]\n",
    "                geodesics[i, l, j] = (1 - t) * geodesic_start + t * geodesic_end\n",
    "    \n",
    "    print(f\"Geodesics shape: {geodesics.shape}\")\n",
    "    return geodesics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Example data\n",
    "data = torch.rand(10, 2)  # 10 points in 3D\n",
    "K = 3  # Number of clusters\n",
    "learning_rate = 0.01\n",
    "max_iterations = 100\n",
    "\n",
    "# Initialization\n",
    "centers, probabilities = initialize_centers(data, K)\n",
    "\n",
    "# Optimizer parameters\n",
    "optimizer_params = set_optimizer_parameters(learning_rate, max_iterations)\n",
    "\n",
    "# Dummy memberships (random assignment for initialization)\n",
    "memberships = torch.randint(0, K, (data.shape[0],))\n",
    "\n",
    "# Geodesic update\n",
    "geodesics = geodesic_update(data, centers, memberships, optimizer_params['learning_rate'])\n",
    "\n",
    "# Frechet mean update\n",
    "updated_centers = update_frechet_mean(data, memberships, K)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
